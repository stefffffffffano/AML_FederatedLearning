{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import sys\n",
    "sys.path.append('../data/cifar100/')  \n",
    "from cifar100_loader import load_cifar100\n",
    "from models.model import LeNet5 #import the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Check whether a GPU is available and if so, use it\n",
    "print(DEVICE)\n",
    "#Momentum and batch size have not been tuned\n",
    "BATCH_SIZE = 64    # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "\n",
    "NUM_EPOCHS = 100      # Total number of training epochs (iterations over dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#25% of the dataset kept for validation\n",
    "trainloader, validloader, testloader = load_cifar100(batch_size=32, validation_split=0.25)\n",
    "#The previous function has been verified to ensure that the distribution among classes\n",
    "#is preserved in the training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified version of LeNet5 to work with CIFAR100, paper cited in model.py\n",
    "model = LeNet5().to(DEVICE) # Create the model\n",
    "# Define loss function -> softmax used by the model, It doesn't make sense to use the CrossqEntropyLoss\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler_factory(num_epochs):\n",
    "    \"\"\"\n",
    "    Return a set of predefined learning rate scheduler factories with reasonable parameters.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Total number of epochs.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples with scheduler names and factory functions.\n",
    "    \"\"\"\n",
    "    schedulers = [\n",
    "        # StepLR\n",
    "        (\"StepLR (step_size=num_epochs//3, gamma=0.1)\",\n",
    "         lambda optimizer: torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs // 3, gamma=0.1)),\n",
    "        (\"StepLR (step_size=num_epochs//5, gamma=0.5)\",\n",
    "         lambda optimizer: torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs // 5, gamma=0.5)),\n",
    "\n",
    "        # CosineAnnealingLR\n",
    "        (\"CosineAnnealingLR (T_max=num_epochs//2, eta_min=1e-4)\",\n",
    "         lambda optimizer: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs // 3, eta_min=1e-4)),\n",
    "\n",
    "        # ExponentialLR\n",
    "        (\"ExponentialLR (gamma=0.95)\",\n",
    "         lambda optimizer: torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)),\n",
    "        (\"ExponentialLR (gamma=0.9)\",\n",
    "         lambda optimizer: torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)),\n",
    "\n",
    "        # ReduceLROnPlateau\n",
    "        (\"ReduceLROnPlateau (patience=5, factor=0.5)\",\n",
    "         lambda optimizer: torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5))\n",
    "    ]\n",
    "\n",
    "    return schedulers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given dataset.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.train(False) # Set Network to evaluation mode\n",
    "        running_corrects = 0\n",
    "        losses = []\n",
    "        for data, targets in dataloader:\n",
    "            data = data.to(DEVICE)        # Move the data to the GPU\n",
    "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
    "            # Forward Pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            # Update Corrects\n",
    "            running_corrects += torch.sum(preds == targets.data).data.item()\n",
    "            # Calculate Accuracy\n",
    "            accuracy = running_corrects / float(len(dataloader.dataset))\n",
    "\n",
    "    return accuracy, mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, scheduler,optimizer):\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    cudnn.benchmark  # Calling this optimizes runtime\n",
    "\n",
    "    best_val_acc = 0.0  \n",
    "    best_model_state = None  # The model with the best accuracy\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs): \n",
    "        for data, targets in trainloader:\n",
    "            data = data.to(DEVICE)        # Move the data to the GPU\n",
    "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
    "            model.train()                # Set Network to train mode\n",
    "            optimizer.zero_grad()         # Zero the gradients\n",
    "            outputs = model(data)         # Pass data through the model\n",
    "            loss = criterion(outputs, targets)  # Compute loss\n",
    "            loss.backward()               # Backpropagation\n",
    "            optimizer.step()              # Update model parameters\n",
    "\n",
    "        # Evaluate on the training set\n",
    "        train_acc, train_loss = evaluate(model, trainloader)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        val_acc, val_loss = evaluate(model, validloader)\n",
    "        #print(f'Epoch {epoch+1}, Validation Accuracy: {val_acc*100:.2f}%')\n",
    "\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Update the best model if validation accuracy improves\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()  # Salva lo stato corrente del modello\n",
    "            #print(f\"New best model found with accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        #print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "    # Alla fine del training, ritorna il miglior modello\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_accuracies, train_losses, val_accuracies, val_losses, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader):\n",
    "    \"\"\"\n",
    "    Test the model on the test set.\n",
    "    \"\"\"\n",
    "    accuracy, loss = evaluate(model, testloader)\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  # low value for parameter tuning\n",
    "learning_rates = [0.05, 0.01, 0.005, 0.001]\n",
    "weight_decays = [1e-5, 5e-5, 1e-4]\n",
    "scheduler_factories = get_scheduler_factory(num_epochs)\n",
    "results = []\n",
    "best_validation_accuracy_overall = 0.0\n",
    "best_setting = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        for scheduler_name, scheduler_factory in scheduler_factories:\n",
    "            # Reset the model\n",
    "            model = LeNet5().to(DEVICE)\n",
    "            # Create the optimizer\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "            # Create the scheduler\n",
    "            scheduler = scheduler_factory(optimizer)\n",
    "            # Ezecute training\n",
    "            train_accuracies, train_losses, val_accuracies, val_losses, model = train(num_epochs, scheduler,optimizer)\n",
    "            # Print the best validation accuracy\n",
    "            best_val_accuracy = max(val_accuracies)\n",
    "            if best_val_accuracy > best_validation_accuracy_overall:\n",
    "                best_validation_accuracy_overall = best_val_accuracy\n",
    "                best_setting = (lr, wd, scheduler_name)\n",
    "            print(f'Learning Rate: {lr}, Weight Decay: {wd}, Scheduler: {scheduler_name}, Best Validation Accuracy: {best_val_accuracy*100:.2f}%')\n",
    "\n",
    "            results.append({\n",
    "                'learning_rate': lr,\n",
    "                'weight_decay': wd,\n",
    "                'scheduler_name': scheduler_name,\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'val_losses': val_losses,\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting and saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_results(results, save_dir='./plots_centralized'):\n",
    "    \"\"\"\n",
    "    Save plots comparing training accuracy and validation accuracy per epoch for each combination of hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of dictionaries, where each dictionary contains:\n",
    "                        - 'learning_rate': Learning rate used.\n",
    "                        - 'weight_decay': Weight decay used.\n",
    "                        - 'scheduler_name': Name of the scheduler.\n",
    "                        - 'train_accuracies': List of training accuracies.\n",
    "                        - 'val_accuracies': List of validation accuracies.\n",
    "        save_dir (str): Directory where the plots will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for res in results:\n",
    "        # Extract hyperparameter values\n",
    "        lr = res['learning_rate']\n",
    "        wd = res['weight_decay']\n",
    "        scheduler_name = res['scheduler_name']\n",
    "\n",
    "        # Generate a unique filename prefix for each configuration\n",
    "        file_prefix = f\"LR_{lr}_WD_{wd}_Scheduler_{scheduler_name.replace(' ', '_')}\"\n",
    "\n",
    "        # Plot training and validation accuracy per epoch\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(res['train_accuracies'], label='Training Accuracy')\n",
    "        plt.plot(res['val_accuracies'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f\"Training vs Validation Accuracy (LR={lr}, WD={wd}, Scheduler={scheduler_name})\")\n",
    "        plt.legend()\n",
    "        accuracy_plot_path = os.path.join(save_dir, f\"{file_prefix}_training_vs_validation_accuracy.png\")\n",
    "        plt.savefig(accuracy_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Plots saved to directory: {save_dir}\")\n",
    "plot_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
