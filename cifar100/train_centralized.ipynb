{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../data/cifar100/')  \n",
    "from cifar100_loader import CIFAR100DataLoader\n",
    "from models.model import LeNet5 #import the model\n",
    "from utils.utils import evaluate\n",
    "from utils.federated_utils import plot_metrics,test,save_data,load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # Check whether a GPU is available and if so, use it\n",
    "print(DEVICE)\n",
    "#Momentum and batch size have not been tuned\n",
    "BATCH_SIZE = 64    # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "LOG_FREQUENCY = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#10% of the dataset kept for validation\n",
    "data_loader = CIFAR100DataLoader(batch_size=BATCH_SIZE, validation_split=0.1, download=True, num_workers=4, pin_memory=True)\n",
    "trainloader, validloader, testloader = data_loader.train_loader, data_loader.val_loader, data_loader.test_loader\n",
    "#The previous function has been verified to ensure that the distribution among classes\n",
    "#is preserved in the training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified version of LeNet5 to work with CIFAR100, paper cited in model.py\n",
    "model = LeNet5().to(DEVICE) # Create the model\n",
    "# Define loss function -> log_softmax used by the model, NLL is required\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedulers\n",
    "List of schedulers to be experimented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler_factory(num_epochs):\n",
    "    \"\"\"\n",
    "    Return a set of predefined learning rate scheduler factories with reasonable parameters.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Total number of epochs.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples with scheduler names and factory functions.\n",
    "    \"\"\"\n",
    "    schedulers = [\n",
    "        # StepLR\n",
    "        (\"StepLR (step_size=num_epochs//3, gamma=0.1)\",\n",
    "         lambda optimizer: torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs // 3, gamma=0.1)),\n",
    "\n",
    "        # CosineAnnealingLR\n",
    "        (\"CosineAnnealingLR\",\n",
    "         lambda optimizer: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)),\n",
    "\n",
    "        # ExponentialLR\n",
    "        (\"ExponentialLR (gamma=0.9)\",\n",
    "         lambda optimizer: torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)),\n",
    "    ]\n",
    "    return schedulers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, scheduler,optimizer,model):\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    cudnn.benchmark  # Calling this optimizes runtime\n",
    "\n",
    "    best_val_acc = 0.0  \n",
    "    best_model_state = None  # The model with the best accuracy\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs): \n",
    "        epoch_train_loss = 0.0 \n",
    "        correct = 0  # Number of correct predictions\n",
    "        total = 0  # Total number of examples\n",
    "        for data, targets in trainloader:\n",
    "            data = data.to(DEVICE)        # Move the data to the GPU\n",
    "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
    "            model.train()                # Set Network to train mode\n",
    "            optimizer.zero_grad()         # Zero the gradients\n",
    "            outputs = model(data)         # Pass data through the model\n",
    "            loss = criterion(outputs, targets)  # Compute loss\n",
    "            loss.backward()               # Backpropagation\n",
    "            optimizer.step()              # Update model parameters\n",
    "            # Accumulate training loss\n",
    "            epoch_train_loss += loss.item() * data.size(0)  # Multiply by batch dimension\n",
    "            # Compute accuracy\n",
    "            _, predicted = outputs.max(1)  # Predictions\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        # Compute the average training loss and accuracy\n",
    "        train_loss = epoch_train_loss / total\n",
    "        train_acc = (correct / total) * 100 #in percentage\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        # Evaluate on the validation set, done every epoch\n",
    "        val_acc, val_loss = evaluate(model, validloader)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Update the best model if validation accuracy improves\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = deepcopy(model.state_dict())  # Save the current model state\n",
    "        \n",
    "        if(epoch+1%LOG_FREQUENCY==0):\n",
    "            print(f\"--> epoch: {epoch+1}, training accuracy: {train_acc:.2f}, validation accuracy: {val_acc:.2f}\")\n",
    "            \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "    # At the end, return the best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_accuracies, train_losses, val_accuracies, val_losses, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
    "learning_rates = np.logspace(-3, -1, num=3)\n",
    "\n",
    "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
    "weight_decays = np.logspace(-4, -1, num=4)\n",
    "\n",
    "print(\"Learning Rate Values (log-uniform):\", learning_rates)\n",
    "print(\"Weight Decay Values (log-uniform):\", weight_decays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  # low value for parameter tuning\n",
    "\n",
    "scheduler_factories = get_scheduler_factory(num_epochs)\n",
    "results = []\n",
    "best_validation_accuracy_overall = 0.0\n",
    "best_setting = None\n",
    "print('Starting the parameter tuning loop...')\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        for scheduler_name, scheduler_factory in scheduler_factories:\n",
    "            # Reset the model\n",
    "            model = LeNet5().to(DEVICE)\n",
    "            # Create the optimizer\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "            # Create the scheduler\n",
    "            scheduler = scheduler_factory(optimizer)\n",
    "            # Ezecute training\n",
    "            train_accuracies, train_losses, val_accuracies, val_losses, model = train(num_epochs, scheduler,optimizer,model)\n",
    "            # Print the best validation accuracy\n",
    "            best_val_accuracy = max(val_accuracies)\n",
    "            if best_val_accuracy > best_validation_accuracy_overall:\n",
    "                best_validation_accuracy_overall = best_val_accuracy\n",
    "                best_setting = (lr, wd, scheduler_name)\n",
    "            print(f'Learning Rate: {lr}, Weight Decay: {wd}, Scheduler: {scheduler_name}, Best Validation Accuracy: {best_val_accuracy:.2f}%')\n",
    "\n",
    "            results.append({\n",
    "                'learning_rate': lr,\n",
    "                'weight_decay': wd,\n",
    "                'scheduler_name': scheduler_name,\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'val_losses': val_losses,\n",
    "            })\n",
    "print(\"Finished training loop.\")\n",
    "print(f'Best validation accuracy overall: {best_validation_accuracy_overall:.2f}%')\n",
    "print(f'Best setting: {best_setting}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting and saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re  # Imported module for regular expressions\n",
    "\n",
    "def plot_results(results, save_dir='./plots_centralized'):\n",
    "    \"\"\"\n",
    "    Save plots comparing training accuracy and validation accuracy per epoch for each combination of hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of dictionaries, where each dictionary contains:\n",
    "                        - 'learning_rate': Learning rate used.\n",
    "                        - 'weight_decay': Weight decay used.\n",
    "                        - 'scheduler_name': Name of the scheduler.\n",
    "                        - 'train_accuracies': List of training accuracies.\n",
    "                        - 'val_accuracies': List of validation accuracies.\n",
    "        save_dir (str): Directory where the plots will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for res in results:\n",
    "        # Extract hyperparameter values\n",
    "        lr = res['learning_rate']\n",
    "        wd = res['weight_decay']\n",
    "        scheduler_name = res['scheduler_name']\n",
    "\n",
    "        # Clean up the scheduler name for filename compatibility\n",
    "        clean_scheduler_name = re.sub(r\"[^a-zA-Z0-9]\", \"_\", scheduler_name)  # Sostituisce i caratteri non alfanumerici con '_'\n",
    "\n",
    "        # Generate a unique filename prefix for each configuration\n",
    "        file_prefix = f\"LR_{lr}_WD_{wd}_Scheduler_{clean_scheduler_name}\"\n",
    "\n",
    "        # Plot training and validation accuracy per epoch\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(res['train_accuracies'], label='Training Accuracy')\n",
    "        plt.plot(res['val_accuracies'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f\"Training vs Validation Accuracy (LR={lr}, WD={wd}, Scheduler={scheduler_name})\")\n",
    "        plt.legend()\n",
    "        accuracy_plot_path = os.path.join(save_dir, f\"{file_prefix}_training_vs_validation_accuracy.png\")\n",
    "        plt.savefig(accuracy_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Plots saved to directory: {save_dir}\")\n",
    "\n",
    "#Plot only the best result \n",
    "filtered_results = [res for res in results if res['learning_rate'] == 0.01 and res['weight_decay'] == 0.0001 and res['scheduler_name']==\"CosineAnnealingLR (T_max=num_epochs//3, eta_min=1e-4)\"]\n",
    "plot_results(filtered_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss for the results with the learning rate = 0.01 and weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "resLR = []\n",
    "filtered_results = [res for res in results if res['learning_rate'] == 0.01 and res['weight_decay'] == 0.0001]\n",
    "\n",
    "# Stampa dei risultati filtrati per verificare\n",
    "print(filtered_results)\n",
    "\n",
    "#plot only train losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "for res in filtered_results:\n",
    "    plt.plot(res['train_losses'], label=f\"WD={res['weight_decay']}, Scheduler={res['scheduler_name']}\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f\"Training Loss (LR=0.01)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model\n",
    "Given the observations done in the sections before and analyzing all the plots, the following is the configuration that brought the best results. More details are reported in Report.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 200 \n",
    "#Values found in the previous step that gave the best accuracy\n",
    "LR = 0.01\n",
    "WD =  0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m scheduler \u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, NUM_EPOCHS)\n\u001b[0;32m      4\u001b[0m train_accuracies, train_losses, val_accuracies, val_losses, model \u001b[38;5;241m=\u001b[39m train(NUM_EPOCHS, scheduler, optimizer,model)\n\u001b[1;32m----> 5\u001b[0m test_accuracy, test_loss \u001b[38;5;241m=\u001b[39m test(model, testloader)\n\u001b[0;32m      6\u001b[0m plot_metrics(train_accuracies, train_losses, val_accuracies, val_losses, \n\u001b[0;32m      7\u001b[0m                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCentralizedCifar.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m save_data(model, val_accuracies, val_losses, train_accuracies, train_losses, \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m      9\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCentralizedCifar.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "model = LeNet5().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WD)\n",
    "scheduler =torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, NUM_EPOCHS)\n",
    "train_accuracies, train_losses, val_accuracies, val_losses, model = train(NUM_EPOCHS, scheduler, optimizer,model)\n",
    "test_accuracy, test_loss = test(model, testloader)\n",
    "plot_metrics(train_accuracies, train_losses, val_accuracies, val_losses, \n",
    "                 f\"CentralizedCifar.png\")\n",
    "save_data(model, val_accuracies, val_losses, train_accuracies, train_losses, None, \n",
    "              f\"CentralizedCifar.pth\")\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to ./trained_models/CentralizedCifar.pth\n",
      "Test Accuracy: 5565.00%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test(model, testloader)\n",
    "plot_metrics(train_accuracies, train_losses, val_accuracies, val_losses, \n",
    "                 f\"CentralizedCifar.png\")\n",
    "save_data(model, val_accuracies, val_losses, train_accuracies, train_losses, None, \n",
    "              f\"CentralizedCifar.pth\")\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
