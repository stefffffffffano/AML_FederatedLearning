{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Project\n",
    "This notebook demonstrates how to set up and compare Federated Learning (FL) with Centralized Learning (CL) using the CIFAR-100 dataset and the modified version of the LeNet-5 model taken from [Hsu et al., Federated Visual Classification with Real-World Data Distribution, ECCV 2020]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We start by importing necessary libraries and setting global constants for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.model import LeNet5 #import the model\n",
    "import numpy as np\n",
    "sys.path.append('../data/cifar100/')\n",
    "from cifar100_loader import CIFAR100DataLoader\n",
    "from Server import Server\n",
    "from utils.federated_utils import plot_metrics,test, plot_client_selection,save_data,load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Constants for FL training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "\n",
    "NUM_CLIENTS = 100  # Total number of clients in the federation\n",
    "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
    "LOCAL_STEPS = 4  # Number of local steps (J)\n",
    "GLOBAL_ROUNDS = 2000  # Total number of communication rounds\n",
    "\n",
    "BATCH_SIZE = 64  # Batch size for local training\n",
    "MOMENTUM = 0 #Momentum set to 0 in FedAvg\n",
    "CHECKPOINT_DIR = './checkpoints/'\n",
    "LOG_FREQUENCY = 10  # Frequency of logging training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "We load the CIFAR-100 dataset and split it into training, validation, and test sets. This is done using the `data_loader.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#10% of the dataset kept for validation\n",
    "data_loader = CIFAR100DataLoader(batch_size=BATCH_SIZE, validation_split=0.1, download=True, num_workers=4, pin_memory=True)\n",
    "trainloader, validloader, testloader = data_loader.train_loader, data_loader.val_loader, data_loader.test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Federated Training\n",
    "We simulate federated learning by splitting the dataset into shards and training with selected clients in each round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = LeNet5()\n",
    "criterion = nn.NLLLoss()# our loss function for classification tasks on CIFAR-100\n",
    "#We already use log_softmax in the forward pass of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning, Federated baseline\n",
    "J=4, idd shards, uniform client selection, rounds = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Hyperparameter tuning for the learning rate and weight decay\n",
    "J=4, idd shards, uniform client selection, rounds = 100\n",
    "\"\"\"\n",
    "\n",
    "lr_values = [0.1,0.05,0.01,0.005,0.001,0.005,0.0001]\n",
    "\n",
    "\n",
    "wd_values = [0.001,0.0001]\n",
    "\n",
    "print(\"Learning Rate Values (log-uniform):\", lr_values)\n",
    "print(\"Weight Decay Values (log-uniform):\", wd_values)\n",
    "\n",
    "rounds = 100 #fewer communication rounds for hyperparameter tuning\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for lr in lr_values:\n",
    "    for wd in wd_values:\n",
    "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "        global_model = LeNet5() \n",
    "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS)\n",
    "        plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"FederatedBaselineTuning_lr_{lr}_wd_{wd}.png\")\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (lr,wd)\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Training and testing with J=4, 2000 communication rounds\n",
    "\"\"\"\n",
    "lr = 0.1\n",
    "wd = 0.0001\n",
    "global_model = LeNet5() \n",
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS)\n",
    "test_accuracy = test(global_model, testloader)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#If needed for future plots or analysis, no need to train again\n",
    "save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses,client_selection_count, \"FederatedBaseline.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the effect of client participation\n",
    "We implemented a skewed client sampling: each client has a different probability of being selected at each round, and can be used to simulate settings in which some clients are more “active” than others. Client selection values are sampled according to a Dirichlet distribution parameterized by an hyperparameter ɣ.\n",
    "Let's test what happens with different values of gamma:  \n",
    "\n",
    "\n",
    "**gamma = 0.05** <-- Represents extreme heterogeneity. A small number of clients will dominate the selection process, being chosen almost exclusively, while most clients will rarely participate.  \n",
    "\n",
    "\n",
    "**gamma = 0.5** <-- Introduces moderate heterogeneity. Some clients have higher selection probabilities than others, but the imbalance is not extreme.  \n",
    "\n",
    "\n",
    "**gamma = 1**   <-- A standard choice for the Dirichlet distribution. This provides a relatively balanced selection with mild heterogeneity.  \n",
    "\n",
    "\n",
    "**gamma = 5**   <-- Simulates near-uniform participation, where all clients have almost equal probabilities of being selected.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma values to be tested\n",
    "gamma_values = [0.05, 0.5, 1.0, 5.0]\n",
    "\n",
    "\n",
    "rounds = 100  # Fewer communication rounds for hyperparameter tuning\n",
    "\n",
    "# Best results for each gamma\n",
    "best_settings = {}\n",
    "\n",
    "# Cycle through the gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"\\n=== Hyperparameter Tuning for Gamma: {gamma} ===\\n\")\n",
    "    best_val_accuracy = 0\n",
    "    best_setting = None\n",
    "    \n",
    "    for lr in lr_values:\n",
    "        for wd in wd_values:\n",
    "            print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "            \n",
    "            # Initialize the model and server\n",
    "            global_model = LeNet5() \n",
    "            server = Server(global_model, DEVICE, CHECKPOINT_DIR)\n",
    "            \n",
    "            # Federated training with the current hyperparameters\n",
    "            global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(\n",
    "                criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, \n",
    "                rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, \n",
    "                C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS, log_freq=10, detailed_print=False, gamma=gamma\n",
    "            )\n",
    "            \n",
    "            # Plot results (only the best ones are kept at the end)\n",
    "            plot_metrics(train_accuracies, train_losses, val_accuracies, val_losses, \n",
    "                         f\"FederatedGamma{gamma}Tuning_lr_{lr}_wd_{wd}.png\")\n",
    "            \n",
    "            \n",
    "            max_val_accuracy = max(val_accuracies)\n",
    "            print(f\"Validation accuracy: {max_val_accuracy} with lr: {lr} and wd: {wd}\")\n",
    "            \n",
    "            avg_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
    "            if avg_val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = avg_val_accuracy\n",
    "                best_setting = (lr, wd)\n",
    "    \n",
    "    # Trace best setting for the current gamma\n",
    "    best_settings[gamma] = {\n",
    "        'best_lr': best_setting[0],\n",
    "        'best_wd': best_setting[1],\n",
    "        'val_accuracy': best_val_accuracy\n",
    "    }\n",
    "    print(f\"Best setting for Gamma {gamma}: {best_setting} with validation accuracy: {best_val_accuracy}\\n\")\n",
    "\n",
    "    \n",
    "    print(f\"\\n=== Final Training for Gamma: {gamma} ===\\n\")\n",
    "    lr, wd = best_setting\n",
    "    global_model = LeNet5() \n",
    "    server = Server(global_model, DEVICE, CHECKPOINT_DIR)\n",
    "    \n",
    "    global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(\n",
    "        criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, \n",
    "        rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, \n",
    "        C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS, log_freq=10, detailed_print=False, gamma=gamma\n",
    "    )\n",
    "    \n",
    "    # Final test\n",
    "    test_accuracy = test(global_model, testloader)\n",
    "    print(f\"Test accuracy for Gamma {gamma}: {test_accuracy}\")\n",
    "    \n",
    "    # Save the model and plots\n",
    "    plot_metrics(train_accuracies, train_losses, val_accuracies, val_losses, \n",
    "                 f\"FederatedGamma{gamma}_lr_{lr}_wd_{wd}.png\")\n",
    "    plot_client_selection(client_selection_count, f\"ClientSelectionGamma{gamma}_lr_{lr}_wd_{wd}.png\")\n",
    "    save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count, \n",
    "              f\"FederatedGamma{gamma}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "File to be loaded: \n",
    "FederatedBaseline.pth\n",
    "FederatedGamma005.pth\n",
    "FederatedGamma05.pth\n",
    "FederatedGamma5.pth\n",
    "FederatedGamma1.pth\n",
    "\"\"\"\n",
    "mm = LeNet5() #Don't care about the model, pass always the same\n",
    "_, val_accuracies_baseline, _,_,_,_ = load_data(mm,'FederatedBaseline.pth')\n",
    "val_accuracies_baseline = [val_accuracies_baseline[i]*100 for i in range(len(val_accuracies_baseline))] #They were on a different scale\n",
    "_, val_accuracies_gamma005, _,_,_,_ = load_data(mm,'FederatedGamma005.pth')\n",
    "_, val_accuracies_gamma05, _,_,_,_ = load_data(mm,'FederatedGamma05.pth')\n",
    "_, val_accuracies_gamma1, _,_,_,_ = load_data(mm,'FederatedGamma1.pth')\n",
    "_, val_accuracies_gamma5, _,_,_,_ = load_data(mm,'FederatedGamma5.pth')\n",
    "\n",
    "\n",
    "def plot_accuracies_with_menu():\n",
    "    \"\"\"\n",
    "    Plots all validation accuracies for different gamma values on the same graph.\n",
    "    \"\"\"\n",
    "    # Accuracy values for all models\n",
    "    accuracies = {\n",
    "        'Baseline': val_accuracies_baseline,\n",
    "        'Gamma = 0.05': val_accuracies_gamma005,\n",
    "        'Gamma = 0.5': val_accuracies_gamma05,\n",
    "        'Gamma = 1': val_accuracies_gamma1,\n",
    "        'Gamma = 5': val_accuracies_gamma5\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for label, accuracy in accuracies.items():\n",
    "        plt.plot(accuracy, label=label, linewidth=2) \n",
    "\n",
    "    plt.title(\"Validation Accuracies for Different Gamma Values\")\n",
    "    plt.xlabel(\"Rounds\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_accuracies_with_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate heterogeneous distributions\n",
    "#### (15 Exeperiments) -> we include also iid tests\n",
    "Fix K=100 and C=0.1, and simulate several non-iid shardings of the training set of CIFAR-100, by fixing the number of different labels clients have (Nc={1,5,10,50}). Then test the performance of FedAvg, comparing with the iid sharding, varying the number of local steps J={4,8,16}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters tuning function\n",
    "def hyperparameters_tuning(num_classes, local_steps, rounds):\n",
    "    print(f\"Hyperparameter tuning for num_classes={num_classes}, local_steps={local_steps}\")\n",
    "    lr_values = [0.1,0.05,0.01,0.005,0.001,0.005,0.0001]\n",
    "    wd_values = [0.001,0.0001]\n",
    "    best_val_accuracy = 0\n",
    "    best_setting = None\n",
    "    for lr in lr_values:\n",
    "        for wd in wd_values:\n",
    "            print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "            global_model = LeNet5()\n",
    "            server = Server(global_model, DEVICE, CHECKPOINT_DIR)\n",
    "            global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=num_classes, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=local_steps,log_freq=100, detailed_print=False,gamma=None)\n",
    "            plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"FederatedTuning_Nc_{num_classes}_J_{local_steps}_lr_{lr}_wd_{wd}.png\")\n",
    "            print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "            avg_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
    "            if avg_val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = avg_val_accuracy\n",
    "                best_setting = (lr, wd)\n",
    "    print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")\n",
    "    return best_setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "LOCAL_STEPS_VALUES = [4, 8, 16]  # Values for J (number of local steps)\n",
    "NUM_CLASSES_VALUES = [1, 5, 10, 50]  # Number of classes per client for Non-IID\n",
    "NUM_RUNDS = {4: 2000, 8: 1000, 16:500}\n",
    "TUNING_ROUNDS ={4: 100, 8: 75, 16: 50}\n",
    "IID_CLASSES = 100  # Full IID distribution\n",
    "\n",
    "# Function to perform the training and testing for a given configuration\n",
    "def run_experiment(num_classes, local_steps, plot_suffix):\n",
    "    print(f\"Running experiment: num_classes={num_classes}, local_steps={local_steps}\")\n",
    "    global_model = LeNet5()\n",
    "    server = Server(global_model, DEVICE, CHECKPOINT_DIR)\n",
    "\n",
    "    tuning_rounds = int(TUNING_ROUNDS[local_steps])\n",
    "    best_lr, best_wd = hyperparameters_tuning(num_classes = num_classes, local_steps=local_steps, rounds=tuning_rounds)\n",
    "\n",
    "    global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(\n",
    "        criterion, trainloader, validloader, \n",
    "        num_clients=NUM_CLIENTS, num_classes=num_classes, \n",
    "        rounds=NUM_RUNDS[local_steps], lr=best_lr, momentum=MOMENTUM, \n",
    "        batchsize=BATCH_SIZE, wd=best_wd, C=FRACTION_CLIENTS, \n",
    "        local_steps=local_steps, log_freq=100, \n",
    "        detailed_print=False, gamma=None  # No skewed sampling for this experiment\n",
    "    )\n",
    "\n",
    "    # Testing and plotting\n",
    "    test_accuracy = test(global_model, testloader)\n",
    "    plot_metrics(train_accuracies, train_losses, val_accuracies, val_losses, f\"Federated_{plot_suffix}_LR_{best_lr}_WD_{best_wd}.png\")\n",
    "    print(f\"Test accuracy for num_classes={num_classes}, local_steps={local_steps}: {test_accuracy}\")\n",
    "\n",
    "    # Save data for future analysis\n",
    "    save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count, f\"Federated_{plot_suffix}_LR_{best_lr}_WD_{best_wd}.pth\")\n",
    "\n",
    "# Main experiment loop\n",
    "for num_classes in NUM_CLASSES_VALUES + [IID_CLASSES]:  # Include IID setting\n",
    "    for local_steps in LOCAL_STEPS_VALUES:\n",
    "        plot_suffix = f\"num_classes_{num_classes}_local_steps_{local_steps}\"\n",
    "        run_experiment(num_classes, local_steps, plot_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Sharding Data Distribution\n",
    "Plotting histograms with the frequency of each class in the local datasets resulting from our data sharding procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)\n",
    "#Runs all the distributions\n",
    "server.plot_sharding_data_distribution(trainloader, num_clients=100, num_classes=100)\n",
    "server.plot_sharding_data_distribution(trainloader, num_clients=100, num_classes=50)\n",
    "server.plot_sharding_data_distribution(trainloader, num_clients=100, num_classes=10)\n",
    "server.plot_sharding_data_distribution(trainloader, num_clients=100, num_classes=5)\n",
    "server.plot_sharding_data_distribution(trainloader, num_clients=100, num_classes=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
