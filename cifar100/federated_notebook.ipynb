{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Project\n",
    "This notebook demonstrates how to set up and compare Federated Learning (FL) with Centralized Learning (CL) using the CIFAR-100 dataset and the modified version of the LeNet-5 model taken from [Hsu et al., Federated Visual Classification with Real-World Data Distribution, ECCV 2020]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We start by importing necessary libraries and setting global constants for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.model import LeNet5 #import the model\n",
    "import numpy as np\n",
    "sys.path.append('../data/cifar100/')\n",
    "from cifar100_loader import CIFAR100DataLoader\n",
    "from Server import Server\n",
    "from utils.federated_utils import plot_metrics,test, plot_client_selection,save_data,load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Constants for FL training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "\n",
    "NUM_CLIENTS = 100  # Total number of clients in the federation\n",
    "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
    "LOCAL_STEPS = 4  # Number of local steps (J)\n",
    "GLOBAL_ROUNDS = 2000  # Total number of communication rounds\n",
    "\n",
    "BATCH_SIZE = 64  # Batch size for local training\n",
    "LR = 1e-3  # Initial learning rate for local optimizers\n",
    "MOMENTUM = 0.9  # Momentum for SGD optimizer\n",
    "WEIGHT_DECAY = 0.0001  # Regularization term for local training\n",
    "CHECKPOINT_DIR = './checkpoints/'\n",
    "LOG_FREQUENCY = 10  # Frequency of logging training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "We load the CIFAR-100 dataset and split it into training, validation, and test sets. This is done using the `data_loader.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#10% of the dataset kept for validation\n",
    "data_loader = CIFAR100DataLoader(batch_size=BATCH_SIZE, validation_split=0.1, download=True, num_workers=4, pin_memory=True)\n",
    "trainloader, validloader, testloader = data_loader.train_loader, data_loader.val_loader, data_loader.test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Federated Training\n",
    "We simulate federated learning by splitting the dataset into shards and training with selected clients in each round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = LeNet5()\n",
    "criterion = nn.NLLLoss()# our loss function for classification tasks on CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from ./trained_models/FederatedBaseline.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stefano\\AppData\\Local\\Temp\\ipykernel_24244\\2558606882.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  save_dict = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def load_data(model, file_name):\n",
    "    \"\"\"\n",
    "    Load the model weights and metrics from a file.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to load the weights into.\n",
    "        file_name (str): Name of the file to load the data from.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the model, val_accuracies, val_losses, train_accuracies, and train_losses.\n",
    "    \"\"\"\n",
    "    # Fixed base directory\n",
    "    directory = './trained_models/'\n",
    "    # Complete path for the file\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    # Load the saved data from the specified file\n",
    "    save_dict = torch.load(file_path)\n",
    "    \n",
    "    # Load the model state\n",
    "    model.load_state_dict(save_dict['model_state'])\n",
    "    \n",
    "    # Extract the metrics\n",
    "    val_accuracies = save_dict['val_accuracies']\n",
    "    val_losses = save_dict['val_losses']\n",
    "    train_accuracies = save_dict['train_accuracies']\n",
    "    train_losses = save_dict['train_losses']\n",
    "    \n",
    "    print(f\"Data loaded successfully from {file_path}\")\n",
    "    \n",
    "    return model, val_accuracies, val_losses, train_accuracies, train_losses\n",
    "\n",
    "\n",
    "mm = LeNet5()\n",
    "\n",
    "model, val_accuracies, val_losses, train_accuracies, train_losses = load_data(mm,'FederatedBaseline.pth')\n",
    "val_accuracies = [val_accuracies[i]*100 for i in range(len(val_accuracies))]\n",
    "plot_metrics(train_accuracies,train_losses,val_accuracies,val_losses,f\"FederatedBaseline_lr_{0.1}_wd_{0.001}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Hyperparameter tuning for the learning rate and weight decay\n",
    "J=4, rounds = 100\n",
    "\"\"\"\n",
    "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
    "lr_values = np.logspace(-3, -1, num=3)\n",
    "\n",
    "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
    "wd_values = np.logspace(-4, -1, num=4)\n",
    "\n",
    "print(\"Learning Rate Values (log-uniform):\", lr_values)\n",
    "print(\"Weight Decay Values (log-uniform):\", wd_values)\n",
    "\n",
    "rounds = 100 #fewer communication rounds for hyperparameter tuning\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for lr in lr_values:\n",
    "    for wd in wd_values:\n",
    "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "        global_model = LeNet5() \n",
    "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS)\n",
    "        plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"FederatedBaselineTuning_lr_{lr}_wd_{wd}.png\")\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (lr,wd)\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Training and testing with J=4, 2000 communication rounds\n",
    "\"\"\"\n",
    "lr = 0.1\n",
    "wd = 0.001\n",
    "global_model = LeNet5() \n",
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS)\n",
    "test_accuracy = test(global_model, testloader)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#If needed for future plots or analysis, no need to train again\n",
    "save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses,client_selection_count, \"FederatedBaseline.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the effect of client participation\n",
    "We implemented a skewed client sampling: each client has a different probability of being selected at each round, and can be used to simulate settings in which some clients are more “active” than others. Client selection values are sampled according to a Dirichlet distribution parameterized by an hyperparameter ɣ.\n",
    "Let's test what happens with different values of gamma:  \n",
    "\n",
    "\n",
    "**gamma = 0.05** <-- Represents extreme heterogeneity. A small number of clients will dominate the selection process, being chosen almost exclusively, while most clients will rarely participate.  \n",
    "\n",
    "\n",
    "**gamma = 0.5** <-- Introduces moderate heterogeneity. Some clients have higher selection probabilities than others, but the imbalance is not extreme.  \n",
    "\n",
    "\n",
    "**gamma = 1**   <-- A standard choice for the Dirichlet distribution. This provides a relatively balanced selection with mild heterogeneity.  \n",
    "\n",
    "\n",
    "**gamma = 5**   <-- Simulates near-uniform participation, where all clients have almost equal probabilities of being selected.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" \n",
    "gamma = 0.05, first hyperparameter tuning with 100 rounds, then training with 2000 rounds and testing\n",
    "\"\"\"\n",
    "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
    "lr_values = np.logspace(-3, -1, num=3)\n",
    "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
    "wd_values = np.logspace(-4, -1, num=4)\n",
    "rounds = 100 #fewer communication rounds for hyperparameter tuning\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for lr in lr_values:\n",
    "    for wd in wd_values:\n",
    "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "        global_model = LeNet5() \n",
    "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS,log_freq=10, detailed_print=False,gamma=0.05)\n",
    "        plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"Federatedgamma005Tuning_lr_{lr}_wd_{wd}.png\")\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (lr,wd)\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "wd = 0.001\n",
    "global_model = LeNet5() \n",
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS,log_freq=10, detailed_print=False,gamma=0.05)\n",
    "test_accuracy = test(global_model, testloader)\n",
    "plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"Federatedgamma005_lr_{lr}_wd_{wd}.png\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#If needed for future plots or analysis, no need to train again\n",
    "save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses,client_selection_count, \"Federatedgamma005.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" \n",
    "gamma = 0.5, first hyperparameter tuning with 100 rounds, then training with 2000 rounds and testing\n",
    "\"\"\"\n",
    "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
    "lr_values = np.logspace(-3, -1, num=3)\n",
    "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
    "wd_values = np.logspace(-4, -1, num=4)\n",
    "rounds = 100 #fewer communication rounds for hyperparameter tuning\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for lr in lr_values:\n",
    "    for wd in wd_values:\n",
    "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "        global_model = LeNet5() \n",
    "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS,log_freq=10, detailed_print=False,gamma=0.5)\n",
    "        plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"Federatedgamma05Tuning_lr_{lr}_wd_{wd}.png\")\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (lr,wd)\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "wd = 0.001\n",
    "global_model = LeNet5() \n",
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS,log_freq=10, detailed_print=False,gamma=0.5)\n",
    "test_accuracy = test(global_model, testloader)\n",
    "plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"Federatedgamma05_lr_{lr}_wd_{wd}.png\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#If needed for future plots or analysis, no need to train again\n",
    "save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses,client_selection_count, \"Federatedgamma05.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" \n",
    "gamma = 1, first hyperparameter tuning with 100 rounds, then training with 2000 rounds and testing\n",
    "\"\"\"\n",
    "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
    "lr_values = np.logspace(-3, -1, num=3)\n",
    "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
    "wd_values = np.logspace(-4, -1, num=4)\n",
    "rounds = 100 #fewer communication rounds for hyperparameter tuning\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for lr in lr_values:\n",
    "    for wd in wd_values:\n",
    "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "        global_model = LeNet5() \n",
    "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS,log_freq=10, detailed_print=False,gamma=1.0)\n",
    "        plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"Federatedgamma1Tuning_lr_{lr}_wd_{wd}.png\")\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (lr,wd)\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "wd = 0.001\n",
    "global_model = LeNet5() \n",
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS,log_freq=10, detailed_print=False,gamma=1.0)\n",
    "test_accuracy = test(global_model, testloader)\n",
    "plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"Federatedgamma1_lr_{lr}_wd_{wd}.png\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#If needed for future plots or analysis, no need to train again\n",
    "save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses,client_selection_count, \"Federatedgamma1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" \n",
    "gamma = 5, first hyperparameter tuning with 100 rounds, then training with 2000 rounds and testing\n",
    "\"\"\"\n",
    "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
    "lr_values = np.logspace(-3, -1, num=3)\n",
    "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
    "wd_values = np.logspace(-4, -1, num=4)\n",
    "rounds = 100 #fewer communication rounds for hyperparameter tuning\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for lr in lr_values:\n",
    "    for wd in wd_values:\n",
    "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "        global_model = LeNet5() \n",
    "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS,log_freq=10, detailed_print=False,gamma=5.0)\n",
    "        plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"Federatedgamma5Tuning_lr_{lr}_wd_{wd}.png\")\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (lr,wd)\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "wd = 0.001\n",
    "global_model = LeNet5() \n",
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS,log_freq=10, detailed_print=False,gamma=5.0)\n",
    "test_accuracy = test(global_model, testloader)\n",
    "plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"Federatedgamma5_lr_{lr}_wd_{wd}.png\")\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#If needed for future plots or analysis, no need to train again\n",
    "save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses,client_selection_count, \"Federatedgamma5.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests with J = 8 and J = 16\n",
    "Communication rounds accordingly reduced to 1000 and 500 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Hyperparameter tuning with J=8, 50 communication rounds\n",
    "\"\"\"\n",
    "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
    "lr_values = np.logspace(-3, -1, num=3)\n",
    "\n",
    "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
    "wd_values = np.logspace(-4, -1, num=4)\n",
    "\n",
    "print(\"Learning Rate Values (log-uniform):\", lr_values)\n",
    "print(\"Weight Decay Values (log-uniform):\", wd_values)\n",
    "\n",
    "rounds = 50 #fewer communication rounds for hyperparameter tuning\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for lr in lr_values:\n",
    "    for wd in wd_values:\n",
    "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "        global_model = LeNet5() \n",
    "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=8)\n",
    "        plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"FederatedTuningJequalto8_lr_{lr}_wd_{wd}.png\")\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (lr,wd)\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Hyperparameter tuning with J=16, 25 communication rounds\n",
    "\"\"\"\n",
    "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
    "lr_values = np.logspace(-3, -1, num=3)\n",
    "\n",
    "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
    "wd_values = np.logspace(-4, -1, num=4)\n",
    "\n",
    "print(\"Learning Rate Values (log-uniform):\", lr_values)\n",
    "print(\"Weight Decay Values (log-uniform):\", wd_values)\n",
    "\n",
    "rounds = 25 #fewer communication rounds for hyperparameter tuning\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for lr in lr_values:\n",
    "    for wd in wd_values:\n",
    "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "        global_model = LeNet5() \n",
    "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=16)\n",
    "        plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"FederatedTuningJequalto16_lr_{lr}_wd_{wd}.png\")\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (lr,wd)\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "8 local steps, 1000 rounds\n",
    "\"\"\"\n",
    "#lr and wd to be defined based on the hyperparameter tuning\n",
    "#lr = 0.1\n",
    "#wd = 0.001\n",
    "global_model = LeNet5() \n",
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=1000, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=8)\n",
    "test_accuracy = test(global_model, testloader)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#If needed for future plots or analysis, no need to train again\n",
    "save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses,client_selection_count, \"FederatedJequalto8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "16 local steps, 500 rounds\n",
    "\"\"\"\n",
    "#lr and wd to be defined based on the hyperparameter tuning\n",
    "#lr = 0.1\n",
    "#wd = 0.001\n",
    "global_model = LeNet5() \n",
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=500, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=16)\n",
    "test_accuracy = test(global_model, testloader)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#If needed for future plots or analysis, no need to train again\n",
    "save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses,client_selection_count, \"FederatedJequalto16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate heterogeneous distributions\n",
    "#### (15 Exeperiments)\n",
    "Fix K=100 and C=0.1, and simulate several non-iid shardings of the training set of CIFAR-100, by fixing the number of different labels clients have (Nc={1,5,10,50}). Then test the performance of FedAvg, comparing with the iid sharding, varying the number of local steps J={4,8,16}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters tuning function\n",
    "def hyperparameters_tuning(num_classes, local_steps, rounds):\n",
    "    print(f\"Hyperparameter tuning for num_classes={num_classes}, local_steps={local_steps}\")\n",
    "    lr_values = np.logspace(-3, -1, num=3)\n",
    "    wd_values = np.logspace(-4, -1, num=4)\n",
    "    best_val_accuracy = 0\n",
    "    best_setting = None\n",
    "    for lr in lr_values:\n",
    "        for wd in wd_values:\n",
    "            print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "            global_model = LeNet5()\n",
    "            server = Server(global_model, DEVICE, CHECKPOINT_DIR)\n",
    "            global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=num_classes, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=local_steps,log_freq=100, detailed_print=False,gamma=None)\n",
    "            plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"FederatedTuning_Nc_{num_classes}_J_{local_steps}_lr_{lr}_wd_{wd}.png\")\n",
    "            print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "            max_val_accuracy = max(val_accuracies)\n",
    "            if max_val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = max_val_accuracy\n",
    "                best_setting = (lr,wd)\n",
    "    print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")\n",
    "    return best_setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "LOCAL_STEPS_VALUES = [4, 8, 16]  # Values for J (number of local steps)\n",
    "NUM_CLASSES_VALUES = [1, 5, 10, 50]  # Number of classes per client for Non-IID\n",
    "NUM_RUNDS = {4: 2000, 8: 1000, 16:500}\n",
    "IID_CLASSES = 100  # Full IID distribution\n",
    "\n",
    "# Function to perform the training and testing for a given configuration\n",
    "def run_experiment(num_classes, local_steps, plot_suffix):\n",
    "    print(f\"Running experiment: num_classes={num_classes}, local_steps={local_steps}\")\n",
    "    global_model = LeNet5()\n",
    "    server = Server(global_model, DEVICE, CHECKPOINT_DIR)\n",
    "\n",
    "    best_lr, best_wd = hyperparameters_tuning(num_classes = num_classes, local_steps=local_steps, rounds=(NUM_RUNDS[local_steps]/20))\n",
    "\n",
    "    global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(\n",
    "        criterion, trainloader, validloader, \n",
    "        num_clients=NUM_CLIENTS, num_classes=num_classes, \n",
    "        rounds=NUM_RUNDS[local_steps], lr=best_lr, momentum=MOMENTUM, \n",
    "        batchsize=BATCH_SIZE, wd=best_wd, C=FRACTION_CLIENTS, \n",
    "        local_steps=local_steps, log_freq=100, \n",
    "        detailed_print=False, gamma=None  # No skewed sampling for this experiment\n",
    "    )\n",
    "\n",
    "    # Testing and plotting\n",
    "    test_accuracy = test(global_model, testloader)\n",
    "    plot_metrics(train_accuracies, train_losses, val_accuracies, val_losses, f\"Federated_{plot_suffix}.png\")\n",
    "    print(f\"Test accuracy for num_classes={num_classes}, local_steps={local_steps}: {test_accuracy}\")\n",
    "    \n",
    "    # Save data for future analysis\n",
    "    save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count, f\"Federated_{plot_suffix}.pth\")\n",
    "\n",
    "# Main experiment loop\n",
    "for num_classes in NUM_CLASSES_VALUES + [IID_CLASSES]:  # Include IID setting\n",
    "    for local_steps in LOCAL_STEPS_VALUES:\n",
    "        plot_suffix = f\"num_classes_{num_classes}_local_steps_{local_steps}\"\n",
    "        run_experiment(num_classes, local_steps, plot_suffix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
