{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Project\n",
    "This notebook demonstrates how to set up and compare Federated Learning (FL) with Centralized Learning (CL) using the CIFAR-100 dataset and the modified version of the LeNet-5 model taken from [Hsu et al., Federated Visual Classification with Real-World Data Distribution, ECCV 2020]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We start by importing necessary libraries and setting global constants for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'checkpointing_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/cifar100/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcifar100_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CIFAR100DataLoader\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfederated_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_federated,plot_client_selection,test,plot_metrics,save_model,load_model\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\OneDrive\\Desktop\\AML_FederatedLearning\\cifar100\\utils\\federated_utils.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcheckpointing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_checkpoint, save_checkpoint\n\u001b[0;32m     17\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()  \u001b[38;5;66;03m# our loss function for classification tasks on CIFAR-100\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'checkpointing_utils'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.model import LeNet5 #import the model\n",
    "import numpy as np\n",
    "sys.path.append('../data/cifar100/')\n",
    "from cifar100_loader import CIFAR100DataLoader\n",
    "from Server import Server\n",
    "from utils.federated_utils import plot_metrics,test, plot_client_selection,save_data,load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for FL training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "\n",
    "NUM_CLIENTS = 100  # Total number of clients in the federation\n",
    "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
    "LOCAL_STEPS = 4  # Number of local steps (J)\n",
    "GLOBAL_ROUNDS = 2000  # Total number of communication rounds\n",
    "\n",
    "BATCH_SIZE = 64  # Batch size for local training\n",
    "LR = 1e-3  # Initial learning rate for local optimizers\n",
    "MOMENTUM = 0.9  # Momentum for SGD optimizer\n",
    "WEIGHT_DECAY = 0.0001  # Regularization term for local training\n",
    "CHECKPOINT_DIR = './checkpoints/'\n",
    "LOG_FREQUENCY = 10  # Frequency of logging training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "We load the CIFAR-100 dataset and split it into training, validation, and test sets. This is done using the `data_loader.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10% of the dataset kept for validation\n",
    "data_loader = CIFAR100DataLoader(batch_size=BATCH_SIZE, validation_split=0.1, download=True, num_workers=4, pin_memory=True)\n",
    "trainloader, validloader, testloader = data_loader.train_loader, data_loader.val_loader, data_loader.test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Federated Training\n",
    "We simulate federated learning by splitting the dataset into shards and training with selected clients in each round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = LeNet5()\n",
    "criterion = nn.NLLLoss()# our loss function for classification tasks on CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate Values (log-uniform): [0.0001     0.00316228 0.1       ]\n",
      "Weight Decay Values (log-uniform): [0.0001     0.00316228 0.1       ]\n"
     ]
    }
   ],
   "source": [
    "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
    "lr_values = np.logspace(-3, -1, num=3)\n",
    "\n",
    "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
    "wd_values = np.logspace(-4, -1, num=4)\n",
    "\n",
    "print(\"Learning Rate Values (log-uniform):\", lr_values)\n",
    "print(\"Weight Decay Values (log-uniform):\", wd_values)\n",
    "\n",
    "rounds = 100 #fewer communication rounds for hyperparameter tuning\n",
    "results = []\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for lr in lr_values:\n",
    "    for wd in wd_values:\n",
    "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
    "        global_model = LeNet5() \n",
    "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS)\n",
    "        plot_metrics(val_accuracies, val_losses, train_accuracies, train_losses, f\"FederatedBaselineTuning_lr_{lr}_wd_{wd}\")\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (lr,wd)\n",
    "        results.append({\n",
    "                'learning_rate': lr,\n",
    "                'weight_decay': wd,\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'val_losses': val_losses,\n",
    "                'client_selection_count': client_selection_count\n",
    "        })\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr  taken from the best values of the previous step, after we have checked there's no overfitting\n",
    "#wd\n",
    "global_model = LeNet5() \n",
    "server = Server(global_model, DEVICE, CHECKPOINT_DIR)                                                                   \n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS)\n",
    "test_accuracy = test(global_model, testloader)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#If needed for future plots or analysis, no need to train again\n",
    "save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses, \"FederatedBaseline\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for the first federated training baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [0.05, 0.01, 0.005, 0.0001]\n",
    "wd = [0.001, 0.0005, 0.0001]\n",
    "rounds = 100 #fewer communication rounds for hyperparameter tuning\n",
    "results = []\n",
    "best_val_accuracy = 0\n",
    "best_setting = None\n",
    "for l in lr:\n",
    "    for w in wd:\n",
    "        print(f\"Learning rate: {l}, Weight decay: {w}\")\n",
    "        global_model = LeNet5()\n",
    "        #global_model,dataset, valid_dataset, num_clients,num_classes, rounds,lr,wd, C=0.1, local_steps=4,gamma=None\n",
    "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = train_federated(global_model, criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS, log_freq=LOG_FREQUENCY, detailed_print=True)\n",
    "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {l} and wd: {w}\")\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        if max_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = max_val_accuracy\n",
    "            best_setting = (l,w)\n",
    "        results.append({\n",
    "                'learning_rate': l,\n",
    "                'weight_decay': w,\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'train_losses': train_losses,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'val_losses': val_losses,\n",
    "                'client_selection_count': client_selection_count\n",
    "        })\n",
    "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")\n",
    "#filter the result with the best setting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2000 rounds using lr and wd found in the step before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr =\n",
    "#wd = \n",
    "\n",
    "#just for now\n",
    "lr = LR\n",
    "wd = WEIGHT_DECAY                                                                                           \n",
    "\n",
    "# Run Federated Learning\n",
    "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = train_federated(global_model, criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=100, rounds=GLOBAL_ROUNDS, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS, log_freq=LOG_FREQUENCY, detailed_print=True)\n",
    "\n",
    "plot_client_selection(client_selection_count, \"clientDistribution_iid\")\n",
    "test_accuracy = test(global_model, testloader)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "#Plot also training accuracy against validation accuracy and validation loss against training loss\n",
    "plot_metrics(train_accuracies, val_accuracies, train_losses, val_losses, \"iid\")\n",
    "#Save the model for the future\n",
    "save_model(global_model, \"iid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
