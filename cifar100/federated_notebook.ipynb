{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Project\n",
    "This notebook demonstrates how to set up and compare Federated Learning (FL) with Centralized Learning (CL) using the CIFAR-100 dataset and the LeNet-5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We start by importing necessary libraries and setting global constants for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from statistics import mean\n",
    "\n",
    "from models.model import LeNet5 #import the model\n",
    "\n",
    "sys.path.append('../data/cifar100/')\n",
    "from cifar100_loader import load_cifar100\n",
    "\n",
    "from federated_utils import sharding, client_update, fedavg_aggregate\n",
    "\n",
    "# Constants for FL training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLIENTS = 100  # Total number of clients in the federation\n",
    "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
    "LOCAL_EPOCHS = 4  # Number of local steps (J)\n",
    "GLOBAL_ROUNDS = 2000  # Total number of communication rounds\n",
    "\n",
    "BATCH_SIZE = 64  # Batch size for local training\n",
    "LR = 0.005  # Initial learning rate for local optimizers: best one from the centralized one\n",
    "MOMENTUM = 0.9  # Momentum for SGD optimizer\n",
    "WEIGHT_DECAY = 5e-5  # Regularization term for local training\n",
    "\n",
    "LOG_FREQUENCY = 10  # Frequency of logging training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "We load the CIFAR-100 dataset and split it into training, validation, and test sets. This is done using the `data_loader.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data loaded successfully!\n",
      "\n",
      "Dimension of the training dataset: 37500\n",
      "Dimension of the validation dataset: 12500\n",
      "Dimension of the test dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "trainloader, validloader, testloader = load_cifar100(batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "\n",
    "print(\"Data loaded successfully!\\n\")\n",
    "print(\"Dimension of the training dataset:\", len(trainloader.dataset))\n",
    "print(\"Dimension of the validation dataset:\", len(validloader.dataset))\n",
    "print(\"Dimension of the test dataset:\", len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Federated Training\n",
    "We simulate federated learning by splitting the dataset into shards and training with selected clients in each round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = LeNet5()\n",
    "criterion = nn.NLLLoss()# our loss function for classification tasks on CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Learning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated Learning Training Loop\n",
    "def federated_training(global_model, criterion, dataset, valid_dataset, num_clients, rounds, C=0.1, local_steps=4,detailed_print=False):\n",
    "    shards = sharding(dataset, num_clients) #each shard represent the training data for one client\n",
    "    client_sizes = [len(shard) for shard in shards]\n",
    "\n",
    "    global_model.to(DEVICE) #as alwayse, we move the global model to the specified device (CPU or GPU)\n",
    "\n",
    "    # ********************* HOW IT WORKS ***************************************\n",
    "    # The training runs for rounds iterations (GLOBAL_ROUNDS=2000)\n",
    "    # Each round simulates one communication step in federated learning, including:\n",
    "    # 1) client selection\n",
    "    # 2) local training (of each client)\n",
    "    # 3) central aggregation\n",
    "    for round_num in range(rounds):\n",
    "        if round_num % LOG_FREQUENCY == 0 and detailed_print:\n",
    "          print(f\"------------------------------------- Round {round_num} ------------------------------------------------\" )\n",
    "        # 1) client selection: In each round, a fraction C (e.g., 10%) of clients is randomly selected to participate.\n",
    "        #     This reduces computation costs and mimics real-world scenarios where not all devices are active.\n",
    "        selected_clients = random.sample(range(num_clients), int(C * num_clients))\n",
    "        client_states = []\n",
    "\n",
    "        # 2) local training: for each client updates the model using the client's data for local_steps epochs\n",
    "        for client_id in selected_clients:\n",
    "            local_model = deepcopy(global_model) #it creates a local copy of the global model\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY) #same of the centralized version\n",
    "            client_loader = DataLoader(shards[client_id], batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "            local_state = client_update(local_model, client_id, client_loader, criterion, optimizer, local_steps, round_num % LOG_FREQUENCY == 0 and detailed_print)\n",
    "            client_states.append(local_state)\n",
    "\n",
    "        # 3) central aggregation: aggregates participating client updates using fedavg_aggregate\n",
    "        #    and replaces the current parameters of global_model with the returned ones.\n",
    "        global_model.load_state_dict(fedavg_aggregate(global_model, client_states, [client_sizes[i] for i in selected_clients]))\n",
    "\n",
    "        # Validation at the server (optional, add metrics here)\n",
    "        if round_num % LOG_FREQUENCY == 0 and detailed_print:\n",
    "              print(f\"------------------------------ Round {round_num} terminated: model updated -----------------------------\\n\\n\" )\n",
    "        #if round_num % LOG_FREQUENCY:\n",
    "          #val_loss, val_accuracy = evaluate_model(global_model, valid_dataset, criterion)\n",
    "          #print(f\"Round {round_num}: Validation Loss = {val_loss:.4f}, Accuracy = {val_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    return global_model #the updated global model is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------- Round 0 ------------------------------------------------\n",
      "Client 26 --> Final Loss (Epoch 4): 4.602545261383057\n",
      "Client 13 --> Final Loss (Epoch 4): 4.594453811645508\n",
      "Client 31 --> Final Loss (Epoch 4): 4.611208915710449\n",
      "Client 39 --> Final Loss (Epoch 4): 4.608333110809326\n",
      "Client 4 --> Final Loss (Epoch 4): 4.599559307098389\n",
      "Client 58 --> Final Loss (Epoch 4): 4.595484733581543\n",
      "Client 49 --> Final Loss (Epoch 4): 4.6042890548706055\n",
      "Client 98 --> Final Loss (Epoch 4): 4.601893901824951\n",
      "Client 92 --> Final Loss (Epoch 4): 4.609266757965088\n",
      "Client 15 --> Final Loss (Epoch 4): 4.601667881011963\n",
      "------------------------------ Round 0 terminated: model updated -----------------------------\n",
      "\n",
      "\n",
      "------------------------------------- Round 10 ------------------------------------------------\n",
      "Client 43 --> Final Loss (Epoch 4): 4.610133171081543\n",
      "Client 20 --> Final Loss (Epoch 4): 4.604770660400391\n",
      "Client 99 --> Final Loss (Epoch 4): 4.592456340789795\n",
      "Client 68 --> Final Loss (Epoch 4): 4.6012654304504395\n",
      "Client 69 --> Final Loss (Epoch 4): 4.6004252433776855\n",
      "Client 65 --> Final Loss (Epoch 4): 4.595179080963135\n",
      "Client 75 --> Final Loss (Epoch 4): 4.59911584854126\n",
      "Client 46 --> Final Loss (Epoch 4): 4.599564552307129\n",
      "Client 95 --> Final Loss (Epoch 4): 4.608823299407959\n",
      "Client 83 --> Final Loss (Epoch 4): 4.592790603637695\n",
      "------------------------------ Round 10 terminated: model updated -----------------------------\n",
      "\n",
      "\n",
      "------------------------------------- Round 20 ------------------------------------------------\n",
      "Client 37 --> Final Loss (Epoch 4): 4.5966291427612305\n",
      "Client 81 --> Final Loss (Epoch 4): 4.5877461433410645\n",
      "Client 4 --> Final Loss (Epoch 4): 4.592054843902588\n",
      "Client 25 --> Final Loss (Epoch 4): 4.583250522613525\n",
      "Client 89 --> Final Loss (Epoch 4): 4.594593524932861\n",
      "Client 7 --> Final Loss (Epoch 4): 4.578869819641113\n",
      "Client 66 --> Final Loss (Epoch 4): 4.605685710906982\n",
      "Client 49 --> Final Loss (Epoch 4): 4.589116096496582\n",
      "Client 70 --> Final Loss (Epoch 4): 4.597915172576904\n",
      "Client 19 --> Final Loss (Epoch 4): 4.5942230224609375\n",
      "------------------------------ Round 20 terminated: model updated -----------------------------\n",
      "\n",
      "\n",
      "------------------------------------- Round 30 ------------------------------------------------\n",
      "Client 15 --> Final Loss (Epoch 4): 4.563234806060791\n",
      "Client 2 --> Final Loss (Epoch 4): 4.576802730560303\n",
      "Client 74 --> Final Loss (Epoch 4): 4.549919128417969\n",
      "Client 83 --> Final Loss (Epoch 4): 4.591457843780518\n",
      "Client 54 --> Final Loss (Epoch 4): 4.5360283851623535\n",
      "Client 68 --> Final Loss (Epoch 4): 4.593367099761963\n",
      "Client 12 --> Final Loss (Epoch 4): 4.574804306030273\n",
      "Client 31 --> Final Loss (Epoch 4): 4.5651631355285645\n",
      "Client 58 --> Final Loss (Epoch 4): 4.555427074432373\n",
      "Client 72 --> Final Loss (Epoch 4): 4.560564994812012\n",
      "------------------------------ Round 30 terminated: model updated -----------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run Federated Learning\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m refined_model \u001b[38;5;241m=\u001b[39m \u001b[43mfederated_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CLIENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGLOBAL_ROUNDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFRACTION_CLIENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetailed_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 28\u001b[0m, in \u001b[0;36mfederated_training\u001b[1;34m(global_model, criterion, dataset, valid_dataset, num_clients, rounds, C, local_steps, detailed_print)\u001b[0m\n\u001b[0;32m     25\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(local_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR, momentum\u001b[38;5;241m=\u001b[39mMOMENTUM, weight_decay\u001b[38;5;241m=\u001b[39mWEIGHT_DECAY) \u001b[38;5;66;03m#same of the centralized version\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     client_loader \u001b[38;5;241m=\u001b[39m DataLoader(shards[client_id], batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 28\u001b[0m     local_state \u001b[38;5;241m=\u001b[39m \u001b[43mclient_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLOG_FREQUENCY\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdetailed_print\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     client_states\u001b[38;5;241m.\u001b[39mappend(local_state)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 3) central aggregation: aggregates participating client updates using fedavg_aggregate\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#    and replaces the current parameters of global_model with the returned ones.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\OneDrive\\Desktop\\AML_FederatedLearning\\cifar100\\federated_utils.py:79\u001b[0m, in \u001b[0;36mclient_update\u001b[1;34m(model, client_id, client_data, criterion, optimizer, local_steps, detailed_print)\u001b[0m\n\u001b[0;32m     76\u001b[0m model.train()  # Set the model to training mode\n\u001b[0;32m     77\u001b[0m for epoch in range(local_steps):\n\u001b[0;32m     78\u001b[0m     for data, targets in client_data:\n\u001b[1;32m---> 79\u001b[0m         # Move data and targets to the specified device (e.g., GPU or CPU)\n\u001b[0;32m     80\u001b[0m         data, targets = data.to(DEVICE), targets.to(DEVICE)\n\u001b[0;32m     82\u001b[0m         # Reset the gradients before backpropagation\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T_co]:\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Stefano\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 172\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Federated Learning\n",
    "refined_model = federated_training(global_model, criterion, trainloader.dataset, validloader.dataset, num_clients=NUM_CLIENTS, rounds=GLOBAL_ROUNDS, C=FRACTION_CLIENTS, local_steps=LOCAL_EPOCHS, detailed_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.train(False) # Set Network to evaluation mode\n",
    "        running_corrects = 0\n",
    "        losses = []\n",
    "        for data, targets in dataloader:\n",
    "            data = data.to(DEVICE)        # Move the data to the GPU\n",
    "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
    "            # Forward Pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            # Update Corrects\n",
    "            running_corrects += torch.sum(preds == targets.data).data.item()\n",
    "            # Calculate Accuracy\n",
    "            accuracy = running_corrects / float(len(dataloader.dataset))\n",
    "\n",
    "    return accuracy, mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(refined_model, testloader, criterion)[0]\n",
    "print('\\nTest Accuracy: {}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
