{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Project\n",
    "This notebook demonstrates how to set up and compare Federated Learning (FL) with Centralized Learning (CL) using the CIFAR-100 dataset and the LeNet-5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We start by importing necessary libraries and setting global constants for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LeNet5 \u001b[38;5;66;03m#import the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/cifar100/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcifar100_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_cifar100\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Constants for FL training\u001b[39;00m\n\u001b[0;32m     16\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\Desktop\\POLITO\\AdvancedMachineLearning\\Project_Federated_Learning\\AML_FederatedLearning\\cifar100\\../data/cifar100\\cifar100_loader.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Subset\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_cifar100\u001b[39m(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      7\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5071\u001b[39m, \u001b[38;5;241m0.4867\u001b[39m, \u001b[38;5;241m0.4408\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.2675\u001b[39m, \u001b[38;5;241m0.2565\u001b[39m, \u001b[38;5;241m0.2761\u001b[39m])\n\u001b[0;32m     10\u001b[0m     ])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from statistics import mean\n",
    "\n",
    "from models.model import LeNet5 #import the model\n",
    "\n",
    "sys.path.append('../data/cifar100/')\n",
    "from cifar100_loader import load_cifar100\n",
    "\n",
    "from federated_utils import shard_dataset_iid, client_update, fedavg_aggregate\n",
    "\n",
    "# Constants for FL training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLIENTS = 100  # Total number of clients in the federation\n",
    "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
    "LOCAL_EPOCHS = 4  # Number of local steps (J)\n",
    "GLOBAL_ROUNDS = 200  # Total number of communication rounds\n",
    "\n",
    "BATCH_SIZE = 64  # Batch size for local training\n",
    "LR = 1e-3  # Initial learning rate for local optimizers\n",
    "MOMENTUM = 0.9  # Momentum for SGD optimizer\n",
    "WEIGHT_DECAY = 5e-5  # Regularization term for local training\n",
    "\n",
    "LOG_FREQUENCY = 10  # Frequency of logging training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "We load the CIFAR-100 dataset and split it into training, validation, and test sets. This is done using the `data_loader.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "trainloader, validloader, testloader = load_cifar100(batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "\n",
    "print(\"Data loaded successfully!\\n\")\n",
    "print(\"Dimension of the training dataset:\", len(trainloader.dataset))\n",
    "print(\"Dimension of the validation dataset:\", len(validloader.dataset))\n",
    "print(\"Dimension of the test dataset:\", len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Federated Training\n",
    "We simulate federated learning by splitting the dataset into shards and training with selected clients in each round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = LeNet5()\n",
    "criterion = nn.NLLLoss()# our loss function for classification tasks on CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Learning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated Learning Training Loop\n",
    "def federated_training(global_model, criterion, dataset, valid_dataset, num_clients, rounds, C=0.1, local_steps=4,detailed_print=False):\n",
    "    shards = shard_dataset_iid(dataset, num_clients) #each shard represent the training data for one client\n",
    "    client_sizes = [len(shard) for shard in shards]\n",
    "\n",
    "    global_model.to(DEVICE) #as alwayse, we move the global model to the specified device (CPU or GPU)\n",
    "\n",
    "    # ********************* HOW IT WORKS ***************************************\n",
    "    # The training runs for rounds iterations (GLOBAL_ROUNDS=2000)\n",
    "    # Each round simulates one communication step in federated learning, including:\n",
    "    # 1) client selection\n",
    "    # 2) local training (of each client)\n",
    "    # 3) central aggregation\n",
    "    for round_num in range(rounds):\n",
    "        if round_num % LOG_FREQUENCY == 0 and detailed_print:\n",
    "          print(f\"------------------------------------- Round {round_num} ------------------------------------------------\" )\n",
    "        # 1) client selection: In each round, a fraction C (e.g., 10%) of clients is randomly selected to participate.\n",
    "        #     This reduces computation costs and mimics real-world scenarios where not all devices are active.\n",
    "        selected_clients = random.sample(range(num_clients), int(C * num_clients))\n",
    "        client_states = []\n",
    "\n",
    "        # 2) local training: for each client updates the model using the client's data for local_steps epochs\n",
    "        for client_id in selected_clients:\n",
    "            local_model = deepcopy(global_model) #it creates a local copy of the global model\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY) #same of the centralized version\n",
    "            client_loader = DataLoader(shards[client_id], batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "            local_state = client_update(local_model, client_id, client_loader, criterion, optimizer, local_steps, round_num % LOG_FREQUENCY == 0 and detailed_print)\n",
    "            client_states.append(local_state)\n",
    "\n",
    "        # 3) central aggregation: aggregates participating client updates using fedavg_aggregate\n",
    "        #    and replaces the current parameters of global_model with the returned ones.\n",
    "        global_model.load_state_dict(fedavg_aggregate(global_model, client_states, [client_sizes[i] for i in selected_clients]))\n",
    "\n",
    "        # Validation at the server (optional, add metrics here)\n",
    "        if round_num % LOG_FREQUENCY == 0 and detailed_print:\n",
    "              print(f\"------------------------------ Round {round_num} terminated: model updated -----------------------------\\n\\n\" )\n",
    "        #if round_num % LOG_FREQUENCY:\n",
    "          #val_loss, val_accuracy = evaluate_model(global_model, valid_dataset, criterion)\n",
    "          #print(f\"Round {round_num}: Validation Loss = {val_loss:.4f}, Accuracy = {val_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    return global_model #the updated global model is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Federated Learning\n",
    "refined_model = federated_training(global_model, criterion, trainloader.dataset, validloader.dataset, num_clients=NUM_CLIENTS, rounds=GLOBAL_ROUNDS, C=FRACTION_CLIENTS, local_steps=LOCAL_EPOCHS, detailed_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.train(False) # Set Network to evaluation mode\n",
    "        running_corrects = 0\n",
    "        losses = []\n",
    "        for data, targets in dataloader:\n",
    "            data = data.to(DEVICE)        # Move the data to the GPU\n",
    "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
    "            # Forward Pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            # Update Corrects\n",
    "            running_corrects += torch.sum(preds == targets.data).data.item()\n",
    "            # Calculate Accuracy\n",
    "            accuracy = running_corrects / float(len(dataloader.dataset))\n",
    "\n",
    "    return accuracy, mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(refined_model, testloader, criterion)[0]\n",
    "print('\\nTest Accuracy: {}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
