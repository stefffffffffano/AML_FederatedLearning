{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Project\n",
    "This notebook demonstrates how to set up and compare Federated Learning (FL) with Centralized Learning (CL) using the CIFAR-100 dataset and the LeNet-5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We start by importing necessary libraries and setting global constants for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from statistics import mean\n",
    "\n",
    "from models.model import LeNet5 #import the model\n",
    "\n",
    "sys.path.append('../data/cifar100/')\n",
    "from cifar100_loader import load_cifar100\n",
    "\n",
    "from federated_utils import sharding, client_selection, client_update, fedavg_aggregate\n",
    "\n",
    "# Constants for FL training\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLIENTS = 100  # Total number of clients in the federation\n",
    "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
    "LOCAL_EPOCHS = 4  # Number of local steps (J)\n",
    "GLOBAL_ROUNDS = 2000  # Total number of communication rounds\n",
    "\n",
    "BATCH_SIZE = 64  # Batch size for local training\n",
    "LR = 0.005  # Initial learning rate for local optimizers: best one from the centralized one\n",
    "MOMENTUM = 0.9  # Momentum for SGD optimizer\n",
    "WEIGHT_DECAY = 5e-5  # Regularization term for local training\n",
    "\n",
    "LOG_FREQUENCY = 10  # Frequency of logging training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "We load the CIFAR-100 dataset and split it into training, validation, and test sets. This is done using the `data_loader.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data loaded successfully!\n",
      "\n",
      "Dimension of the training dataset: 37500\n",
      "Dimension of the validation dataset: 12500\n",
      "Dimension of the test dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "trainloader, validloader, testloader = load_cifar100(batch_size=BATCH_SIZE, validation_split=0.25)\n",
    "\n",
    "print(\"Data loaded successfully!\\n\")\n",
    "print(\"Dimension of the training dataset:\", len(trainloader.dataset))\n",
    "print(\"Dimension of the validation dataset:\", len(validloader.dataset))\n",
    "print(\"Dimension of the test dataset:\", len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Federated Training\n",
    "We simulate federated learning by splitting the dataset into shards and training with selected clients in each round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.train(False) # Set Network to evaluation mode\n",
    "        running_corrects = 0\n",
    "        losses = []\n",
    "        for data, targets in dataloader:\n",
    "            data = data.to(DEVICE)        # Move the data to the GPU\n",
    "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
    "            # Forward Pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            # Update Corrects\n",
    "            running_corrects += torch.sum(preds == targets.data).data.item()\n",
    "            # Calculate Accuracy\n",
    "            accuracy = running_corrects / float(len(dataloader.dataset))\n",
    "\n",
    "    return accuracy, mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = LeNet5()\n",
    "criterion = nn.NLLLoss()# our loss function for classification tasks on CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Learning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated Learning Training Loop\n",
    "def federated_training(global_model, criterion, dataset, valid_dataset, num_clients, rounds, C=0.1, local_steps=4,detailed_print=False):\n",
    "    shards = sharding(dataset, num_clients) #each shard represent the training data for one client\n",
    "    client_sizes = [len(shard) for shard in shards]\n",
    "\n",
    "    global_model.to(DEVICE) #as alwayse, we move the global model to the specified device (CPU or GPU)\n",
    "\n",
    "    # ********************* HOW IT WORKS ***************************************\n",
    "    # The training runs for rounds iterations (GLOBAL_ROUNDS=2000)\n",
    "    # Each round simulates one communication step in federated learning, including:\n",
    "    # 1) client selection\n",
    "    # 2) local training (of each client)\n",
    "    # 3) central aggregation\n",
    "    for round_num in range(rounds):\n",
    "        if round_num % LOG_FREQUENCY == 0 and detailed_print:\n",
    "          print(f\"------------------------------------- Round {round_num} ------------------------------------------------\" )\n",
    "        # 1) client selection: In each round, a fraction C (e.g., 10%) of clients is randomly selected to participate.\n",
    "        #     This reduces computation costs and mimics real-world scenarios where not all devices are active.\n",
    "        selected_clients = client_selection(num_clients, C) \n",
    "        client_states = []\n",
    "\n",
    "        # 2) local training: for each client updates the model using the client's data for local_steps epochs\n",
    "        for client_id in selected_clients:\n",
    "            local_model = deepcopy(global_model) #it creates a local copy of the global model\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY) #same of the centralized version\n",
    "            client_loader = DataLoader(shards[client_id], batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "            local_state = client_update(local_model, client_id, client_loader, criterion, optimizer, local_steps, round_num % LOG_FREQUENCY == 0 and detailed_print)\n",
    "            client_states.append(local_state)\n",
    "\n",
    "        # 3) central aggregation: aggregates participating client updates using fedavg_aggregate\n",
    "        #    and replaces the current parameters of global_model with the returned ones.\n",
    "        global_model.load_state_dict(fedavg_aggregate(global_model, client_states, [client_sizes[i] for i in selected_clients]))\n",
    "\n",
    "        # Validation done server side on the validation dataset using the global model, printed every LOG_FREQUENCY rounds\n",
    "        if round_num % LOG_FREQUENCY == 0 and detailed_print:\n",
    "          print(f\"------------------------------ Round {round_num} terminated: model updated -----------------------------\\n\\n\" )\n",
    "          val_accuracy, val_loss = evaluate(global_model, valid_dataset, criterion)\n",
    "          print(f\"Round {round_num}: Validation Loss = {val_loss:.4f}, Accuracy = {val_accuracy*100:.2f}%\")\n",
    "\n",
    "\n",
    "    return global_model #the updated global model is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------- Round 0 ------------------------------------------------\n",
      "Client 52 --> Final Loss (Epoch 4): 4.588075160980225\n",
      "Client 16 --> Final Loss (Epoch 4): 4.582740306854248\n",
      "Client 25 --> Final Loss (Epoch 4): 4.6108503341674805\n",
      "Client 43 --> Final Loss (Epoch 4): 4.600342273712158\n",
      "Client 65 --> Final Loss (Epoch 4): 4.5865702629089355\n",
      "Client 27 --> Final Loss (Epoch 4): 4.587915420532227\n",
      "Client 9 --> Final Loss (Epoch 4): 4.598239421844482\n",
      "Client 87 --> Final Loss (Epoch 4): 4.603259086608887\n",
      "Client 79 --> Final Loss (Epoch 4): 4.5913004875183105\n",
      "Client 35 --> Final Loss (Epoch 4): 4.595902442932129\n",
      "------------------------------ Round 0 terminated: model updated -----------------------------\n",
      "\n",
      "\n",
      "Round 0: Validation Loss = 4.6054, Accuracy = 1.39%\n"
     ]
    }
   ],
   "source": [
    "# Run Federated Learning\n",
    "refined_model = federated_training(global_model, criterion, trainloader.dataset, validloader, num_clients=NUM_CLIENTS, rounds=GLOBAL_ROUNDS, C=FRACTION_CLIENTS, local_steps=LOCAL_EPOCHS, detailed_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(refined_model, testloader, criterion)[0]\n",
    "print('\\nTest Accuracy: {}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
