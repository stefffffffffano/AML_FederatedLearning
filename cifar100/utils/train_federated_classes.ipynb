{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "Ku-3jicvv8v0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../data/cifar100/')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from torch.utils.data import Subset\n",
        "from statistics import mean\n",
        "#from cifar100_loader import load_cifar100\n",
        "#from models.model import LeNet5 #import the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtzbKmCRJUa2"
      },
      "source": [
        "### Constants for FL training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WawTSpXyn5aS",
        "outputId": "a1168c20-e499-480b-e258-3b46a1250edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Constants for FL training\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(DEVICE)\n",
        "\n",
        "NUM_CLIENTS = 100  # Total number of clients in the federation\n",
        "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
        "LOCAL_STEPS = 4  # Number of local steps (J)\n",
        "GLOBAL_ROUNDS = 2000  # Total number of communication rounds\n",
        "\n",
        "BATCH_SIZE = 64  # Batch size for local training\n",
        "LR = 1e-2  # Initial learning rate for local optimizers\n",
        "MOMENTUM = 0.9  # Momentum for SGD optimizer\n",
        "WEIGHT_DECAY = 1e-4  # Regularization term for local training\n",
        "\n",
        "LOG_FREQUENCY = 10  # Frequency of logging training progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "146JFJRhwuyY"
      },
      "source": [
        "# Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "Ec9eETBgwy5L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class CIFAR100DataLoader:\n",
        "    def __init__(self, batch_size=32, validation_split=0.1, download=True, num_workers=4, pin_memory=True):\n",
        "        self.batch_size = batch_size\n",
        "        self.validation_split = validation_split\n",
        "        self.download = download\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        # Define transformations\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(24, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "        ])\n",
        "\n",
        "        self.test_transform = transforms.Compose([\n",
        "            transforms.CenterCrop(24),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "        ])\n",
        "\n",
        "        # Load datasets\n",
        "        self.train_loader, self.val_loader, self.test_loader = self._prepare_loaders()\n",
        "\n",
        "    def _prepare_loaders(self):\n",
        "        # Load the full training dataset\n",
        "        full_trainset = datasets.CIFAR100(root='./data', train=True, download=self.download, transform=self.train_transform)\n",
        "\n",
        "        # Split indices for training and validation\n",
        "        indexes = list(range(len(full_trainset)))\n",
        "        train_indexes, val_indexes = train_test_split(\n",
        "            indexes,\n",
        "            train_size=1 - self.validation_split,\n",
        "            test_size=self.validation_split,\n",
        "            random_state=42,\n",
        "            stratify=full_trainset.targets,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Create training and validation subsets\n",
        "        train_dataset = Subset(full_trainset, train_indexes)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=self.batch_size, shuffle=True,\n",
        "            num_workers=self.num_workers, pin_memory=self.pin_memory\n",
        "        )\n",
        "\n",
        "        full_trainset_val = datasets.CIFAR100(root='./data', train=True, download=self.download, transform=self.test_transform)\n",
        "        val_dataset = Subset(full_trainset_val, val_indexes)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=self.pin_memory\n",
        "        )\n",
        "\n",
        "        # Load the test dataset\n",
        "        testset = datasets.CIFAR100(root='./data', train=False, download=self.download, transform=self.test_transform)\n",
        "        test_loader = DataLoader(\n",
        "            testset, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=self.pin_memory\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Allows iteration over all loaders for unified access.\"\"\"\n",
        "        return iter([self.train_loader, self.val_loader, self.test_loader])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayrx5sBRKzQ5"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X8F2JHHKyly",
        "outputId": "9631806e-7110-4b70-bee6-b7c9411e24fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dimension of the training dataset: 45000\n",
            "Dimension of the validation dataset: 5000\n",
            "Dimension of the test dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "#10% of the dataset kept for validation\n",
        "data_loader = CIFAR100DataLoader(batch_size=32, validation_split=0.1, download=True, num_workers=2, pin_memory=True)\n",
        "trainloader, validloader, testloader = data_loader.train_loader, data_loader.val_loader, data_loader.test_loader\n",
        "\n",
        "print(\"Dimension of the training dataset:\", len(trainloader.dataset))\n",
        "print(\"Dimension of the validation dataset:\", len(validloader.dataset))\n",
        "print(\"Dimension of the test dataset:\", len(testloader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpHgxPxaBvmP"
      },
      "source": [
        "## Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "frTjN7V0B6ZG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Directory where checkpoints are stored\n",
        "CHECKPOINT_DIR = '../checkpoints/'\n",
        "\n",
        "# Ensure the checkpoint directory exists, creating it if necessary\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", checkpoint_data=None):\n",
        "    \"\"\"\n",
        "    Saves the model checkpoint and removes the previous one if it exists.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state is to be saved.\n",
        "    optimizer -- The optimizer whose state is to be saved (can be None).\n",
        "    epoch -- The current epoch of the training process.\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to save the checkpoint.\n",
        "    checkpoint_data -- Data to save in a JSON file (e.g., training logs).\n",
        "    \"\"\"\n",
        "    # Define the path for the subfolder where checkpoints will be stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    # Create the subfolder if it doesn't exist\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # Construct filenames for both the model checkpoint and the associated JSON file\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "    # Define the filenames for the previous checkpoint files, to remove them if necessary\n",
        "    previous_filepath = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.pth\")\n",
        "    previous_filepath_json = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.json\")\n",
        "\n",
        "    # Remove the previous checkpoint if it exists, but only for epochs greater than 1\n",
        "    if epoch > 1 and os.path.exists(previous_filepath):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Prepare the checkpoint data dictionary\n",
        "    checkpoint = {'model_state_dict': model.state_dict(), 'epoch': epoch}\n",
        "    # If an optimizer is provided, save its state as well\n",
        "    if optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    # Save the model and optimizer (if provided) state dictionary to the checkpoint file\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    # If additional data (e.g., training logs) is provided, save it to a JSON file\n",
        "    if checkpoint_data:\n",
        "        with open(filepath_json, 'w') as json_file:\n",
        "            json.dump(checkpoint_data, json_file, indent=4)\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Loads the latest checkpoint available based on the specified hyperparameters.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state will be updated from the checkpoint.\n",
        "    optimizer -- The optimizer whose state will be updated from the checkpoint (can be None).\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to look for checkpoints.\n",
        "\n",
        "    Returns:\n",
        "    The next epoch to resume from and the associated JSON data if available.\n",
        "    \"\"\"\n",
        "    # Define the path to the subfolder where checkpoints are stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "\n",
        "    # If the subfolder doesn't exist, print a message and start from epoch 1\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, starting from epoch 1.\")\n",
        "        return 1, None  # Epoch starts from 1\n",
        "\n",
        "    # Search for checkpoint files in the subfolder that match the hyperparameters\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "\n",
        "    # If checkpoint files are found, load the one with the highest epoch number\n",
        "    if files:\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))  # Find the latest epoch file\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath, weights_only=True)\n",
        "\n",
        "        # Load the model state from the checkpoint\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        # If an optimizer is provided, load its state as well\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Try to load the associated JSON file if available\n",
        "        json_filepath = os.path.join(subfolder_path, latest_file.replace('.pth', '.json'))\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            # If the JSON file exists, load its contents\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(\"Data loaded!\")\n",
        "        else:\n",
        "            # If no JSON file exists, print a message\n",
        "            print(\"No data found\")\n",
        "\n",
        "        # Print the epoch from which the model is resuming\n",
        "        print(f\"Checkpoint found: Resuming from epoch {checkpoint['epoch'] + 1}\\n\\n\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    # If no checkpoint is found, print a message and start from epoch 1\n",
        "    print(\"No checkpoint found, starting from epoch 1..\\n\\n\")\n",
        "    return 1, None  # Epoch starts from 1\n",
        "\n",
        "\n",
        "def delete_existing_checkpoints(subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Deletes all existing checkpoints in the specified subfolder.\n",
        "\n",
        "    Arguments:\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to delete checkpoints from.\n",
        "    \"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if os.path.exists(subfolder_path):\n",
        "        for file_name in os.listdir(subfolder_path):\n",
        "            file_path = os.path.join(subfolder_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        print(f\"All existing checkpoints in {subfolder_path} have been deleted.\")\n",
        "    else:\n",
        "        print(f\"No checkpoint folder found at {subfolder_path}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7bNGf9nwFcf"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "g6ArbBrCwG7l"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\"\"\"\n",
        "Model architecture for the CIFAR-100 dataset.\n",
        "The model is based on the LeNet-5 architecture with some modifications.\n",
        "Reference: Hsu et al., Federated Visual Classification with Real-World Data Distribution, ECCV 2020\n",
        "\n",
        "CNN similar to LeNet5 which has two 5×5, 64-channel convolution layers, each precedes a 2×2\n",
        "max-pooling layer, followed by two fully-connected layers with 384 and 192\n",
        "channels respectively and finally a softmax linear classifier\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\"\"\"\n",
        "Model architecture for the CIFAR-100 dataset.\n",
        "The model is based on the LeNet-5 architecture with some modifications.\n",
        "Reference: Hsu et al., Federated Visual Classification with Real-World Data Distribution, ECCV 2020\n",
        "\n",
        "CNN similar to LeNet5 which has two 5×5, 64-channel convolution layers, each precedes a 2×2\n",
        "max-pooling layer, followed by two fully-connected layers with 384 and 192\n",
        "channels respectively and finally a softmax linear classifier\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(64 * 3 * 3, 384),  # Updated to be consistent with data augmentation\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 192),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(192, 100)  # 100 classes for CIFAR-100\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output of the conv layers\n",
        "        x = self.fc_layer(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SveGFpEsxCkK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44DzyWDJJjs5"
      },
      "source": [
        "### Data Sharding for IID (Indipendent and Identically Distributed) FL Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "XoQkps10Irdl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sharding(dataset, number_of_clients, number_of_classes=100):\n",
        "    \"\"\"\n",
        "    Function that performs the sharding of the dataset given as input.\n",
        "    dataset: dataset to be split;\n",
        "    number_of_clients: the number of partitions we want to obtain;\n",
        "    number_of_classes: (int) the number of classes inside each partition, or 100 for IID;\n",
        "    \"\"\"\n",
        "\n",
        "    # Validation of input parameters\n",
        "    if not (1 <= number_of_classes <= 100):\n",
        "        raise ValueError(\"number_of_classes should be an integer between 1 and 100\")\n",
        "\n",
        "    # Shuffle dataset indices for randomness\n",
        "    indices = np.random.permutation(len(dataset))\n",
        "\n",
        "    # Compute basic partition sizes\n",
        "    basic_partition_size = len(dataset) // number_of_clients\n",
        "    remainder = len(dataset) % number_of_clients\n",
        "\n",
        "    shards = []\n",
        "    start_idx = 0\n",
        "\n",
        "    if number_of_classes == 100:  # IID Case\n",
        "        # Equally distribute indices among clients: we can just randomly assign to each client an equal amount of records\n",
        "        for i in range(number_of_clients):\n",
        "            end_idx = start_idx + basic_partition_size + (1 if i < remainder else 0)\n",
        "            shards.append(Subset(dataset, indices[start_idx:end_idx]))\n",
        "            start_idx = end_idx\n",
        "    else:  # non-IID Case\n",
        "        # Count of each class in the dataset\n",
        "        from collections import Counter\n",
        "        target_counts = Counter(target for _, target in dataset)\n",
        "\n",
        "        # Calculate per client class allocation\n",
        "        class_per_client = np.random.choice(list(target_counts.keys()), size=number_of_classes, replace=False)\n",
        "        class_idx = {class_: np.where([target == class_ for _, target in dataset])[0] for class_ in class_per_client}\n",
        "\n",
        "        # Assign class indices evenly to clients\n",
        "        for i in range(number_of_clients):\n",
        "            client_indices = np.array([], dtype=int)\n",
        "            for class_ in class_per_client:\n",
        "                n_samples = len(class_idx[class_]) // number_of_clients + (1 if i < remainder else 0)\n",
        "                client_indices = np.concatenate((client_indices, class_idx[class_][:n_samples]))\n",
        "                class_idx[class_] = np.delete(class_idx[class_], np.arange(n_samples))\n",
        "\n",
        "            shards.append(Subset(dataset, indices=client_indices))\n",
        "\n",
        "    return shards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THU5-nN_Jp1E"
      },
      "source": [
        "### Client Local Update"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Client class"
      ],
      "metadata": {
        "id": "1Z0HbBmlngSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.backends import cudnn\n",
        "import time\n",
        "\n",
        "\n",
        "class Client:\n",
        "    def __init__(self, client_id, data_loader, model, device):\n",
        "        \"\"\"\n",
        "        Initializes a federated learning client.\n",
        "        :param client_id: Unique identifier for the client.\n",
        "        :param data_loader: Data loader specific to the client.\n",
        "        :param model: The model class to be used by the client.\n",
        "        :param device: The device (CPU/GPU) to perform computations.\n",
        "        \"\"\"\n",
        "        self.client_id = client_id\n",
        "        self.data_loader = data_loader\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def client_update(self, client_data, criterion, optimizer, local_steps=4, detailed_print=False):\n",
        "        \"\"\"\n",
        "        Trains a given client's local model on its dataset for a fixed number of steps (`local_steps`).\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): The local model to be updated.\n",
        "            client_id (int): Identifier for the client (used for logging/debugging purposes).\n",
        "            client_data (DataLoader): The data loader for the client's dataset.\n",
        "            criterion (Loss): The loss function used for training (e.g., CrossEntropyLoss).\n",
        "            optimizer (Optimizer): The optimizer used for updating model parameters (e.g., SGD).\n",
        "            local_steps (int): Number of local epochs to train on the client's dataset.\n",
        "            detailed_print (bool): If True, logs the final loss after training.\n",
        "\n",
        "        Returns:\n",
        "            dict: The state dictionary of the updated model.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.model.train()  # Set the model to training mode\n",
        "        step_count = 0\n",
        "        while step_count < local_steps:\n",
        "            for data, targets in client_data:\n",
        "                # Move data and targets to the specified device (e.g., GPU or CPU)\n",
        "                data, targets = data.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "\n",
        "                start_time = time.time()  # for testing-----------------------------\n",
        "\n",
        "                # Reset the gradients before backpropagation\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass: compute model predictions\n",
        "                outputs = self.model(data)\n",
        "\n",
        "                # Compute the loss\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                # Backward pass: compute gradients and update weights\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # for testing ------------------------------------------------------\n",
        "                if detailed_print:\n",
        "                  end_time = time.time()  # Record the end time\n",
        "                  elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "                  print(f'Single step time taken: {elapsed_time:.4f} seconds')\n",
        "\n",
        "                step_count +=1\n",
        "                if step_count >= local_steps:\n",
        "                  break\n",
        "\n",
        "        # Optionally, print the loss for the last epoch of training\n",
        "        if detailed_print:\n",
        "          print(f'Client {self.client_id} --> Final Loss (Step {step_count}/{local_steps}): {loss.item()}')\n",
        "\n",
        "\n",
        "        # Return the updated model's state dictionary (weights)\n",
        "        return self.model.state_dict()"
      ],
      "metadata": {
        "id": "q1R5FNBnniF1"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed0w2OeTKFiQ"
      },
      "source": [
        "### Central Server Aggregation with FedAvg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Server class"
      ],
      "metadata": {
        "id": "2jum6Gwpo8sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Server:\n",
        "    def __init__(self, global_model):\n",
        "        self.global_model = global_model\n",
        "\n",
        "    def fedavg_aggregate(self, client_states, client_sizes):\n",
        "        # Aggregation logic\n",
        "        new_state = deepcopy(self.global_model.state_dict())\n",
        "        total_samples = sum(client_sizes)\n",
        "        for key in new_state:\n",
        "            new_state[key] = torch.zeros_like(new_state[key])\n",
        "        for state, size in zip(client_states, client_sizes):\n",
        "            for key in new_state:\n",
        "                new_state[key] += state[key] * size / total_samples\n",
        "        return new_state\n",
        "\n",
        "\n",
        "\n",
        "    # Federated Learning Training Loop\n",
        "    def train_federated(self, criterion, trainloader, validloader, num_clients, num_classes, rounds, lr, momentum, batchsize, wd, C=0.1, local_steps=4, log_freq=10, detailed_print=False):\n",
        "        val_accuracies = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        train_losses = []\n",
        "        best_model_state = None  # The model with the best accuracy\n",
        "        client_selection_count = [0] * num_clients #Count how many times a client has been selected\n",
        "        best_val_acc = 0.0\n",
        "\n",
        "        shards = sharding(trainloader.dataset, num_clients, num_classes) #each shard represent the training data for one client\n",
        "        client_sizes = [len(shard) for shard in shards]\n",
        "\n",
        "        self.global_model.to(DEVICE) #as alwayse, we move the global model to the specified device (CPU or GPU)\n",
        "\n",
        "        #loading checkpoint if it exists\n",
        "        checkpoint_start_step, data_to_load = load_checkpoint(model=global_model,optimizer=None,hyperparameters=f\"LR{lr}_WD{wd}\", subfolder=\"Federated/\")\n",
        "        if data_to_load is not None:\n",
        "          val_accuracies = data_to_load['val_accuracies']\n",
        "          val_losses = data_to_load['val_losses']\n",
        "          train_accuracies = data_to_load['train_accuracies']\n",
        "          train_losses = data_to_load['train_losses']\n",
        "          client_selection_count = data_to_load['client_selection_count']\n",
        "\n",
        "\n",
        "        # ********************* HOW IT WORKS ***************************************\n",
        "        # The training runs for rounds iterations (GLOBAL_ROUNDS=2000)\n",
        "        # Each round simulates one communication step in federated learning, including:\n",
        "        # 1) client selection\n",
        "        # 2) local training (of each client)\n",
        "        # 3) central aggregation\n",
        "        for round_num in range(checkpoint_start_step, rounds):\n",
        "            if round_num % log_freq == 0:\n",
        "              print(f\"------------------------------------- Round {round_num} ------------------------------------------------\" )\n",
        "\n",
        "            #start_time = time.time()  # for testing-----------------------------\n",
        "\n",
        "            # 1) client selection: In each round, a fraction C (e.g., 10%) of clients is randomly selected to participate.\n",
        "            #     This reduces computation costs and mimics real-world scenarios where not all devices are active.\n",
        "            selected_clients = random.sample(range(num_clients), int(C * num_clients))\n",
        "            client_states = []\n",
        "            for client_id in selected_clients:\n",
        "                client_selection_count[client_id] += 1\n",
        "\n",
        "            # 2) local training: for each client updates the model using the client's data for local_steps epochs\n",
        "            for client_id in selected_clients:\n",
        "                local_model = deepcopy(self.global_model) #it creates a local copy of the global model\n",
        "                optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=momentum, weight_decay=wd) #same of the centralized version\n",
        "                client_loader = DataLoader(shards[client_id], batch_size=batchsize, shuffle=True)\n",
        "\n",
        "                client = Client(client_id, client_loader, local_model, DEVICE)\n",
        "\n",
        "                local_state = client.client_update(client_loader, criterion, optimizer, local_steps, round_num % log_freq == 0 and detailed_print)\n",
        "                client_states.append(local_state)\n",
        "\n",
        "\n",
        "            # 3) central aggregation: aggregates participating client updates using fedavg_aggregate\n",
        "            #    and replaces the current parameters of global_model with the returned ones.\n",
        "            aggregated_state = self.fedavg_aggregate(client_states, [client_sizes[i] for i in selected_clients])\n",
        "            self.global_model.load_state_dict(aggregated_state)\n",
        "\n",
        "            #Validation at the server\n",
        "            #if round_num % log_freq:\n",
        "            val_accuracy, val_loss = evaluate(self.global_model, validloader,criterion)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            val_losses.append(val_loss)\n",
        "            if val_accuracy > best_val_acc:\n",
        "                best_val_acc = val_accuracy\n",
        "                best_model_state = deepcopy(self.global_model.state_dict())\n",
        "\n",
        "            if round_num % log_freq == 0:\n",
        "                train_accuracy, train_loss = evaluate(self.global_model, trainloader,criterion)\n",
        "                train_accuracies.append(train_accuracy)\n",
        "                train_losses.append(train_loss)\n",
        "\n",
        "                print(f\"--> best validation accuracy: {best_val_acc}\\n--> training accuracy: {train_accuracy}\")\n",
        "                print(f\"--> validation loss: {val_loss}\\n--> training loss: {train_loss}\")\n",
        "\n",
        "                # checkpointing\n",
        "                checkpoint_data = {\n",
        "                    'val_accuracies': val_accuracies,\n",
        "                    'val_losses': val_losses,\n",
        "                    'train_accuracies': train_accuracies,\n",
        "                    'train_losses': train_losses,\n",
        "                    'client_selection_count': client_selection_count\n",
        "                }\n",
        "                save_checkpoint(model=self.global_model, optimizer=None, epoch=round_num, hyperparameters=f\"LR{lr}_WD{wd}\", subfolder=\"Federated/\", checkpoint_data=checkpoint_data)\n",
        "\n",
        "                print(f\"------------------------------ Round {round_num} terminated: model updated -----------------------------\\n\\n\" )\n",
        "\n",
        "\n",
        "            # for testing ------------------------------------------------------\n",
        "            #end_time = time.time()  # Record the end time\n",
        "            #elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "            #print(f'Single round time taken: {elapsed_time:.4f} seconds\\n\\n')\n",
        "\n",
        "\n",
        "        global_model.load_state_dict(best_model_state)\n",
        "\n",
        "        return global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count\n"
      ],
      "metadata": {
        "id": "luRcM2WXo-jx"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "fZyWmIM49-XE"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.train(False) # Set Network to evaluation mode\n",
        "        running_corrects = 0\n",
        "        losses = []\n",
        "\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(DEVICE)        # Move the data to the GPU\n",
        "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == targets.data).data.item()\n",
        "            # Calculate Accuracy\n",
        "            accuracy = running_corrects / float(len(dataloader.dataset))\n",
        "\n",
        "    return accuracy, mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY8ptTgOKnQN"
      },
      "source": [
        "### Federated Learning Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHOLidiTLHz0"
      },
      "source": [
        "### Initialize Model & Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "H5aKGpEELHYf"
      },
      "outputs": [],
      "source": [
        "global_model = LeNet5()\n",
        "criterion = nn.NLLLoss()# our loss function for classification tasks on CIFAR-100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGk6k4GPLZy0"
      },
      "source": [
        "### Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kXEc1etLgq6",
        "outputId": "41d5acfb-fcb7-4710-9a40-77c8d7386afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found, starting from epoch 1.\n",
            "------------------------------------- Round 10 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0108\n",
            "--> training accuracy: 0.009888888888888888\n",
            "--> validation loss: 4.605470189623013\n",
            "--> training loss: 4.605592923547329\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_10_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 10 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 20 ------------------------------------------------\n",
            "--> best validation accuracy: 0.011\n",
            "--> training accuracy: 0.01071111111111111\n",
            "--> validation loss: 4.604295551397239\n",
            "--> training loss: 4.604527847599119\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_20_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 20 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 30 ------------------------------------------------\n",
            "--> best validation accuracy: 0.011\n",
            "--> training accuracy: 0.010422222222222222\n",
            "--> validation loss: 4.603078116277221\n",
            "--> training loss: 4.60335776987191\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_30_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 30 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 40 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0122\n",
            "--> training accuracy: 0.012\n",
            "--> validation loss: 4.601582515011927\n",
            "--> training loss: 4.601980772947028\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_40_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 40 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 50 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0132\n",
            "--> training accuracy: 0.012911111111111111\n",
            "--> validation loss: 4.599678051699499\n",
            "--> training loss: 4.600022533893924\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_50_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 50 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 60 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0142\n",
            "--> training accuracy: 0.013888888888888888\n",
            "--> validation loss: 4.5971380282359515\n",
            "--> training loss: 4.597590377335863\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_60_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 60 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 70 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0172\n",
            "--> training accuracy: 0.017244444444444444\n",
            "--> validation loss: 4.593718079245015\n",
            "--> training loss: 4.594112276268412\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_70_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 70 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 80 ------------------------------------------------\n",
            "--> best validation accuracy: 0.02\n",
            "--> training accuracy: 0.02033333333333333\n",
            "--> validation loss: 4.588995799896823\n",
            "--> training loss: 4.589792705039734\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_80_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 80 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 90 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0232\n",
            "--> training accuracy: 0.02228888888888889\n",
            "--> validation loss: 4.581954734340595\n",
            "--> training loss: 4.582724275365313\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_90_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 90 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 100 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0234\n",
            "--> training accuracy: 0.0208\n",
            "--> validation loss: 4.572027227681154\n",
            "--> training loss: 4.573445302845318\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_100_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 100 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 110 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0242\n",
            "--> training accuracy: 0.02277777777777778\n",
            "--> validation loss: 4.557545950458308\n",
            "--> training loss: 4.560032335510471\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_110_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 110 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 120 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0264\n",
            "--> training accuracy: 0.024555555555555556\n",
            "--> validation loss: 4.537549401544462\n",
            "--> training loss: 4.541437115835313\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_120_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 120 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 130 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0302\n",
            "--> training accuracy: 0.028155555555555555\n",
            "--> validation loss: 4.508596793861146\n",
            "--> training loss: 4.514857550080821\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_130_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 130 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 140 ------------------------------------------------\n",
            "--> best validation accuracy: 0.032\n",
            "--> training accuracy: 0.02931111111111111\n",
            "--> validation loss: 4.46742085742343\n",
            "--> training loss: 4.479234431767684\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_140_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 140 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 150 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0336\n",
            "--> training accuracy: 0.03437777777777778\n",
            "--> validation loss: 4.414839170540974\n",
            "--> training loss: 4.430294703149423\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_150_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 150 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 160 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0392\n",
            "--> training accuracy: 0.03746666666666667\n",
            "--> validation loss: 4.3607271097268265\n",
            "--> training loss: 4.381503951744399\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_160_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 160 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 170 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0434\n",
            "--> training accuracy: 0.03977777777777778\n",
            "--> validation loss: 4.309690440536305\n",
            "--> training loss: 4.33251287959193\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_170_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 170 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 180 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0496\n",
            "--> training accuracy: 0.04466666666666667\n",
            "--> validation loss: 4.271150418907214\n",
            "--> training loss: 4.301316293826235\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_180_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 180 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 190 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0496\n",
            "--> training accuracy: 0.046266666666666664\n",
            "--> validation loss: 4.2497160358793415\n",
            "--> training loss: 4.277912290874067\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_190_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 190 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 200 ------------------------------------------------\n",
            "--> best validation accuracy: 0.055\n",
            "--> training accuracy: 0.04671111111111111\n",
            "--> validation loss: 4.235613511626128\n",
            "--> training loss: 4.263975497756177\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_200_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 200 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 210 ------------------------------------------------\n",
            "--> best validation accuracy: 0.055\n",
            "--> training accuracy: 0.04548888888888889\n",
            "--> validation loss: 4.227679412076427\n",
            "--> training loss: 4.2583109049739445\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_210_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 210 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 220 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0586\n",
            "--> training accuracy: 0.0494\n",
            "--> validation loss: 4.214838700689328\n",
            "--> training loss: 4.248286766792411\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_220_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 220 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 230 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0598\n",
            "--> training accuracy: 0.05057777777777778\n",
            "--> validation loss: 4.203579330140618\n",
            "--> training loss: 4.2386127424816245\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_230_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 230 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 240 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0598\n",
            "--> training accuracy: 0.055\n",
            "--> validation loss: 4.193412194586104\n",
            "--> training loss: 4.230628072241137\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_240_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 240 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 250 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0622\n",
            "--> training accuracy: 0.05355555555555556\n",
            "--> validation loss: 4.184226575171112\n",
            "--> training loss: 4.223498542341003\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_250_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 250 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 260 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0628\n",
            "--> training accuracy: 0.053288888888888886\n",
            "--> validation loss: 4.17760157888862\n",
            "--> training loss: 4.216415481187811\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_260_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 260 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 270 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0666\n",
            "--> training accuracy: 0.0548\n",
            "--> validation loss: 4.1670930917095985\n",
            "--> training loss: 4.20892515263832\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_270_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 270 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 280 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0678\n",
            "--> training accuracy: 0.057133333333333335\n",
            "--> validation loss: 4.155003500592177\n",
            "--> training loss: 4.198773648608383\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_280_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 280 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 290 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0692\n",
            "--> training accuracy: 0.05553333333333333\n",
            "--> validation loss: 4.1467574493140935\n",
            "--> training loss: 4.195541305243757\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_290_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 290 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 300 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0728\n",
            "--> training accuracy: 0.05575555555555556\n",
            "--> validation loss: 4.137893896953315\n",
            "--> training loss: 4.187172425750646\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_300_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 300 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 310 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0728\n",
            "--> training accuracy: 0.06155555555555556\n",
            "--> validation loss: 4.124435490104044\n",
            "--> training loss: 4.174989411910545\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_310_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 310 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 320 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0748\n",
            "--> training accuracy: 0.059955555555555554\n",
            "--> validation loss: 4.113481463900038\n",
            "--> training loss: 4.168635842812002\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_320_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 320 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 330 ------------------------------------------------\n",
            "--> best validation accuracy: 0.0752\n",
            "--> training accuracy: 0.0638\n",
            "--> validation loss: 4.1052005199869726\n",
            "--> training loss: 4.152775309995802\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_330_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 330 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 340 ------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#just for now\n",
        "lr = LR\n",
        "wd = WEIGHT_DECAY\n",
        "#delete_existing_checkpoints(\"Federated/\")\n",
        "# Run Federated Learning\n",
        "# Instantiate the server\n",
        "server = Server(global_model)\n",
        "#run federeted learning\n",
        "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(\n",
        "    criterion=criterion,\n",
        "    trainloader=trainloader,\n",
        "    validloader=validloader,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    num_classes=100,\n",
        "    rounds=GLOBAL_ROUNDS,\n",
        "    lr=lr,\n",
        "    momentum=MOMENTUM,\n",
        "    batchsize=BATCH_SIZE,\n",
        "    wd=wd,\n",
        "    C=FRACTION_CLIENTS,\n",
        "    local_steps=LOCAL_STEPS,\n",
        "    log_freq=LOG_FREQUENCY,\n",
        "    detailed_print=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WqJIdzei1Y0"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmNUVz0mjAJ7"
      },
      "source": [
        "### Run the test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR948jsajH5g"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate(global_model, testloader, criterion)[0]\n",
        "print('\\nTest Accuracy: {}'.format(accuracy))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}