{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m18P2U7u_-Nh"
      },
      "outputs": [],
      "source": [
        "from torch.backends import cudnn\n",
        "import time\n",
        "\n",
        "\n",
        "class Client:\n",
        "    def __init__(self, client_id, data_loader, model, device):\n",
        "        \"\"\"\n",
        "        Initializes a federated learning client.\n",
        "        :param client_id: Unique identifier for the client.\n",
        "        :param data_loader: Data loader specific to the client.\n",
        "        :param model: The model class to be used by the client.\n",
        "        :param device: The device (CPU/GPU) to perform computations.\n",
        "        \"\"\"\n",
        "        self.client_id = client_id\n",
        "        self.data_loader = data_loader\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def client_update(self, client_data, criterion, optimizer, local_steps=4, detailed_print=False):\n",
        "        \"\"\"\n",
        "        Trains a given client's local model on its dataset for a fixed number of steps (`local_steps`).\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): The local model to be updated.\n",
        "            client_id (int): Identifier for the client (used for logging/debugging purposes).\n",
        "            client_data (DataLoader): The data loader for the client's dataset.\n",
        "            criterion (Loss): The loss function used for training (e.g., CrossEntropyLoss).\n",
        "            optimizer (Optimizer): The optimizer used for updating model parameters (e.g., SGD).\n",
        "            local_steps (int): Number of local epochs to train on the client's dataset.\n",
        "            detailed_print (bool): If True, logs the final loss after training.\n",
        "\n",
        "        Returns:\n",
        "            dict: The state dictionary of the updated model.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.model.train()  # Set the model to training mode\n",
        "        step_count = 0\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        while step_count < local_steps:\n",
        "            for data, targets in client_data:\n",
        "\n",
        "                # Move data and targets to the specified device (e.g., GPU or CPU)\n",
        "                data, targets = data.to(self.device), targets.to(self.device)\n",
        "\n",
        "                # Reset the gradients before backpropagation\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass: compute model predictions\n",
        "                outputs = self.model(data)\n",
        "\n",
        "                # Compute the loss\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                # Backward pass: compute gradients and update weights\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                #---------- Accumulate metrics\n",
        "                #  Accumulates the weighted loss for the number of samples in the batch to account for any variation in\n",
        "                #  batch size due to, for example, the smaller final batch. A little too precise? :)\n",
        "                total_loss += loss.item() * data.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                correct_predictions += predicted.eq(targets).sum().item()\n",
        "                total_samples += data.size(0)\n",
        "\n",
        "                step_count +=1\n",
        "                if step_count >= local_steps:\n",
        "                  break\n",
        "\n",
        "        #---------- Compute averaged metrics\n",
        "        avg_loss = total_loss / total_samples\n",
        "        avg_accuracy = correct_predictions / total_samples * 100\n",
        "\n",
        "        # Optionally, print the loss for the last epoch of training\n",
        "        if detailed_print:\n",
        "          print(f'Client {self.client_id} --> Final Loss (Step {step_count}/{local_steps}): {loss.item()}')\n",
        "\n",
        "\n",
        "        # Return the updated model's state dictionary (weights)\n",
        "        return self.model.state_dict(), avg_loss, avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "import os\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "#from utils.utils import evaluate\n",
        "#from utils.checkpointing_utils import save_checkpoint, load_checkpoint\n",
        "import logging\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "class Server:\n",
        "    def __init__(self, global_model, device, CHECKPOINT_DIR):\n",
        "        self.global_model = global_model\n",
        "        self.device = device\n",
        "        self.CHECKPOINT_DIR = CHECKPOINT_DIR\n",
        "        # Ensure the checkpoint directory exists, creating it if necessary\n",
        "        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    def fedavg_aggregate(self, client_states, client_sizes, client_avg_losses, client_avg_accuracies):\n",
        "        \"\"\"\n",
        "        Aggregates model updates and client metrics from selected clients using the Federated Averaging (FedAvg) algorithm.\n",
        "        The updates and metrics are weighted by the size of each client's dataset.\n",
        "\n",
        "        Args:\n",
        "            global_model (nn.Module): The global model whose structure is used for aggregation.\n",
        "            client_states (list[dict]): A list of state dictionaries (model weights) from participating clients.\n",
        "            client_sizes (list[int]): A list of dataset sizes for the participating clients.\n",
        "            client_avg_losses (list[float]): A list of average losses for the participating clients.\n",
        "            client_avg_accuracies (list[float]): A list of average accuracies for the participating clients.\n",
        "\n",
        "        Returns:\n",
        "            tuple: The aggregated state dictionary with updated model parameters, global average loss, and global average accuracy.\n",
        "        \"\"\"\n",
        "        # Copy the global model's state dictionary for aggregation\n",
        "        new_state = deepcopy(self.global_model.state_dict())\n",
        "\n",
        "        # Calculate the total number of samples across all participating clients\n",
        "        total_samples = sum(client_sizes)\n",
        "\n",
        "        # Initialize all parameters in the new state to zero\n",
        "        for key in new_state:\n",
        "            new_state[key] = torch.zeros_like(new_state[key])\n",
        "\n",
        "        # Initialize metrics\n",
        "        total_loss = 0.0\n",
        "        total_accuracy = 0.0\n",
        "\n",
        "        # Perform a weighted average of client updates and metrics\n",
        "        for state, size, avg_loss, avg_accuracy in zip(client_states, client_sizes, client_avg_losses, client_avg_accuracies):\n",
        "            for key in new_state:\n",
        "                # Add the weighted contribution of each client's parameters\n",
        "                new_state[key] += (state[key] * size / total_samples)\n",
        "            total_loss += avg_loss * size\n",
        "            total_accuracy += avg_accuracy * size\n",
        "\n",
        "        # Calculate global metrics\n",
        "        global_avg_loss = total_loss / total_samples\n",
        "        global_avg_accuracy = total_accuracy / total_samples\n",
        "\n",
        "        # Return the aggregated state dictionary with updated weights and global metrics\n",
        "        return new_state, global_avg_loss, global_avg_accuracy\n",
        "\n",
        "\n",
        "    # Federated Learning Training Loop\n",
        "    ### UPDATED\n",
        "    #0. receives the shards (the result of sharding method)\n",
        "    #1. receive the n clients to perform one round\n",
        "    #2. averages the results coming from the model of the n clients\n",
        "    #3. returns the model and the results averaged.\n",
        "    def train_federated(self, criterion, lr, momentum, batchsize, wd, selected_clients, shards, local_steps=4):\n",
        "      # selected_clients are objects of Individual class\n",
        "\n",
        "        train_accuracies = []\n",
        "        train_losses = []\n",
        "\n",
        "        #shards = self.sharding(trainloader.dataset, num_clients, num_classes) #each shard represent the training data for one client\n",
        "        client_sizes = [len(shard) for shard in shards]\n",
        "\n",
        "        self.global_model.to(self.device) #as alwayse, we move the global model to the specified device (CPU or GPU)\n",
        "\n",
        "        # we only mantain a round\n",
        "        client_states = []\n",
        "        client_avg_losses = []\n",
        "        client_avg_accuracies = []\n",
        "\n",
        "        # 2) local training: for each client updates the model using the client's data for local_steps epochs\n",
        "        for client_id in selected_clients.genome:\n",
        "            local_model = deepcopy(self.global_model) #it creates a local copy of the global model\n",
        "            optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=momentum, weight_decay=wd) #same of the centralized version\n",
        "            client_loader = DataLoader(shards[client_id], batch_size=batchsize, shuffle=True)\n",
        "\n",
        "            client = Client(client_id, client_loader, local_model, self.device)\n",
        "            client_local_state, client_avg_loss, client_avg_accuracy  = client.client_update(client_loader, criterion, optimizer, local_steps)\n",
        "\n",
        "            client_states.append(client_local_state)\n",
        "            client_avg_losses.append(client_avg_loss)\n",
        "            client_avg_accuracies.append(client_avg_accuracy)\n",
        "\n",
        "\n",
        "        # 3) central aggregation: aggregates participating client updates using fedavg_aggregate\n",
        "        #    and replaces the current parameters of global_model with the returned ones.\n",
        "        aggregated_state, train_loss, train_accuracy = self.fedavg_aggregate(client_states, [client_sizes[i] for i in selected_clients.genome], client_avg_losses, client_avg_accuracies)\n",
        "        # UPDATE THE FITNESS OF INDIVIDUAL:\n",
        "        selected_clients.fitness = train_accuracy\n",
        "\n",
        "        self.global_model.load_state_dict(aggregated_state)\n",
        "\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "\n",
        "        return self.global_model.state_dict(), train_accuracy, train_loss\n",
        "\n",
        "    def skewed_probabilities(self, number_of_clients, gamma=0.5):\n",
        "            # Generate skewed probabilities using a Dirichlet distribution\n",
        "            probabilities = np.random.dirichlet(np.ones(number_of_clients) * gamma)\n",
        "            return probabilities\n",
        "\n",
        "    def client_selection(self,number_of_clients, clients_fraction, probabilities=None):\n",
        "        \"\"\"\n",
        "        Selects a subset of clients based on uniform or skewed distribution.\n",
        "\n",
        "        Args:\n",
        "        number_of_clients (int): Total number of clients.\n",
        "        clients_fraction (float): Fraction of clients to be selected.\n",
        "        uniform (bool): If True, selects clients uniformly. If False, selects clients based on a skewed distribution.\n",
        "        gamma (float): Hyperparameter for the Dirichlet distribution controlling the skewness (only used if uniform=False).\n",
        "\n",
        "        Returns:\n",
        "        list: List of selected client indices.\n",
        "        \"\"\"\n",
        "        num_clients_to_select = int(number_of_clients * clients_fraction)\n",
        "\n",
        "        if probabilities is None:\n",
        "            # Uniformly select clients without replacement\n",
        "            selected_clients = np.random.choice(number_of_clients, num_clients_to_select, replace=False)\n",
        "        else:\n",
        "            selected_clients = np.random.choice(number_of_clients, num_clients_to_select, replace=False, p=probabilities)\n",
        "\n",
        "        return selected_clients\n",
        "\n",
        "\n",
        "\n",
        "    def sharding(self, dataset, number_of_clients, number_of_classes=100):\n",
        "        \"\"\"\n",
        "        Function that performs the sharding of the dataset given as input.\n",
        "        dataset: dataset to be split (should be a PyTorch dataset or similar);\n",
        "        number_of_clients: the number of partitions we want to obtain (e.g., 100 for 100 clients);\n",
        "        number_of_classes: (int) the number of classes inside each partition, or 100 for IID (default to 100).\n",
        "        \"\"\"\n",
        "\n",
        "        # Validate the number of classes input\n",
        "        if not (1 <= number_of_classes <= 100):\n",
        "            raise ValueError(\"number_of_classes should be an integer between 1 and 100\")\n",
        "\n",
        "        # Shuffle dataset indices for randomness\n",
        "        indices = np.random.permutation(len(dataset))\n",
        "\n",
        "        if number_of_classes == 100:  # IID Case\n",
        "            # Equally distribute indices among clients: we can just randomly assign an equal number of records to each client\n",
        "\n",
        "            # Compute basic partition sizes\n",
        "            basic_partition_size = len(dataset) // number_of_clients\n",
        "            remainder = len(dataset) % number_of_clients\n",
        "\n",
        "            shards = []  # This will hold the final dataset shards\n",
        "            start_idx = 0\n",
        "\n",
        "            for i in range(number_of_clients):\n",
        "                end_idx = start_idx + basic_partition_size + (1 if i < remainder else 0)\n",
        "                shards.append(Subset(dataset, indices[start_idx:end_idx]))\n",
        "                start_idx = end_idx\n",
        "            return shards\n",
        "\n",
        "        else:  # non-IID Case\n",
        "            # Get labels for sorting\n",
        "            labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
        "            TOTAL_NUM_CLASSES = len(set(labels))\n",
        "\n",
        "            shard_size = len(dataset) // (number_of_clients * number_of_classes)  # Shard size for each class per client\n",
        "            print(\"dataset len: \", len(dataset), \", shard size: \", shard_size, \", number of shards: \",(number_of_clients * number_of_classes))\n",
        "            if shard_size == 0:\n",
        "                raise ValueError(\"Shard size is too small; increase dataset size or reduce number of clients/classes.\")\n",
        "\n",
        "\n",
        "            # Divide the dataset into shards, each containing samples from one class\n",
        "            shards = {}\n",
        "            for i in range(TOTAL_NUM_CLASSES):\n",
        "                # Filter samples for the current class\n",
        "                class_samples = [j for j in range(len(labels)) if labels[j] == i]\n",
        "                shards_of_class_i = []\n",
        "                # While there are enough samples to form a shard\n",
        "                while len(class_samples) >= shard_size:\n",
        "                    # Take a shard of shard_size samples\n",
        "                    shards_of_class_i.append(class_samples[:shard_size])\n",
        "                    # Remove the shard_size samples from class_samples\n",
        "                    class_samples = class_samples[shard_size:]\n",
        "                # Add the last shard (which might be smaller than shard_size)\n",
        "                if class_samples:\n",
        "                    shards_of_class_i.append(class_samples)\n",
        "                # Store the class shards\n",
        "                shards[i] = shards_of_class_i  # Store shards by class\n",
        "\n",
        "            client_shards = []  # List to store the dataset for each client\n",
        "            for client_id in range(number_of_clients):\n",
        "\n",
        "                client_labels = [label % TOTAL_NUM_CLASSES for label in range(client_id, client_id + number_of_classes)]\n",
        "                #print(client_labels)\n",
        "\n",
        "                # Collect the shards for the selected classes\n",
        "                client_shard_indices = []\n",
        "                for label in client_labels:\n",
        "                    shard = shards[label].pop(0)  # Pop the first shard from the class's shard list\n",
        "                    client_shard_indices.append(shard)\n",
        "\n",
        "                # Flatten and combine the shard indices into one list\n",
        "                client_indices = [idx for shard in client_shard_indices for idx in shard]\n",
        "\n",
        "                #print(f\"Client {client_id} has {len(client_indices)} samples divided in {len(client_shard_indices)} shards (classes).\")\n",
        "                # Create a Subset for the client\n",
        "                client_dataset = Subset(dataset, client_indices)\n",
        "                client_shards.append(client_dataset)\n",
        "\n",
        "            return client_shards  # Return the list of dataset subsets (shards) for each client\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "HSytwB6DAI63"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\"\"\"\n",
        "Model architecture for the CIFAR-100 dataset.\n",
        "The model is based on the LeNet-5 architecture with some modifications.\n",
        "Reference: Hsu et al., Federated Visual Classification with Real-World Data Distribution, ECCV 2020\n",
        "\n",
        "CNN similar to LeNet5 which has two 5×5, 64-channel convolution layers, each precedes a 2×2\n",
        "max-pooling layer, followed by two fully-connected layers with 384 and 192\n",
        "channels respectively and finally a softmax linear classifier\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(64 * 5 * 5, 384),  # Updated to be consistent with data augmentation\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 192),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(192, 100)  # 100 classes for CIFAR-100\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output of the conv layers\n",
        "        x = self.fc_layer(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "JT3UTdtmAQID"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class CIFAR100DataLoader:\n",
        "    def __init__(self, batch_size=64, validation_split=0.1, download=True, num_workers=0, pin_memory=True):\n",
        "        self.batch_size = batch_size\n",
        "        self.validation_split = validation_split\n",
        "        self.download = download\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        # Define transformations\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "        ])\n",
        "\n",
        "        self.test_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "        ])\n",
        "\n",
        "        # Load datasets\n",
        "        self.train_loader, self.val_loader, self.test_loader = self._prepare_loaders()\n",
        "\n",
        "    def _prepare_loaders(self):\n",
        "        # Load the full training dataset\n",
        "        full_trainset = datasets.CIFAR100(root='./data', train=True, download=self.download, transform=self.train_transform)\n",
        "\n",
        "        # Split indices for training and validation\n",
        "        indexes = list(range(len(full_trainset)))\n",
        "        train_indexes, val_indexes = train_test_split(\n",
        "            indexes,\n",
        "            train_size=1 - self.validation_split,\n",
        "            test_size=self.validation_split,\n",
        "            random_state=42,\n",
        "            stratify=full_trainset.targets,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Create training and validation subsets\n",
        "        train_dataset = Subset(full_trainset, train_indexes)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=self.batch_size, shuffle=True,\n",
        "            num_workers=self.num_workers, pin_memory=self.pin_memory\n",
        "        )\n",
        "\n",
        "        full_trainset_val = datasets.CIFAR100(root='./data', train=True, download=self.download, transform=self.test_transform)\n",
        "        val_dataset = Subset(full_trainset_val, val_indexes)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=self.pin_memory\n",
        "        )\n",
        "\n",
        "        # Load the test dataset\n",
        "        testset = datasets.CIFAR100(root='./data', train=False, download=self.download, transform=self.test_transform)\n",
        "        test_loader = DataLoader(\n",
        "            testset, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=self.pin_memory\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Allows iteration over all loaders for unified access.\"\"\"\n",
        "        return iter([self.train_loader, self.val_loader, self.test_loader])"
      ],
      "metadata": {
        "id": "2AHTaUUkASt_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class Individual:\n",
        "    def __init__(self, genome, total_clients=100,number_selected_clients = 2):\n",
        "        \"\"\"\n",
        "        Initialize an Individual.\n",
        "\n",
        "        :param genome: List of selected clients (subset of integers).\n",
        "        :param total_clients: Total number of available clients (default 100).\n",
        "        :param number_selected_clients: Number of clients to be selected (default 10).\n",
        "        \"\"\"\n",
        "        self.genome = genome\n",
        "        self.fitness = None  # Fitness will be computed separately\n",
        "        self.total_clients = total_clients\n",
        "        self.number_selected_clients = number_selected_clients\n",
        "\n",
        "    def set_fitness(self, fitness_value):\n",
        "        \"\"\"\n",
        "        Set the fitness value for the individual.\n",
        "\n",
        "        :param fitness_value: Float value representing the fitness (e.g., loss or accuracy).\n",
        "        \"\"\"\n",
        "        self.fitness = fitness_value\n",
        "\n",
        "    def point_mutation(self):\n",
        "        \"\"\"\n",
        "        Mutate the genome by changing 1 client randomly.\n",
        "        Ensures that the selected clients remain disjoint.\n",
        "        \"\"\"\n",
        "        num_changes = 1  # Number of mutations\n",
        "        available_clients = set(range(self.total_clients)) - set(self.genome)  # Clients not in genome\n",
        "\n",
        "        # Remove random clients from the genome\n",
        "        to_remove = random.sample(self.genome, k=num_changes)\n",
        "        for client in to_remove:\n",
        "            self.genome.remove(client)\n",
        "\n",
        "        # Add new random clients from the available set\n",
        "        to_add = random.sample(list(available_clients), k=num_changes)\n",
        "        self.genome.extend(to_add)\n",
        "\n",
        "    @staticmethod\n",
        "    def crossover(parent1, parent2):\n",
        "        \"\"\"\n",
        "        Perform crossover between two parents.\n",
        "        Select half genes from parent1 and half from parent2.\n",
        "        Ensures that the genome is disjoint and valid.\n",
        "\n",
        "        :param parent1: First parent Individual.\n",
        "        :param parent2: Second parent Individual.\n",
        "        :return: New Individual (offspring).\n",
        "        \"\"\"\n",
        "        half_size = len(parent1.genome) // 2\n",
        "\n",
        "        # Randomly select half genes from each parent\n",
        "        genome1_part = random.sample(parent1.genome, k=half_size)\n",
        "        genome2_part = [gene for gene in parent2.genome if gene not in genome1_part]\n",
        "\n",
        "        # Combine to form new genome\n",
        "        new_genome = genome1_part + genome2_part[:len(parent1.genome) - half_size]\n",
        "\n",
        "        #Ensure that the genome is long enough\n",
        "        if(len(new_genome)< parent1.number_selected_clients):\n",
        "            new_genome = new_genome + random.sample(range(parent1.total_clients), k=parent1.number_selected_clients - len(new_genome))\n",
        "\n",
        "        return Individual(genome=new_genome, total_clients=parent1.total_clients)"
      ],
      "metadata": {
        "id": "OobS6DaRAZV7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from statistics import mean\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def tournament_selection(population, tau=2):\n",
        "    \"\"\"\n",
        "    Perform tournament selection to choose parents.\n",
        "    Randomly select tau individuals and choose the best one.\n",
        "\n",
        "    :param population: List of Individuals.\n",
        "    :param tau: Number of individuals to select.\n",
        "    :return: Selected Individual.\n",
        "    \"\"\"\n",
        "    participants = random.sample(population, tau)\n",
        "    winner = max(participants, key=lambda ind: ind.fitness)\n",
        "    return deepcopy(winner)\n",
        "\n",
        "def client_size(individual, client_sizes):\n",
        "    \"\"\"\n",
        "    Computes the number of total samples for individual\n",
        "    \"\"\"\n",
        "    val = 0\n",
        "    for client in individual.genome:\n",
        "        val += client_sizes[client]\n",
        "    return val\n",
        "\n",
        "def evaluate(model, dataloader, DEVICE, criterion):\n",
        "    with torch.no_grad():\n",
        "        model.train(False) # Set Network to evaluation mode\n",
        "        running_corrects = 0\n",
        "        losses = []\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(DEVICE)        # Move the data to the GPU\n",
        "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
        "            # Forward Pass\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == targets.data).data.item()\n",
        "            # Calculate Accuracy\n",
        "            accuracy = running_corrects / float(len(dataloader.dataset))\n",
        "\n",
        "    return accuracy*100, mean(losses)\n",
        "\n",
        "\n",
        "def EA_algorithm(generations,population_size,num_clients,crossover_probability, dataset, valid_loader):\n",
        "    \"\"\"\n",
        "    Perform the Evolutionary Algorithm (EA) to optimize the selection of clients.\n",
        "    The EA consists of the following steps:\n",
        "    1. Initialization: Create a population of individuals.\n",
        "    2. Evaluation: Compute the fitness of each individual.\n",
        "    3. Selection: Choose parents based on their fitness.\n",
        "    4. Offspring to create the new population (generational model).\n",
        "    6. Repeat from step 2 maximum iterations.\n",
        "\n",
        "    :param generations: Number of generations to run the algorithm.\n",
        "    :param population_size: Number of individuals in the population.\n",
        "    :param num_clients: clients selected by each individual.\n",
        "    :param crossover_probability: Probability of crossover for each individual.\n",
        "\n",
        "    :return global_model: The global model obtained after the EA.\n",
        "    :return global_accuracy: The validation accuracy of the global model at each generation.\n",
        "    :return global_loss: The validation loss of the global model at each generation.\n",
        "    return training_loss: The training loss of the global model at each generation.\n",
        "    return training_accuracy: The training accuracy of the global model at each generation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the population\n",
        "    population = [Individual(genome=random.sample(range(100), k=num_clients)) for _ in range(population_size)]\n",
        "    model = LeNet5()\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    criterion = nn.NLLLoss()\n",
        "    # Create the Server instance:\n",
        "    server = Server(model,DEVICE,\"server_data\")\n",
        "    num_classes = 100\n",
        "    shards = server.sharding(dataset.dataset, num_clients, num_classes)\n",
        "    client_sizes = [len(shard) for shard in shards]\n",
        "    LR = 0.01\n",
        "    batchsize = 64\n",
        "    WD = 0.001\n",
        "    MOMENTUM = 0.9\n",
        "\n",
        "\n",
        "    for i in range(generations):\n",
        "        # Select randomly 3 individuals:\n",
        "        selected_individuals = population\n",
        "        # For each of them apply the fed_avg algorithm:\n",
        "\n",
        "        param_list = []\n",
        "        averages_acc = []\n",
        "        average_loss = []\n",
        "        for choosen_individual in selected_individuals:\n",
        "            resulting_model, acc_res, loss_res = server.train_federated(criterion, LR, MOMENTUM, batchsize, WD, choosen_individual, shards)\n",
        "            param_list.append(resulting_model)\n",
        "            averages_acc.append(acc_res)\n",
        "            average_loss.append(loss_res)\n",
        "\n",
        "\n",
        "        #Here we should average all the models to obtain the global model...\n",
        "        averaged_model,  global_avg_loss, global_avg_accuracy = server.fedavg_aggregate(param_list, [client_size(i, client_sizes) for i in selected_individuals], average_loss, averages_acc)\n",
        "        # Update the model with the result of the average:\n",
        "        model.load_state_dict(averaged_model)\n",
        "\n",
        "\n",
        "        # Then evaluate the validation accuracy of the global model\n",
        "        acc, loss = evaluate(model, valid_loader, DEVICE, criterion)\n",
        "\n",
        "        print(f\"Generation {i+1}, accuracy {acc}, loss {loss}\")\n",
        "\n",
        "        offspring = []\n",
        "        #Offspring-> offspring size is the same as population size\n",
        "        for i in range(population_size):\n",
        "            # Crossover\n",
        "            if random.random() < crossover_probability:\n",
        "                parent1 = tournament_selection(population)\n",
        "                parent2 = tournament_selection(population)\n",
        "                offspring.append(Individual.crossover(parent1, parent2))\n",
        "            else:\n",
        "                parent = tournament_selection(population)\n",
        "                #parent.point_mutation()\n",
        "                offspring.append(parent)\n",
        "\n",
        "        # Replace the population with the new offspring\n",
        "        population = offspring\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #10% of the dataset kept for validation\n",
        "    BATCH_SIZE = 64\n",
        "    data_loader = CIFAR100DataLoader(batch_size=BATCH_SIZE, validation_split=0.1, download=True, num_workers=4, pin_memory=True)\n",
        "    trainloader, validloader, testloader = data_loader.train_loader, data_loader.val_loader, data_loader.test_loader\n",
        "    EA_algorithm(10, 3, 100, 0.7, trainloader, validloader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "0XKl9GZ7AWE5",
        "outputId": "28aabcbc-e329-45a2-c923-243da0535c85"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Generation 1, accuracy 1.8800000000000001, loss 4.605861941470375\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Sample larger than population or is negative",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-705e9cc6ba95>\u001b[0m in \u001b[0;36m<cell line: 136>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR100DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mEA_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-705e9cc6ba95>\u001b[0m in \u001b[0;36mEA_algorithm\u001b[0;34m(generations, population_size, num_clients, crossover_probability, dataset, valid_loader)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtournament_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoint_mutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0moffspring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5b04aaa4cba5>\u001b[0m in \u001b[0;36mpoint_mutation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Add new random clients from the available set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mto_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_clients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_changes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
          ]
        }
      ]
    }
  ]
}