{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku-3jicvv8v0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../data/cifar100/')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from torch.utils.data import Subset\n",
        "from statistics import mean\n",
        "#from cifar100_loader import load_cifar100\n",
        "#from models.model import LeNet5 #import the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtzbKmCRJUa2"
      },
      "source": [
        "### Constants for FL training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WawTSpXyn5aS",
        "outputId": "253d33d8-3224-4306-babd-0b76e013c7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Constants for FL training\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(DEVICE)\n",
        "\n",
        "NUM_CLIENTS = 100  # Total number of clients in the federation\n",
        "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
        "LOCAL_STEPS = 4  # Number of local steps (J)\n",
        "GLOBAL_ROUNDS = 2000  # Total number of communication rounds\n",
        "\n",
        "BATCH_SIZE = 64  # Batch size for local training\n",
        "LR = 1e-2  # Initial learning rate for local optimizers\n",
        "MOMENTUM = 0.9  # Momentum for SGD optimizer\n",
        "WEIGHT_DECAY = 1e-4  # Regularization term for local training\n",
        "\n",
        "LOG_FREQUENCY = 10  # Frequency of logging training progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "146JFJRhwuyY"
      },
      "source": [
        "# Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec9eETBgwy5L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class CIFAR100DataLoader:\n",
        "    def __init__(self, batch_size=32, validation_split=0.1, download=True, num_workers=4, pin_memory=True):\n",
        "        self.batch_size = batch_size\n",
        "        self.validation_split = validation_split\n",
        "        self.download = download\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "        # Define transformations\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(24, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "        ])\n",
        "\n",
        "        self.test_transform = transforms.Compose([\n",
        "            transforms.CenterCrop(24),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "        ])\n",
        "\n",
        "        # Load datasets\n",
        "        self.train_loader, self.val_loader, self.test_loader = self._prepare_loaders()\n",
        "\n",
        "    def _prepare_loaders(self):\n",
        "        # Load the full training dataset\n",
        "        full_trainset = datasets.CIFAR100(root='./data', train=True, download=self.download, transform=self.train_transform)\n",
        "\n",
        "        # Split indices for training and validation\n",
        "        indexes = list(range(len(full_trainset)))\n",
        "        train_indexes, val_indexes = train_test_split(\n",
        "            indexes,\n",
        "            train_size=1 - self.validation_split,\n",
        "            test_size=self.validation_split,\n",
        "            random_state=42,\n",
        "            stratify=full_trainset.targets,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Create training and validation subsets\n",
        "        train_dataset = Subset(full_trainset, train_indexes)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=self.batch_size, shuffle=True,\n",
        "            num_workers=self.num_workers, pin_memory=self.pin_memory\n",
        "        )\n",
        "\n",
        "        full_trainset_val = datasets.CIFAR100(root='./data', train=True, download=self.download, transform=self.test_transform)\n",
        "        val_dataset = Subset(full_trainset_val, val_indexes)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=self.pin_memory\n",
        "        )\n",
        "\n",
        "        # Load the test dataset\n",
        "        testset = datasets.CIFAR100(root='./data', train=False, download=self.download, transform=self.test_transform)\n",
        "        test_loader = DataLoader(\n",
        "            testset, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=self.pin_memory\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Allows iteration over all loaders for unified access.\"\"\"\n",
        "        return iter([self.train_loader, self.val_loader, self.test_loader])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayrx5sBRKzQ5"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X8F2JHHKyly",
        "outputId": "2839e3b3-df4e-440f-da24-e86600ee2c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:05<00:00, 30.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dimension of the training dataset: 45000\n",
            "Dimension of the validation dataset: 5000\n",
            "Dimension of the test dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "#10% of the dataset kept for validation\n",
        "data_loader = CIFAR100DataLoader(batch_size=32, validation_split=0.1, download=True, num_workers=2, pin_memory=True)\n",
        "trainloader, validloader, testloader = data_loader.train_loader, data_loader.val_loader, data_loader.test_loader\n",
        "\n",
        "print(\"Dimension of the training dataset:\", len(trainloader.dataset))\n",
        "print(\"Dimension of the validation dataset:\", len(validloader.dataset))\n",
        "print(\"Dimension of the test dataset:\", len(testloader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpHgxPxaBvmP"
      },
      "source": [
        "## Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frTjN7V0B6ZG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Directory where checkpoints are stored\n",
        "CHECKPOINT_DIR = '../checkpoints/'\n",
        "\n",
        "# Ensure the checkpoint directory exists, creating it if necessary\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", checkpoint_data=None):\n",
        "    \"\"\"\n",
        "    Saves the model checkpoint and removes the previous one if it exists.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state is to be saved.\n",
        "    optimizer -- The optimizer whose state is to be saved (can be None).\n",
        "    epoch -- The current epoch of the training process.\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to save the checkpoint.\n",
        "    checkpoint_data -- Data to save in a JSON file (e.g., training logs).\n",
        "    \"\"\"\n",
        "    # Define the path for the subfolder where checkpoints will be stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    # Create the subfolder if it doesn't exist\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # Construct filenames for both the model checkpoint and the associated JSON file\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "    # Define the filenames for the previous checkpoint files, to remove them if necessary\n",
        "    previous_filepath = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.pth\")\n",
        "    previous_filepath_json = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.json\")\n",
        "\n",
        "    # Remove the previous checkpoint if it exists, but only for epochs greater than 1\n",
        "    if epoch > 1 and os.path.exists(previous_filepath):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Prepare the checkpoint data dictionary\n",
        "    checkpoint = {'model_state_dict': model.state_dict(), 'epoch': epoch}\n",
        "    # If an optimizer is provided, save its state as well\n",
        "    if optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    # Save the model and optimizer (if provided) state dictionary to the checkpoint file\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    # If additional data (e.g., training logs) is provided, save it to a JSON file\n",
        "    if checkpoint_data:\n",
        "        with open(filepath_json, 'w') as json_file:\n",
        "            json.dump(checkpoint_data, json_file, indent=4)\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Loads the latest checkpoint available based on the specified hyperparameters.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state will be updated from the checkpoint.\n",
        "    optimizer -- The optimizer whose state will be updated from the checkpoint (can be None).\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to look for checkpoints.\n",
        "\n",
        "    Returns:\n",
        "    The next epoch to resume from and the associated JSON data if available.\n",
        "    \"\"\"\n",
        "    # Define the path to the subfolder where checkpoints are stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "\n",
        "    # If the subfolder doesn't exist, print a message and start from epoch 1\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, starting from epoch 1.\")\n",
        "        return 1, None  # Epoch starts from 1\n",
        "\n",
        "    # Search for checkpoint files in the subfolder that match the hyperparameters\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "\n",
        "    # If checkpoint files are found, load the one with the highest epoch number\n",
        "    if files:\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))  # Find the latest epoch file\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath, weights_only=True)\n",
        "\n",
        "        # Load the model state from the checkpoint\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        # If an optimizer is provided, load its state as well\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Try to load the associated JSON file if available\n",
        "        json_filepath = os.path.join(subfolder_path, latest_file.replace('.pth', '.json'))\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            # If the JSON file exists, load its contents\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(\"Data loaded!\")\n",
        "        else:\n",
        "            # If no JSON file exists, print a message\n",
        "            print(\"No data found\")\n",
        "\n",
        "        # Print the epoch from which the model is resuming\n",
        "        print(f\"Checkpoint found: Resuming from epoch {checkpoint['epoch'] + 1}\\n\\n\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    # If no checkpoint is found, print a message and start from epoch 1\n",
        "    print(\"No checkpoint found, starting from epoch 1..\\n\\n\")\n",
        "    return 1, None  # Epoch starts from 1\n",
        "\n",
        "\n",
        "def delete_existing_checkpoints(subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Deletes all existing checkpoints in the specified subfolder.\n",
        "\n",
        "    Arguments:\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to delete checkpoints from.\n",
        "    \"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if os.path.exists(subfolder_path):\n",
        "        for file_name in os.listdir(subfolder_path):\n",
        "            file_path = os.path.join(subfolder_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        print(f\"All existing checkpoints in {subfolder_path} have been deleted.\")\n",
        "    else:\n",
        "        print(f\"No checkpoint folder found at {subfolder_path}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7bNGf9nwFcf"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6ArbBrCwG7l"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\"\"\"\n",
        "Model architecture for the CIFAR-100 dataset.\n",
        "The model is based on the LeNet-5 architecture with some modifications.\n",
        "Reference: Hsu et al., Federated Visual Classification with Real-World Data Distribution, ECCV 2020\n",
        "\n",
        "CNN similar to LeNet5 which has two 5×5, 64-channel convolution layers, each precedes a 2×2\n",
        "max-pooling layer, followed by two fully-connected layers with 384 and 192\n",
        "channels respectively and finally a softmax linear classifier\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\"\"\"\n",
        "Model architecture for the CIFAR-100 dataset.\n",
        "The model is based on the LeNet-5 architecture with some modifications.\n",
        "Reference: Hsu et al., Federated Visual Classification with Real-World Data Distribution, ECCV 2020\n",
        "\n",
        "CNN similar to LeNet5 which has two 5×5, 64-channel convolution layers, each precedes a 2×2\n",
        "max-pooling layer, followed by two fully-connected layers with 384 and 192\n",
        "channels respectively and finally a softmax linear classifier\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(64 * 3 * 3, 384),  # Updated to be consistent with data augmentation\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 192),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(192, 100)  # 100 classes for CIFAR-100\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output of the conv layers\n",
        "        x = self.fc_layer(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SveGFpEsxCkK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44DzyWDJJjs5"
      },
      "source": [
        "### Data Sharding for IID (Indipendent and Identically Distributed) FL Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoQkps10Irdl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sharding(dataset, number_of_clients, number_of_classes=100):\n",
        "    \"\"\"\n",
        "    Function that performs the sharding of the dataset given as input.\n",
        "    dataset: dataset to be split;\n",
        "    number_of_clients: the number of partitions we want to obtain;\n",
        "    number_of_classes: (int) the number of classes inside each partition, or 100 for IID;\n",
        "    \"\"\"\n",
        "\n",
        "    # Validation of input parameters\n",
        "    if not (1 <= number_of_classes <= 100):\n",
        "        raise ValueError(\"number_of_classes should be an integer between 1 and 100\")\n",
        "\n",
        "    # Shuffle dataset indices for randomness\n",
        "    indices = np.random.permutation(len(dataset))\n",
        "\n",
        "    # Compute basic partition sizes\n",
        "    basic_partition_size = len(dataset) // number_of_clients\n",
        "    remainder = len(dataset) % number_of_clients\n",
        "\n",
        "    shards = []\n",
        "    start_idx = 0\n",
        "\n",
        "    if number_of_classes == 100:  # IID Case\n",
        "        # Equally distribute indices among clients: we can just randomly assign to each client an equal amount of records\n",
        "        for i in range(number_of_clients):\n",
        "            end_idx = start_idx + basic_partition_size + (1 if i < remainder else 0)\n",
        "            shards.append(Subset(dataset, indices[start_idx:end_idx]))\n",
        "            start_idx = end_idx\n",
        "    else:  # non-IID Case\n",
        "        # Count of each class in the dataset\n",
        "        from collections import Counter\n",
        "        target_counts = Counter(target for _, target in dataset)\n",
        "\n",
        "        # Calculate per client class allocation\n",
        "        class_per_client = np.random.choice(list(target_counts.keys()), size=number_of_classes, replace=False)\n",
        "        class_idx = {class_: np.where([target == class_ for _, target in dataset])[0] for class_ in class_per_client}\n",
        "\n",
        "        # Assign class indices evenly to clients\n",
        "        for i in range(number_of_clients):\n",
        "            client_indices = np.array([], dtype=int)\n",
        "            for class_ in class_per_client:\n",
        "                n_samples = len(class_idx[class_]) // number_of_clients + (1 if i < remainder else 0)\n",
        "                client_indices = np.concatenate((client_indices, class_idx[class_][:n_samples]))\n",
        "                class_idx[class_] = np.delete(class_idx[class_], np.arange(n_samples))\n",
        "\n",
        "            shards.append(Subset(dataset, indices=client_indices))\n",
        "\n",
        "    return shards"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to estimate data distributions between clients"
      ],
      "metadata": {
        "id": "SjVskkWgHcZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_clients(global_model, shards, epochs, batch_size, device):\n",
        "    \"\"\"\n",
        "    Pre-trains the model locally on each client's data to estimate distributions.\n",
        "    Args:\n",
        "        global_model: The model to pre-train.\n",
        "        shards: List of client datasets.\n",
        "        epochs: Number of local training epochs.\n",
        "        batch_size: Batch size for local training.\n",
        "        device: Device (CPU/GPU).\n",
        "    Returns:\n",
        "        A dictionary mapping client IDs to their estimated distributions.\n",
        "    \"\"\"\n",
        "    client_distributions = {}\n",
        "    for client_id, shard in enumerate(shards):\n",
        "        local_loader = DataLoader(shard, batch_size=batch_size, shuffle=True)\n",
        "        local_model = deepcopy(global_model).to(device)\n",
        "        optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9)\n",
        "        local_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            for data, targets in local_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = local_model(data)\n",
        "                loss = nn.CrossEntropyLoss()(output, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        # Extract the distribution (e.g., classifier weights or softmax predictions)\n",
        "        client_distributions[client_id] = local_model.fc_layer[-1].weight.detach().cpu().numpy()\n",
        "    return client_distributions"
      ],
      "metadata": {
        "id": "rIHQVK6wHiIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Client class"
      ],
      "metadata": {
        "id": "1Z0HbBmlngSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.backends import cudnn\n",
        "import time\n",
        "\n",
        "\n",
        "class Client:\n",
        "    def __init__(self, client_id, data_loader, model, device):\n",
        "        \"\"\"\n",
        "        Initializes a federated learning client.\n",
        "        :param client_id: Unique identifier for the client.\n",
        "        :param data_loader: Data loader specific to the client.\n",
        "        :param model: The model class to be used by the client.\n",
        "        :param device: The device (CPU/GPU) to perform computations.\n",
        "        \"\"\"\n",
        "        self.client_id = client_id\n",
        "        self.data_loader = data_loader\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def client_update(self, client_data, criterion, optimizer, local_steps=4, detailed_print=False):\n",
        "        \"\"\"\n",
        "        Trains a given client's local model on its dataset for a fixed number of steps (`local_steps`).\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): The local model to be updated.\n",
        "            client_id (int): Identifier for the client (used for logging/debugging purposes).\n",
        "            client_data (DataLoader): The data loader for the client's dataset.\n",
        "            criterion (Loss): The loss function used for training (e.g., CrossEntropyLoss).\n",
        "            optimizer (Optimizer): The optimizer used for updating model parameters (e.g., SGD).\n",
        "            local_steps (int): Number of local epochs to train on the client's dataset.\n",
        "            detailed_print (bool): If True, logs the final loss after training.\n",
        "\n",
        "        Returns:\n",
        "            dict: The state dictionary of the updated model.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.model.train()  # Set the model to training mode\n",
        "        step_count = 0\n",
        "        while step_count < local_steps:\n",
        "            for data, targets in client_data:\n",
        "                # Move data and targets to the specified device (e.g., GPU or CPU)\n",
        "                data, targets = data.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "\n",
        "                start_time = time.time()  # for testing-----------------------------\n",
        "\n",
        "                # Reset the gradients before backpropagation\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass: compute model predictions\n",
        "                outputs = self.model(data)\n",
        "\n",
        "                # Compute the loss\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                # Backward pass: compute gradients and update weights\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # for testing ------------------------------------------------------\n",
        "                if detailed_print:\n",
        "                  end_time = time.time()  # Record the end time\n",
        "                  elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "                  print(f'Single step time taken: {elapsed_time:.4f} seconds')\n",
        "\n",
        "                step_count +=1\n",
        "                if step_count >= local_steps:\n",
        "                  break\n",
        "\n",
        "        # Optionally, print the loss for the last epoch of training\n",
        "        if detailed_print:\n",
        "          print(f'Client {self.client_id} --> Final Loss (Step {step_count}/{local_steps}): {loss.item()}')\n",
        "\n",
        "\n",
        "        # Return the updated model's state dictionary (weights)\n",
        "        return self.model.state_dict()"
      ],
      "metadata": {
        "id": "q1R5FNBnniF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed0w2OeTKFiQ"
      },
      "source": [
        "# Superclient class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Superclient:\n",
        "    def __init__(self, client_ids, data_distributions):\n",
        "        \"\"\"\n",
        "        Represents a superclient made of a group of clients.\n",
        "        Args:\n",
        "            client_ids: List of client IDs in the superclient.\n",
        "            data_distributions: Estimated data distributions for each client.\n",
        "        \"\"\"\n",
        "        self.client_ids = client_ids\n",
        "        self.data_distributions = data_distributions\n",
        "\n",
        "    def train_sequentially(self, global_model, epochs_per_client, data_loader_map, device):\n",
        "        \"\"\"\n",
        "        Trains the global model sequentially across all clients in the superclient.\n",
        "        Args:\n",
        "            global_model: The model to train.\n",
        "            epochs_per_client: Number of epochs per client.\n",
        "            data_loader_map: A map of client IDs to their respective DataLoaders.\n",
        "            device: The device (CPU/GPU) to use for training.\n",
        "        Returns:\n",
        "            Updated global model after training.\n",
        "        \"\"\"\n",
        "        global_model = global_model.to(device)\n",
        "        for client_id in self.client_ids:\n",
        "            local_loader = data_loader_map[client_id]\n",
        "            optimizer = torch.optim.SGD(global_model.parameters(), lr=0.01, momentum=0.9)\n",
        "            global_model.train()\n",
        "            for epoch in range(epochs_per_client):\n",
        "                for data, targets in local_loader:\n",
        "                    data, targets = data.to(device), targets.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    output = global_model(data)\n",
        "                    loss = nn.CrossEntropyLoss()(output, targets)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "        return global_model"
      ],
      "metadata": {
        "id": "SPkwk6WMHW7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group function for grouping multiple clients inside the same superclient"
      ],
      "metadata": {
        "id": "bXwt7wzvHmaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def create_superclients(client_distributions, num_superclients):\n",
        "    \"\"\"\n",
        "    Groups clients into superclients based on their data distributions.\n",
        "    Args:\n",
        "        client_distributions: Dictionary of client distributions (should be flattened).\n",
        "        num_superclients: Number of superclients to form.\n",
        "    Returns:\n",
        "        List of Superclient objects.\n",
        "    \"\"\"\n",
        "    # Flatten each distribution to make it compatible with KMeans\n",
        "    distributions = [dist.flatten() for dist in client_distributions.values()]  # Flatten distributions\n",
        "    client_ids = list(client_distributions.keys())\n",
        "\n",
        "    # Perform clustering with KMeans\n",
        "    kmeans = KMeans(n_clusters=num_superclients, random_state=42)\n",
        "    labels = kmeans.fit_predict(distributions)\n",
        "\n",
        "    # Group clients into superclients based on KMeans labels\n",
        "    superclients = []\n",
        "    for label in set(labels):\n",
        "        grouped_clients = [client_ids[i] for i in range(len(labels)) if labels[i] == label]\n",
        "        superclients.append(Superclient(grouped_clients, [client_distributions[c] for c in grouped_clients]))\n",
        "    return superclients\n"
      ],
      "metadata": {
        "id": "q-mqypnyHrRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Server class"
      ],
      "metadata": {
        "id": "2jum6Gwpo8sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Server:\n",
        "    def __init__(self, global_model):\n",
        "        self.global_model = global_model\n",
        "\n",
        "    def fedavg_aggregate(self, client_states, client_sizes):\n",
        "        # Aggregation logic\n",
        "        new_state = deepcopy(self.global_model.state_dict())\n",
        "        total_samples = sum(client_sizes)\n",
        "        for key in new_state:\n",
        "            new_state[key] = torch.zeros_like(new_state[key])\n",
        "        for state, size in zip(client_states, client_sizes):\n",
        "            for key in new_state:\n",
        "                new_state[key] += state[key] * size / total_samples\n",
        "        return new_state\n",
        "\n",
        "\n",
        "\n",
        "    # Federated Learning Training Loop\n",
        "    def train_federated(self, criterion, trainloader, validloader, num_clients, num_classes, rounds, lr, momentum, batchsize, wd, C=0.1, local_steps=4, log_freq=10, detailed_print=False):\n",
        "        val_accuracies = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        train_losses = []\n",
        "        best_model_state = None  # The model with the best accuracy\n",
        "        client_selection_count = [0] * num_clients #Count how many times a client has been selected\n",
        "        best_val_acc = 0.0\n",
        "\n",
        "        shards = sharding(trainloader.dataset, num_clients, num_classes) #each shard represent the training data for one client\n",
        "        client_sizes = [len(shard) for shard in shards]\n",
        "\n",
        "        self.global_model.to(DEVICE) #as alwayse, we move the global model to the specified device (CPU or GPU)\n",
        "\n",
        "        #loading checkpoint if it exists\n",
        "        checkpoint_start_step, data_to_load = load_checkpoint(model=global_model,optimizer=None,hyperparameters=f\"LR{lr}_WD{wd}\", subfolder=\"Federated/\")\n",
        "        if data_to_load is not None:\n",
        "          val_accuracies = data_to_load['val_accuracies']\n",
        "          val_losses = data_to_load['val_losses']\n",
        "          train_accuracies = data_to_load['train_accuracies']\n",
        "          train_losses = data_to_load['train_losses']\n",
        "          client_selection_count = data_to_load['client_selection_count']\n",
        "\n",
        "\n",
        "        # ********************* HOW IT WORKS ***************************************\n",
        "        # The training runs for rounds iterations (GLOBAL_ROUNDS=2000)\n",
        "        # Each round simulates one communication step in federated learning, including:\n",
        "        # 1) client selection\n",
        "        # 2) local training (of each client)\n",
        "        # 3) central aggregation\n",
        "        for round_num in range(checkpoint_start_step, rounds):\n",
        "            if round_num % log_freq == 0:\n",
        "              print(f\"------------------------------------- Round {round_num} ------------------------------------------------\" )\n",
        "\n",
        "            #start_time = time.time()  # for testing-----------------------------\n",
        "\n",
        "            # Pretrain clients to estimate distributions\n",
        "            client_distributions = pretrain_clients(self.global_model, shards, epochs=5, batch_size=batchsize, device=DEVICE)\n",
        "\n",
        "\n",
        "            # Create superclients\n",
        "            superclients = create_superclients(client_distributions, num_superclients=10)\n",
        "\n",
        "            # Select a random fraction of superclients\n",
        "            selected_superclients = random.sample(superclients, int(C * len(superclients)))\n",
        "            for superclient in selected_superclients:\n",
        "                self.global_model = superclient.train_sequentially(\n",
        "                    self.global_model, epochs_per_client=local_steps, data_loader_map={i: DataLoader(shards[i], batch_size=batchsize, shuffle=True) for i in range(num_clients)}, device=DEVICE\n",
        "                )\n",
        "            # Central aggregation (FedAvg)\n",
        "            aggregated_state = self.fedavg_aggregate([deepcopy(self.global_model.state_dict())], [len(sc.client_ids) for sc in selected_superclients])\n",
        "            self.global_model.load_state_dict(aggregated_state)\n",
        "\n",
        "            #Validation at the server\n",
        "            #if round_num % log_freq:\n",
        "            val_accuracy, val_loss = evaluate(self.global_model, validloader,criterion)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            val_losses.append(val_loss)\n",
        "            if val_accuracy > best_val_acc:\n",
        "                best_val_acc = val_accuracy\n",
        "                best_model_state = deepcopy(self.global_model.state_dict())\n",
        "\n",
        "            if round_num % log_freq == 0:\n",
        "                train_accuracy, train_loss = evaluate(self.global_model, trainloader,criterion)\n",
        "                train_accuracies.append(train_accuracy)\n",
        "                train_losses.append(train_loss)\n",
        "\n",
        "                print(f\"--> best validation accuracy: {best_val_acc}\\n--> training accuracy: {train_accuracy}\")\n",
        "                print(f\"--> validation loss: {val_loss}\\n--> training loss: {train_loss}\")\n",
        "\n",
        "                # checkpointing\n",
        "                checkpoint_data = {\n",
        "                    'val_accuracies': val_accuracies,\n",
        "                    'val_losses': val_losses,\n",
        "                    'train_accuracies': train_accuracies,\n",
        "                    'train_losses': train_losses,\n",
        "                    'client_selection_count': client_selection_count\n",
        "                }\n",
        "                save_checkpoint(model=self.global_model, optimizer=None, epoch=round_num, hyperparameters=f\"LR{lr}_WD{wd}\", subfolder=\"Federated/\", checkpoint_data=checkpoint_data)\n",
        "\n",
        "                print(f\"------------------------------ Round {round_num} terminated: model updated -----------------------------\\n\\n\" )\n",
        "\n",
        "\n",
        "            # for testing ------------------------------------------------------\n",
        "            #end_time = time.time()  # Record the end time\n",
        "            #elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "            #print(f'Single round time taken: {elapsed_time:.4f} seconds\\n\\n')\n",
        "\n",
        "\n",
        "        global_model.load_state_dict(best_model_state)\n",
        "\n",
        "        return global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count\n"
      ],
      "metadata": {
        "id": "luRcM2WXo-jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZyWmIM49-XE"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.train(False) # Set Network to evaluation mode\n",
        "        running_corrects = 0\n",
        "        losses = []\n",
        "\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(DEVICE)        # Move the data to the GPU\n",
        "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == targets.data).data.item()\n",
        "            # Calculate Accuracy\n",
        "            accuracy = running_corrects / float(len(dataloader.dataset))\n",
        "\n",
        "    return accuracy, mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY8ptTgOKnQN"
      },
      "source": [
        "### Federated Learning Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHOLidiTLHz0"
      },
      "source": [
        "### Initialize Model & Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5aKGpEELHYf"
      },
      "outputs": [],
      "source": [
        "global_model = LeNet5()\n",
        "criterion = nn.NLLLoss()# our loss function for classification tasks on CIFAR-100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGk6k4GPLZy0"
      },
      "source": [
        "### Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3kXEc1etLgq6",
        "outputId": "a9224fc1-ba7d-49c8-84b7-2a4a58ef28a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found, starting from epoch 1.\n",
            "------------------------------------- Round 10 ------------------------------------------------\n",
            "--> best validation accuracy: 0.1008\n",
            "--> training accuracy: 0.05622222222222222\n",
            "--> validation loss: 4.133662990703704\n",
            "--> training loss: 4.1728510494001725\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_10_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 10 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 20 ------------------------------------------------\n",
            "--> best validation accuracy: 0.1204\n",
            "--> training accuracy: 0.10773333333333333\n",
            "--> validation loss: 3.7497503210784524\n",
            "--> training loss: 3.7959591761157285\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_20_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 20 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 30 ------------------------------------------------\n",
            "--> best validation accuracy: 0.1488\n",
            "--> training accuracy: 0.12717777777777778\n",
            "--> validation loss: 3.6373274599670604\n",
            "--> training loss: 3.7168173803534166\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_30_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 30 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 40 ------------------------------------------------\n",
            "--> best validation accuracy: 0.1568\n",
            "--> training accuracy: 0.13204444444444444\n",
            "--> validation loss: 3.6140721132800837\n",
            "--> training loss: 3.7028672151219872\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_40_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 40 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 50 ------------------------------------------------\n",
            "--> best validation accuracy: 0.1794\n",
            "--> training accuracy: 0.11175555555555555\n",
            "--> validation loss: 3.825089319496398\n",
            "--> training loss: 3.922757178905083\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_50_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 50 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 60 ------------------------------------------------\n",
            "--> best validation accuracy: 0.1794\n",
            "--> training accuracy: 0.12784444444444445\n",
            "--> validation loss: 3.642849615425061\n",
            "--> training loss: 3.7280107562899505\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_60_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 60 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 70 ------------------------------------------------\n",
            "--> best validation accuracy: 0.1794\n",
            "--> training accuracy: 0.14342222222222223\n",
            "--> validation loss: 3.5736748048454334\n",
            "--> training loss: 3.6520161052587277\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_70_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 70 terminated: model updated -----------------------------\n",
            "\n",
            "\n",
            "------------------------------------- Round 80 ------------------------------------------------\n",
            "--> best validation accuracy: 0.1794\n",
            "--> training accuracy: 0.15088888888888888\n",
            "--> validation loss: 3.5417943684158812\n",
            "--> training loss: 3.6566099635912495\n",
            "Checkpoint saved: ../checkpoints/Federated/model_epoch_80_params_LR0.01_WD0.0001.pth\n",
            "------------------------------ Round 80 terminated: model updated -----------------------------\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4f7782e4acfb>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#run federeted learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrainloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-e87b85759ee3>\u001b[0m in \u001b[0;36mtrain_federated\u001b[0;34m(self, criterion, trainloader, validloader, num_clients, num_classes, rounds, lr, momentum, batchsize, wd, C, local_steps, log_freq, detailed_print)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Pretrain clients to estimate distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mclient_distributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrain_clients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b305611f0fef>\u001b[0m in \u001b[0;36mpretrain_clients\u001b[0;34m(global_model, shards, epochs, batch_size, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocal_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"std evaluated to zero after conversion to {dtype}, leading to division by zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#just for now\n",
        "lr = LR\n",
        "wd = WEIGHT_DECAY\n",
        "#delete_existing_checkpoints(\"Federated/\")\n",
        "# Run Federated Learning\n",
        "# Instantiate the server\n",
        "server = Server(global_model)\n",
        "#run federeted learning\n",
        "global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(\n",
        "    criterion=criterion,\n",
        "    trainloader=trainloader,\n",
        "    validloader=validloader,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    num_classes=100,\n",
        "    rounds=GLOBAL_ROUNDS,\n",
        "    lr=lr,\n",
        "    momentum=MOMENTUM,\n",
        "    batchsize=BATCH_SIZE,\n",
        "    wd=wd,\n",
        "    C=FRACTION_CLIENTS,\n",
        "    local_steps=LOCAL_STEPS,\n",
        "    log_freq=LOG_FREQUENCY,\n",
        "    detailed_print=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WqJIdzei1Y0"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmNUVz0mjAJ7"
      },
      "source": [
        "### Run the test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR948jsajH5g"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate(global_model, testloader, criterion)[0]\n",
        "print('\\nTest Accuracy: {}'.format(accuracy))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}