{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWAhRJt5_i3e"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3okGAXgq_i3j",
        "outputId": "a522c09a-3d83-499d-e481-e0e65c70d1fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import string\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import io\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from google.colab import files\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTmrobfeI5Q7"
      },
      "source": [
        "# Import the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TD59XwRI7PL"
      },
      "source": [
        "We must import the dataset manually since it is taken by the LEAF project.\n",
        "\n",
        "So far the project is to go on the data folder of shakespeare and:\n",
        "1. ./get_data.sh inside the preprocess folder\n",
        "2. ./data_to_json.sh\n",
        "3. cd ..\n",
        "3. ././preprocess.sh -s niid --sf 0.2 -k 0 -t sample -tf 0.8 [depending on the preferencies]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM07ejLQKP3u"
      },
      "source": [
        "## Upload the training and the testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "CYwrsHfkPPvj",
        "outputId": "0b607ace-2d06-4138-c6dc-5585336f56a4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-80a6e9c9-57c4-4d29-9924-047221bec0c1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-80a6e9c9-57c4-4d29-9924-047221bec0c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving all_data_iid_089_06.json to all_data_iid_089_06 (2).json\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()\n",
        "file_train = next(iter(uploaded))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "nVl9FFoFlKZ2",
        "outputId": "0107e60b-0b98-4ceb-bfa9-ebc9964cdd6a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6892cc3d-4ed1-4e19-925e-ec6ca7cfeb8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6892cc3d-4ed1-4e19-925e-ec6ca7cfeb8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving all_data_iid_089_06_test_8.json to all_data_iid_089_06_test_8.json\n"
          ]
        }
      ],
      "source": [
        "uploaded2 = files.upload()\n",
        "file_test = next(iter(uploaded2))\n",
        "\n",
        "data = json.load(io.BytesIO(uploaded[file_train]))\n",
        "\n",
        "test_data  = json.load(io.BytesIO(uploaded2[file_test]))\n",
        "\n",
        "with open(file_train, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "with open(file_test, 'r') as f:\n",
        "    test_data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCGPeiE9K5Ve"
      },
      "source": [
        "## Statistics of the dataset\n",
        "Just for testing porpouses we can print some statistics about the uploaded dataset.\n",
        "\n",
        "The values used for the train/test split and the number of his samples are inspired by:\n",
        "Acar, Durmus Alp Emre, et al. \"Federated learning based on dynamic regularization.\" arXiv preprint arXiv:2111.04263 (2021)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVm6X0uN3PO",
        "outputId": "3dd0972c-66f3-4d03-f4d8-7e3fb786909c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of train samples: 253569\n",
            "Total number of test samples: 50769\n"
          ]
        }
      ],
      "source": [
        "total_samples = sum(data['num_samples'])\n",
        "print(f\"Total number of train samples: {total_samples}\")\n",
        "\n",
        "total_samples = sum(test_data['num_samples'])\n",
        "print(f\"Total number of test samples: {total_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlmGoKJWOCN8"
      },
      "outputs": [],
      "source": [
        "users = data['users']\n",
        "num_samples = data['num_samples']\n",
        "user_data = data['user_data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir8cweSzR-FO",
        "outputId": "85da940b-faff-4bab-9922-375522855e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of clients: 100\n"
          ]
        }
      ],
      "source": [
        "number_of_clients = len(users)\n",
        "print(f\"Number of clients: {number_of_clients}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkuz0ZEULp-O"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOrgnNFvc3MR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The batch size has been inspired by:\n",
        "Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "Jakub Konečný, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021.\n",
        "'''\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "wd = 0.0001\n",
        "momentum = 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy1FP3iYSGoD"
      },
      "source": [
        "## Vocab creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scMFoC0WSI3r"
      },
      "outputs": [],
      "source": [
        "all_texts = ''.join([''.join(seq) for user in users for seq in user_data[user]['x']])\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "\n",
        "# Add the padding character\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi9H7dJNSwoc"
      },
      "source": [
        "## Covert data into indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZaJF07uSy2W"
      },
      "outputs": [],
      "source": [
        "inputs = [[char_to_idx[char] for char in user_data[user]['x'][0]] for user in users]\n",
        "targets = [[char_to_idx[char] for char in user_data[user]['y'][0]] for user in users]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P_okFPTTAls"
      },
      "source": [
        "## Creation of TensorDataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysDl6DJNUC_d"
      },
      "outputs": [],
      "source": [
        "input_tensors = [torch.tensor(seq) for seq in inputs]\n",
        "target_tensors = [torch.tensor([seq]) for seq in targets]\n",
        "\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
        "\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "\n",
        "target_tensors = torch.cat(target_tensors, dim=0)\n",
        "\n",
        "dataset = TensorDataset(padded_inputs, target_tensors)\n",
        "batch_size = 4\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N94bWSOHZIsd"
      },
      "outputs": [],
      "source": [
        "def tensor_to_string(tensor, idx_to_char):\n",
        "    \"\"\"Converte un tensore di indici in una stringa di caratteri.\"\"\"\n",
        "    return ''.join(idx_to_char[idx.item()] for idx in tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOSOJnQXVYiZ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAPXXX77VYHr"
      },
      "outputs": [],
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level LSTM model for text processing tasks.\n",
        "    Includes embedding, LSTM, and a fully connected output layer.\n",
        "    We use:\n",
        "    - embedding size equal to 8;\n",
        "    - 2 LSTM layers, each with 256 nodes;\n",
        "    - densely connected softmax output layer.\n",
        "\n",
        "    We can avoid to use explicitly the softmax function in the model and\n",
        "    keep a cross entropy loss function as a loss function.\n",
        "\n",
        "    as mentioned in paper [2] (Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "    Jakub Konečný, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size = 70, embedding_size = 8, lstm_hidden_dim = 256, seq_length=80):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
        "        self.lstm1 = nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size=lstm_hidden_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \"\"\"\n",
        "        # Layer 1: Embedding\n",
        "        x = self.embedding(x)  # Output shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Layer 2: First LSTM\n",
        "        x, _ = self.lstm1(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 3: Second LSTM\n",
        "        x, hidden = self.lstm2(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 4: Fully Connected Layer\n",
        "        x = self.fc(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Softmax Activation\n",
        "        #x = self.softmax(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "        return x[:, -1, :], hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden and cell states for the LSTM.\"\"\"\n",
        "        return (torch.zeros(2, batch_size, self.lstm_hidden_dim),\n",
        "            torch.zeros(2, batch_size, self.lstm_hidden_dim))\n",
        "        #2 is equal to the number of lstm layers!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iY7b3pVVc62"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFq79CKNy8EW"
      },
      "outputs": [],
      "source": [
        "# Function to convert character in indices:\n",
        "# def char_to_tensor(characters):\n",
        "#     indices = [char_to_idx[char] for char in characters]\n",
        "#     return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "def char_to_tensor(characters):\n",
        "    indices = [char_to_idx.get(char, char_to_idx['<pad>']) for char in characters] # Get the index for the character. If not found, use the index for padding.\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Prepare the test samples:\n",
        "'''\n",
        "The leaf dataset is structured in the following way:\n",
        "Users: Each dataset in LEAF is distributed across a simulated set of users (playing actor). The data for\n",
        "each user is stored separately to mimic real-world scenarios where data is distributed\n",
        "across devices or clients.\n",
        "Data Format: For each user, the data include:\n",
        "    x: sentences declared by the \"user\"/\"device\".\n",
        "    y: Labels or outputs associated with the inputs.\n",
        "'''\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in data['users']:\n",
        "    for entry, target in zip(data['user_data'][user]['x'], data['user_data'][user]['y']):\n",
        "        input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
        "        target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
        "\n",
        "# Padding and creation ofDataLoader\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "targets = torch.cat(target_tensors)\n",
        "dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhwIgYH9r5Cn"
      },
      "source": [
        "# Checkpointing management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_pKNLFEr65X",
        "outputId": "3685a883-7998-4d4d-b4b0-a0f9451ce4c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive.mount('/content/drive')\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/colab_checkpoints/'\n",
        "\n",
        "\n",
        "# Ensure the checkpoint directory exists, creating it if necessary\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, train_accuracies, train_losses, subfolder=\"\", checkpoint_data=None):\n",
        "    \"\"\"\n",
        "    Saves the model checkpoint and removes the previous one if it exists.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state is to be saved.\n",
        "    optimizer -- The optimizer whose state is to be saved (can be None).\n",
        "    epoch -- The current epoch of the training process.\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    train_accuracies -- List of training accuracies to save.\n",
        "    train_losses -- List of training losses to save.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to save the checkpoint.\n",
        "    checkpoint_data -- Additional data to save in a JSON file (e.g., training logs).\n",
        "    \"\"\"\n",
        "    # Define the path for the subfolder where checkpoints will be stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # Construct filenames for both the model checkpoint and the associated JSON file\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "    # Define the filenames for the previous checkpoint files\n",
        "    previous_filepath = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.pth\")\n",
        "    previous_filepath_json = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.json\")\n",
        "\n",
        "    # Remove the previous checkpoint if it exists\n",
        "    if epoch >= 1:\n",
        "        if os.path.exists(previous_filepath):\n",
        "            os.remove(previous_filepath)\n",
        "        if os.path.exists(previous_filepath_json):\n",
        "            os.remove(previous_filepath_json)\n",
        "\n",
        "    # Prepare the checkpoint data dictionary\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'train_losses': train_losses\n",
        "    }\n",
        "    if optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    # Save the model and optimizer state dictionary to the checkpoint file\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    # Save additional data to a JSON file\n",
        "    if checkpoint_data:\n",
        "        with open(filepath_json, 'w') as json_file:\n",
        "            json.dump(checkpoint_data, json_file, indent=4)\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Loads the latest checkpoint available based on the specified hyperparameters.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state will be updated from the checkpoint.\n",
        "    optimizer -- The optimizer whose state will be updated from the checkpoint (can be None).\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to look for checkpoints.\n",
        "\n",
        "    Returns:\n",
        "    The next epoch to resume from, train_accuracies, train_losses, and the associated JSON data if available.\n",
        "    \"\"\"\n",
        "    # Define the path to the subfolder where checkpoints are stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint directory found, starting from epoch 1.\")\n",
        "        return 1, [], [], None\n",
        "\n",
        "    # Search for checkpoint files in the subfolder that match the hyperparameters\n",
        "    files = [f for f in os.listdir(subfolder_path) if f.endswith('.pth')]\n",
        "\n",
        "    if files:\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))  # Find the latest epoch file\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        # Load the model state from the checkpoint\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        # If an optimizer is provided, load its state as well\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Load training accuracies and losses\n",
        "        train_accuracies = checkpoint.get('train_accuracies', [])\n",
        "        train_losses = checkpoint.get('train_losses', [])\n",
        "\n",
        "        # Try to load the associated JSON file if available\n",
        "        json_filepath = os.path.join(subfolder_path, latest_file.replace('.pth', '.json'))\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "\n",
        "        print(f\"Checkpoint found: Resuming from epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1, train_accuracies, train_losses, json_data\n",
        "\n",
        "    print(\"No checkpoint found, starting from epoch 1.\")\n",
        "    return 1, [], [], None\n",
        "\n",
        "\n",
        "def delete_existing_checkpoints(subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Deletes all existing checkpoints in the specified subfolder.\n",
        "\n",
        "    Arguments:\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to delete checkpoints from.\n",
        "    \"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if os.path.exists(subfolder_path):\n",
        "        for file_name in os.listdir(subfolder_path):\n",
        "            file_path = os.path.join(subfolder_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        print(f\"All existing checkpoints in {subfolder_path} have been deleted.\")\n",
        "    else:\n",
        "        print(f\"No checkpoint folder found at {subfolder_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwCJcIPLL5qm",
        "outputId": "6a742ee9-daac-4551-f22f-a3c07ec78afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsWEZS4kL92C",
        "outputId": "2c092575-f02f-4de2-ad0e-9ec2037fc9c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/My Drive/colab_checkpoints/Federated/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/My Drive/colab_checkpoints/Federated/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzEZbh5BvJXs"
      },
      "source": [
        "# Plot function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9fMwjCJvLPU"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(train_accuracies, train_losses, file_name):\n",
        "    \"\"\"\n",
        "    Plot the training metrics.\n",
        "\n",
        "    Args:\n",
        "        train_accuracies (list): List of training accuracies.\n",
        "        train_losses (list): List of training losses.\n",
        "        file_name (str): Name of the file to save the plot.\n",
        "    \"\"\"\n",
        "    DIR = '/content/drive/MyDrive/colab_plots/'\n",
        "    # Fixed base directory\n",
        "    directory = DIR + '/plots_federated/'\n",
        "    # Ensure the base directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Create a list of epochs for the x-axis\n",
        "    epochs = list(range(1, len(train_losses) + 1))\n",
        "\n",
        "    # Plot the training and validation losses\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
        "    plt.xlabel('Rounds', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.title('Training and Validation Loss', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path.replace('.png', '_loss.png'), format='png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the training and validation accuracies\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n",
        "    plt.xlabel('Rounds', fontsize=14)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.title('Training and Validation Accuracy', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path.replace('.png', '_accuracy.png'), format='png', dpi=300)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "suVFX7jpVbiX",
        "outputId": "14994364-8b69-4d87-f615-89045e1958f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_0_params_LR0.001.pth\n",
            "Epoch 1/200, Loss: 2.8118, Accuracy: 0.2516, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_1_params_LR0.001.pth\n",
            "Epoch 2/200, Loss: 2.2792, Accuracy: 0.3624, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_2_params_LR0.001.pth\n",
            "Epoch 3/200, Loss: 2.0491, Accuracy: 0.4162, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_3_params_LR0.001.pth\n",
            "Epoch 4/200, Loss: 1.9187, Accuracy: 0.4461, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_4_params_LR0.001.pth\n",
            "Epoch 5/200, Loss: 1.8330, Accuracy: 0.4672, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_5_params_LR0.001.pth\n",
            "Epoch 6/200, Loss: 1.7682, Accuracy: 0.4830, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_6_params_LR0.001.pth\n",
            "Epoch 7/200, Loss: 1.7164, Accuracy: 0.4951, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_7_params_LR0.001.pth\n",
            "Epoch 8/200, Loss: 1.6733, Accuracy: 0.5052, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_8_params_LR0.001.pth\n",
            "Epoch 9/200, Loss: 1.6365, Accuracy: 0.5138, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_9_params_LR0.001.pth\n",
            "Epoch 10/200, Loss: 1.6039, Accuracy: 0.5221, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_10_params_LR0.001.pth\n",
            "Epoch 11/200, Loss: 1.5743, Accuracy: 0.5291, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_11_params_LR0.001.pth\n",
            "Epoch 12/200, Loss: 1.5470, Accuracy: 0.5360, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_12_params_LR0.001.pth\n",
            "Epoch 13/200, Loss: 1.5217, Accuracy: 0.5426, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_13_params_LR0.001.pth\n",
            "Epoch 14/200, Loss: 1.4974, Accuracy: 0.5494, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_14_params_LR0.001.pth\n",
            "Epoch 15/200, Loss: 1.4743, Accuracy: 0.5552, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_15_params_LR0.001.pth\n",
            "Epoch 16/200, Loss: 1.4519, Accuracy: 0.5611, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_16_params_LR0.001.pth\n",
            "Epoch 17/200, Loss: 1.4302, Accuracy: 0.5670, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_17_params_LR0.001.pth\n",
            "Epoch 18/200, Loss: 1.4082, Accuracy: 0.5737, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_18_params_LR0.001.pth\n",
            "Epoch 19/200, Loss: 1.3866, Accuracy: 0.5797, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_19_params_LR0.001.pth\n",
            "Epoch 20/200, Loss: 1.3643, Accuracy: 0.5863, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_20_params_LR0.001.pth\n",
            "Epoch 21/200, Loss: 1.3423, Accuracy: 0.5930, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_21_params_LR0.001.pth\n",
            "Epoch 22/200, Loss: 1.3199, Accuracy: 0.5996, %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# Define the Model to use\n",
        "model = CharLSTM(vocab_size=len(char_to_idx))\n",
        "model.train()  # Set the model in training mode\n",
        "model = model.to(DEVICE)  # Move the entire model to the right device\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=200)  # T_max is the number of epochs\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "\n",
        "            # Reset the existing gradients (if any)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Initialize the hidden state:\n",
        "            hidden = model.init_hidden(inputs.size(0))\n",
        "            hidden = (hidden[0].to(DEVICE), hidden[1].to(DEVICE))\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = model(inputs, hidden)\n",
        "\n",
        "            # Calculate loss\n",
        "            outputs_flat = outputs.view(-1, len(char_to_idx))\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = criterion(outputs_flat, targets_flat)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_samples += targets_flat.size(0)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = outputs_flat.max(1)\n",
        "            total_correct += (predicted == targets_flat).sum().item()\n",
        "\n",
        "        # Adjust learning rate based on the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate loss and accuracy for the epoch\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = total_correct / total_samples\n",
        "        train_losses.append(avg_loss)\n",
        "        train_accuracies.append(accuracy)\n",
        "\n",
        "        # Save the checkpoint:\n",
        "        save_checkpoint(model, optimizer=None, epoch=epoch, hyperparameters=f\"LR{lr}\", train_accuracies=train_accuracies, train_losses=train_losses, subfolder=\"Federated/\")\n",
        "\n",
        "        # Print statistics\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, %')\n",
        "\n",
        "# Execute the model:\n",
        "train_model(model, loader, criterion, optimizer, scheduler, num_epochs=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTiz78nbaCRx"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJOZBBEobRE1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in test_data['users']:\n",
        "    for entry, target in zip(test_data['user_data'][user]['x'], test_data['user_data'][user]['y']):\n",
        "        input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
        "        target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
        "\n",
        "\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "targets = torch.cat(target_tensors)\n",
        "test_dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in test_dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, targets in test_loader:\n",
        "    inputs = inputs.to(DEVICE)  # move input to correct dev\n",
        "    targets = targets.to(DEVICE)  # move target to correct dev\n",
        "\n",
        "    # Inizialize the hidden state\n",
        "    hidden = model.init_hidden(inputs.size(0))\n",
        "    hidden = (hidden[0].to(DEVICE), hidden[1].to(DEVICE))  # Move the hidden state to correct dev\n",
        "\n",
        "    outputs, _ = model(inputs, hidden)\n",
        "    outputs_flat = outputs.view(-1, len(char_to_idx))\n",
        "    targets_flat = targets.view(-1)\n",
        "    _, predicted = outputs_flat.max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += (predicted == targets_flat).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}