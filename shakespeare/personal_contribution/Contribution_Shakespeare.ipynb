{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6AYtngaiB0cJ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from Server import Server\n",
        "from Client import Client\n",
        "from Individual import Individual\n",
        "from shakespeare_model import CharLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjnl6b5eDPMY",
        "outputId": "c4d10b74-5e53-4753-fcdb-3b2d12da965d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Constants for FL training\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(DEVICE)\n",
        "NUM_CLIENTS = 1129  # Total number of clients in the federation\n",
        "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
        "LOCAL_STEPS = 4  # Number of local steps (J)\n",
        "GLOBAL_ROUNDS = 2000  # Total number of communication rounds\n",
        "\n",
        "BATCH_SIZE = 100 # Batch size for local training\n",
        "\n",
        "MOMENTUM = 0  # Momentum for SGD optimizer\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/colab_checkpoints/'\n",
        "LOG_FREQUENCY = 10 # Frequency of logging training progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uqI29GCvE8F_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from statistics import mean\n",
        "import torch.nn as nn\n",
        "\n",
        "\"\"\"\n",
        "Utility function used both in the centralized and federated learning\n",
        "Computes the accuracy and the loss on the validation/test set depending on the dataloader passed\n",
        "\"\"\"\n",
        "def evaluate(model, dataloader, criterion, DEVICE):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_corrects = 0\n",
        "    total_samples = 0  # Total samples counter\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "            hidden = model.init_hidden(data.size(0))\n",
        "            hidden = (hidden[0].to(DEVICE), hidden[1].to(DEVICE))\n",
        "            outputs, _ = model(data, hidden)\n",
        "            outputs_flat = outputs.view(-1, model.vocab_size)\n",
        "            targets_flat = targets.view(-1)\n",
        "\n",
        "            loss = criterion(outputs_flat, targets_flat)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            _, preds = outputs_flat.max(1)\n",
        "            #running_corrects += torch.sum(preds == targets_flat).item()\n",
        "            running_corrects += (preds == targets_flat).sum().item()\n",
        "            total_samples += targets_flat.size(0)\n",
        "\n",
        "    accuracy = (running_corrects / total_samples) * 100\n",
        "    return accuracy, sum(losses) / len(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sfb3t6fREwje"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Ensure the checkpoint directory exists, creating it if necessary\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", checkpoint_data=None):\n",
        "    \"\"\"\n",
        "    Saves the model checkpoint and removes the previous one if it exists.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state is to be saved.\n",
        "    optimizer -- The optimizer whose state is to be saved (can be None).\n",
        "    epoch -- The current epoch of the training process.\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to save the checkpoint.\n",
        "    checkpoint_data -- Data to save in a JSON file (e.g., training logs).\n",
        "    \"\"\"\n",
        "    # Define the path for the subfolder where checkpoints will be stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    # Create the subfolder if it doesn't exist\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # Construct filenames for both the model checkpoint and the associated JSON file\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "    # Define the filenames for the previous checkpoint files, to remove them if necessary\n",
        "    previous_filepath = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.pth\")\n",
        "    previous_filepath_json = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.json\")\n",
        "\n",
        "    # Remove the previous checkpoint if it exists, but only for epochs greater than 1\n",
        "    if epoch > 1 and os.path.exists(previous_filepath):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Prepare the checkpoint data dictionary\n",
        "    checkpoint = {'model_state_dict': model.state_dict(), 'epoch': epoch}\n",
        "    # If an optimizer is provided, save its state as well\n",
        "    if optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    # Save the model and optimizer (if provided) state dictionary to the checkpoint file\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    # If additional data (e.g., training logs) is provided, save it to a JSON file\n",
        "    if checkpoint_data:\n",
        "        with open(filepath_json, 'w') as json_file:\n",
        "            json.dump(checkpoint_data, json_file, indent=4)\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Loads the latest checkpoint available based on the specified hyperparameters.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state will be updated from the checkpoint.\n",
        "    optimizer -- The optimizer whose state will be updated from the checkpoint (can be None).\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to look for checkpoints.\n",
        "\n",
        "    Returns:\n",
        "    The next epoch to resume from and the associated JSON data if available.\n",
        "    \"\"\"\n",
        "    # Define the path to the subfolder where checkpoints are stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "\n",
        "    # If the subfolder doesn't exist, print a message and start from epoch 1\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, starting from epoch 1.\")\n",
        "        return 1, None  # Epoch starts from 1\n",
        "\n",
        "    # Search for checkpoint files in the subfolder that match the hyperparameters\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "\n",
        "    # If checkpoint files are found, load the one with the highest epoch number\n",
        "    if files:\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))  # Find the latest epoch file\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath, weights_only=True)\n",
        "\n",
        "        # Load the model state from the checkpoint\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        # If an optimizer is provided, load its state as well\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Try to load the associated JSON file if available\n",
        "        json_filepath = os.path.join(subfolder_path, latest_file.replace('.pth', '.json'))\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            # If the JSON file exists, load its contents\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(\"Data loaded!\")\n",
        "        else:\n",
        "            # If no JSON file exists, print a message\n",
        "            print(\"No data found\")\n",
        "\n",
        "        # Print the epoch from which the model is resuming\n",
        "        print(f\"Checkpoint found: Resuming from epoch {checkpoint['epoch'] + 1}\\n\\n\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    # If no checkpoint is found, print a message and start from epoch 1\n",
        "    print(\"No checkpoint found, starting from epoch 1..\\n\\n\")\n",
        "    return 1, None  # Epoch starts from 1\n",
        "\n",
        "def delete_existing_checkpoints(subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Deletes all existing checkpoints in the specified subfolder.\n",
        "\n",
        "    Arguments:\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to delete checkpoints from.\n",
        "    \"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if os.path.exists(subfolder_path):\n",
        "        for file_name in os.listdir(subfolder_path):\n",
        "            file_path = os.path.join(subfolder_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        print(f\"All existing checkpoints in {subfolder_path} have been deleted.\")\n",
        "    else:\n",
        "        print(f\"No checkpoint folder found at {subfolder_path}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVSI_pV5lfmR"
      },
      "source": [
        "# DataLoading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "WBh6Mzm_lh7w",
        "outputId": "4f04311c-6951-425b-aa6f-da155a3c18a2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-964c9cf1-381b-4f0d-b96c-b3eb31bb6d33\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-964c9cf1-381b-4f0d-b96c-b3eb31bb6d33\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving all_data_iid_089_06_train_8.json to all_data_iid_089_06_train_8.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "RuJV2ApSlltx",
        "outputId": "0a44f511-1fa7-42ec-9abf-b599c9be6384"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fcd41725-cca1-4f92-a7f9-e234bff00201\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fcd41725-cca1-4f92-a7f9-e234bff00201\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving all_data_iid_089_06_test_8.json to all_data_iid_089_06_test_8.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded2 = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "S_lhyxJalp8z"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import json\n",
        "\n",
        "data = json.load(io.BytesIO(uploaded['all_data_iid_089_06_train_8.json']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "vehJPpq_lqeu"
      },
      "outputs": [],
      "source": [
        "test_data  = json.load(io.BytesIO(uploaded2['all_data_iid_089_06_test_8.json']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jQzejNPIlvSs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#Load the Json file\n",
        "with open('all_data_iid_089_06_train_8.json', 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "2P8DB4jllyPf"
      },
      "outputs": [],
      "source": [
        "with open('all_data_iid_089_06_test_8.json', 'r') as f:\n",
        "    test_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbR2pJ1WeuLM",
        "outputId": "a07ebacc-8861-4bae-d640-2ee723afafc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of clients: 100\n"
          ]
        }
      ],
      "source": [
        "num_clients = len(data['users'])\n",
        "print(\"Number of clients:\", num_clients)\n",
        "NUM_CLIENTS = num_clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "i9UliC2qnQUe"
      },
      "outputs": [],
      "source": [
        "users = data['users']\n",
        "num_samples = data['num_samples']\n",
        "user_data = data['user_data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Y1an7TUynSM3"
      },
      "outputs": [],
      "source": [
        "all_texts = ''.join([''.join(seq) for user in users for seq in user_data[user]['x']])\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "\n",
        "# Add the padding character\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDItE3pL82K7"
      },
      "source": [
        "## Convert data into indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "SZPpBhzO84qB"
      },
      "outputs": [],
      "source": [
        "inputs = [[char_to_idx[char] for char in user_data[user]['x'][0]] for user in users]\n",
        "targets = [[char_to_idx[char] for char in user_data[user]['y'][0]] for user in users]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL76d_dj86nc"
      },
      "source": [
        "## Creation of TensorDataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "poVRn7mM8_1f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "input_tensors = [torch.tensor(seq) for seq in inputs]\n",
        "target_tensors = [torch.tensor([seq]) for seq in targets]\n",
        "\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
        "\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "\n",
        "target_tensors = torch.cat(target_tensors, dim=0)\n",
        "\n",
        "dataset = TensorDataset(padded_inputs, target_tensors)\n",
        "batch_size = 4\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "OekQphTKnWdt"
      },
      "outputs": [],
      "source": [
        "def tensor_to_string(tensor, idx_to_char):\n",
        "    \"\"\"Converte un tensore di indici in una stringa di caratteri.\"\"\"\n",
        "    return ''.join(idx_to_char[idx.item()] for idx in tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "U8NgTC39nYK9"
      },
      "outputs": [],
      "source": [
        "# Function to convert character values into indices\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# def char_to_tensor(characters):\n",
        "#     indices = [char_to_idx[char] for char in characters]\n",
        "#     return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Function to convert character values into indices\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def char_to_tensor(characters):\n",
        "    indices = [char_to_idx.get(char, char_to_idx['<pad>']) for char in characters] # Get the index for the character. If not found, use the index for padding.\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Prepare the training data_loader\n",
        "# Prepara i dati di test\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in data['users']:\n",
        "    for entry, target in zip(data['user_data'][user]['x'], data['user_data'][user]['y']):\n",
        "        input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
        "        target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
        "\n",
        "# Padding e creazione di DataLoader\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "targets = torch.cat(target_tensors)\n",
        "dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "HrI7p5-wbvVz"
      },
      "outputs": [],
      "source": [
        "# Prepare the test loader:\n",
        "# Prepare the training data_loader\n",
        "\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in test_data['users']:\n",
        "    for entry, target in zip(test_data['user_data'][user]['x'], test_data['user_data'][user]['y']):\n",
        "        input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
        "        target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
        "\n",
        "# Padding e creazione di DataLoader\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "targets = torch.cat(target_tensors)\n",
        "dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "test_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAz5GpfMl8Mv"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "bWSl31M6DjPg"
      },
      "outputs": [],
      "source": [
        "global_model = CharLSTM(vocab_size=len(char_to_idx))\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4qeN8CfmSpG"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "CSdo3O9eoBFv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DIR = '/content/drive/MyDrive/colab_plots/'\n",
        "\n",
        "def plot_client_selection(client_selection_count, file_name):\n",
        "    \"\"\"\n",
        "    Bar plot to visualize the frequency of client selections in a federated learning simulation.\n",
        "\n",
        "    Args:\n",
        "        client_selection_count (list): list containing the number of times each client was selected.\n",
        "        file_name (str): name of the file to save the plot.\n",
        "    \"\"\"\n",
        "    # Fixed base directory\n",
        "    directory = DIR +  'plots_federated/'\n",
        "    # Ensure the base directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    num_clients = len(client_selection_count)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(num_clients), client_selection_count, alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel(\"Client ID\", fontsize=14)\n",
        "    plt.ylabel(\"Selection Count\", fontsize=14)\n",
        "    plt.title(\"Client Selection Frequency\", fontsize=16)\n",
        "    plt.xticks(range(num_clients), fontsize=10, rotation=90 if num_clients > 20 else 0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path, format=\"png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "def test(global_model, test_loader, criterion, DEVICE):\n",
        "    \"\"\"\n",
        "    Evaluate the global model on the test dataset.\n",
        "\n",
        "    Args:\n",
        "        global_model (nn.Module): The global model to be evaluated.\n",
        "        test_loader (DataLoader): DataLoader for the test dataset.\n",
        "\n",
        "    Returns:\n",
        "        float: The accuracy of the model on the test dataset.\n",
        "        float: The loss of the model on the test dataset.\n",
        "    \"\"\"\n",
        "    test_accuracy, loss = evaluate(global_model, test_loader, criterion, DEVICE)\n",
        "    return test_accuracy, loss\n",
        "\n",
        "def plot_metrics(train_accuracies, train_losses, file_name):\n",
        "    \"\"\"\n",
        "    Plot the training metrics for a federated learning simulation.\n",
        "\n",
        "    Args:\n",
        "        train_accuracies (list): List of training accuracies.\n",
        "        train_losses (list): List of training losses.\n",
        "        file_name (str): Name of the file to save the plot.\n",
        "    \"\"\"\n",
        "    # Fixed base directory\n",
        "    directory = DIR + '/plots_federated/'\n",
        "    # Ensure the base directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Create a list of epochs for the x-axis\n",
        "    epochs = list(range(1, len(train_losses) + 1))\n",
        "\n",
        "    # Plot the training loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
        "    plt.xlabel('Rounds', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.title('Training Loss', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path.replace('.png', '_loss.png'), format='png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the training accuracy\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n",
        "    plt.xlabel('Rounds', fontsize=14)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.title('Training Accuracy', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path.replace('.png', '_accuracy.png'), format='png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_data(global_model, train_accuracies, train_losses,client_count, file_name):\n",
        "    \"\"\"\n",
        "    Save the global model, train_accuracies,train_losses and client_count to a file.\n",
        "\n",
        "    Args:\n",
        "        global_model (nn.Module): The global model to be saved.\n",
        "        train_accuracies (list): List of training accuracies.\n",
        "        train_losses (list): List of training losses.\n",
        "        file_name (str): Name of the file to save the data.\n",
        "    \"\"\"\n",
        "    # Fixed base directory\n",
        "    directory = DIR + '/trained_models/'\n",
        "    # Ensure the base directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Save all data (model state and metrics) into a dictionary\n",
        "    save_dict = {\n",
        "        'model_state': global_model.state_dict(),\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'train_losses': train_losses,\n",
        "        'client_count': client_count\n",
        "    }\n",
        "\n",
        "    # Save the dictionary to the specified file\n",
        "    torch.save(save_dict, file_path)\n",
        "    print(f\"Data saved successfully to {file_path}\")\n",
        "\n",
        "def load_data(model, file_name):\n",
        "    \"\"\"\n",
        "    Load the model weights and metrics from a file.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to load the weights into.\n",
        "        file_name (str): Name of the file to load the data from.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the model, train_accuracies train_losses and client_count.\n",
        "    \"\"\"\n",
        "    # Fixed base directory\n",
        "    directory = DIR+ 'trained_models/'\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Load the saved data from the specified file\n",
        "    save_dict = torch.load(file_path)\n",
        "\n",
        "    # Load the model state\n",
        "    model.load_state_dict(save_dict['model_state'])\n",
        "\n",
        "    # Extract the metrics\n",
        "    train_accuracies = save_dict['train_accuracies']\n",
        "    train_losses = save_dict['train_losses']\n",
        "    client_count = save_dict['client_count']\n",
        "\n",
        "    print(f\"Data loaded successfully from {file_path}\")\n",
        "\n",
        "    return model, train_accuracies, train_losses,client_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NZV6hBUnH3F"
      },
      "source": [
        "# Evolutionary algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "ZIvpAtgdnIK8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#constants\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#CRITERION = nn.NLLLoss()\n",
        "#MOMENTUM = 0.9\n",
        "#BATCHSIZE = 64\n",
        "CHECKPOINTING_PATH = '../checkpoints/'\n",
        "\n",
        "def tournament_selection_weakest(population, tau=2, p_diver=0.05):\n",
        "    \"\"\"\n",
        "    Perform tournament selection to choose parents.\n",
        "    Randomly select tau individuals and choose the weakest one.\n",
        "    Fitness hole to introduce a 5% probability of choosing the fittest individual.\n",
        "\n",
        "\n",
        "    :param population: List of Individuals.\n",
        "    :param tau: Number of individuals to select.\n",
        "    :param p_diver: Probability of choosing the worst individual in the tournament, done for the fitness hole.\n",
        "    :return: Selected Individual.\n",
        "    \"\"\"\n",
        "    participants = random.sample(population, tau)\n",
        "    if random.random() < p_diver:\n",
        "        winner = max(participants, key=lambda ind: ind.fitness)\n",
        "    else:\n",
        "      winner = min(participants, key=lambda ind: ind.fitness)\n",
        "    return deepcopy(winner)\n",
        "\n",
        "def tournament_selection_fittest(population, tau=2, p_diver=0.05):\n",
        "    \"\"\"\n",
        "    Perform tournament selection to choose parents.\n",
        "    Randomly select tau individuals and choose the best one.\n",
        "    Fitness hole to introduce a 5% probability of choosing the weakest individual.\n",
        "\n",
        "\n",
        "    :param population: List of Individuals.\n",
        "    :param tau: Number of individuals to select.\n",
        "    :param p_diver: Probability of choosing the worst individual in the tournament, done for the fitness hole.\n",
        "    :return: Selected Individual.\n",
        "    \"\"\"\n",
        "    participants = random.sample(population, tau)\n",
        "    if random.random() < p_diver:\n",
        "        winner = min(participants, key=lambda ind: ind.fitness)\n",
        "    else:\n",
        "      winner = max(participants, key=lambda ind: ind.fitness)\n",
        "    return deepcopy(winner)\n",
        "\n",
        "\n",
        "def client_size(individual, client_sizes):\n",
        "    \"\"\"\n",
        "    Computes the number of total samples for individual\n",
        "    \"\"\"\n",
        "    val = 0\n",
        "    for client in individual.genome:\n",
        "        val += client_sizes[client]\n",
        "    return val\n",
        "\n",
        "\n",
        "def EA_algorithm(generations, population_size, num_clients, num_classes, crossover_probability, dataset, lr, wd, criterion):\n",
        "    \"\"\"\n",
        "    Perform the Evolutionary Algorithm (EA) to optimize the selection of clients.\n",
        "    The EA consists of the following steps:\n",
        "    1. Initialization: Create a population of individuals.\n",
        "    2. Evaluation: Compute the fitness of each individual.\n",
        "    3. Selection: Choose parents based on their fitness.\n",
        "    4. Offspring to create the new population (generational model).\n",
        "    6. Repeat from step 2 maximum iterations.\n",
        "\n",
        "    :param generations: Number of generations to run the algorithm.\n",
        "    :param population_size: Number of individuals in the population.\n",
        "    :param num_clients: clients selected by each individual.\n",
        "    :param num_classes: Number of classes for each client (iid or non-iid).\n",
        "    :param crossover_probability: Probability of crossover for each individual.\n",
        "    :param dataset: The dataset to be used for training.\n",
        "    :param lr: The learning rate to be used for training.\n",
        "    :param wd: The weight decay to be used for training.\n",
        "\n",
        "\n",
        "    :return global_model: The global model obtained after the EA.\n",
        "    :return training_accuracies: The training loss of the global model at each generation.\n",
        "    :return training_losses: The training accuracy of the global model at each generation.\n",
        "    :return client_selection_count: The number of times each client was selected in the population.\n",
        "    \"\"\"\n",
        "\n",
        "    #Check if the checkpointing directory exists\n",
        "    os.makedirs(CHECKPOINTING_PATH, exist_ok=True)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    client_selection_count = [0]*100\n",
        "    best_model_state = None\n",
        "    best_train_loss = float('inf')\n",
        "\n",
        "\n",
        "    # Initialize the population\n",
        "    # Shuffle clients before assigning them\n",
        "    all_clients = list(range(100))\n",
        "    random.shuffle(all_clients)\n",
        "\n",
        "    #No individual, at the beginning, will select a client twice\n",
        "    population = [\n",
        "        Individual(genome=all_clients[i * num_clients:(i + 1) * num_clients])\n",
        "        for i in range(population_size)\n",
        "    ]\n",
        "    #population = [Individual(genome=random.sample(range(100), k=num_clients)) for _ in range(population_size)]\n",
        "    model = CharLSTM(vocab_size=len(char_to_idx))\n",
        "\n",
        "    #load checkpoint if it exists\n",
        "    checkpoint_start_step, data_to_load = load_checkpoint(model=model,optimizer=None,hyperparameters=f\"LR{lr}_WD{wd}\",subfolder=\"personal_contribution\")\n",
        "    if data_to_load is not None:\n",
        "        train_accuracies = data_to_load['train_accuracies']\n",
        "        train_losses = data_to_load['train_losses']\n",
        "        client_selection_count = data_to_load['client_selection_count']\n",
        "        population = [Individual.from_dict(ind) for ind in data_to_load['population']]\n",
        "    # Create the Server instance:\n",
        "    server = Server(model,DEVICE, char_to_idx,CHECKPOINTING_PATH )\n",
        "\n",
        "    shards = server.sharding(dataset)\n",
        "    client_sizes = [len(shard) for shard in shards]\n",
        "\n",
        "    for gen in range(checkpoint_start_step,generations):\n",
        "    #for gen in range(generations):\n",
        "        # For each of them apply the fed_avg algorithm:\n",
        "        param_list = []\n",
        "        averages_acc = []\n",
        "        average_loss = []\n",
        "        for individual in population:\n",
        "            #Update the client selection count\n",
        "            for client in individual.genome:\n",
        "                client_selection_count[client] += 1\n",
        "\n",
        "            resulting_model, acc_res, loss_res = server.train_federated(criterion, lr, MOMENTUM, BATCH_SIZE, wd, individual, shards)\n",
        "            param_list.append(resulting_model)\n",
        "            averages_acc.append(acc_res)\n",
        "            average_loss.append(loss_res)\n",
        "\n",
        "\n",
        "        #Here we should average all the models to obtain the global model...\n",
        "        averaged_model,  train_loss, train_accuracy = server.fedavg_aggregate(param_list, [client_size(i, client_sizes) for i in population], average_loss, averages_acc)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Update the model with the result of the average:\n",
        "        model.load_state_dict(averaged_model)\n",
        "        #Just to be sure:\n",
        "        server.global_model.load_state_dict(averaged_model)\n",
        "\n",
        "        # DO NOT HAVE A VALIDATION SPLIT, SO...\n",
        "        # # Then evaluate the validation accuracy of the global model\n",
        "        # acc, loss = evaluate(model, train_loader)\n",
        "        # if acc > best_val_acc:\n",
        "        #         best_val_acc = acc\n",
        "        #         best_model_state = deepcopy(model.state_dict())\n",
        "\n",
        "        # val_accuracies.append(acc)\n",
        "        # val_losses.append(loss)\n",
        "\n",
        "        if train_loss < best_train_loss:\n",
        "            best_train_loss = train_loss\n",
        "            best_model_state = deepcopy(model.state_dict())\n",
        "\n",
        "        offspring = []\n",
        "        #Offspring-> offspring size is the same as population size\n",
        "        elite = sorted(population, key=lambda ind: ind.fitness, reverse=True)[0]\n",
        "        offspring.append(elite) #Keep the best individual\n",
        "        for j in range(population_size-1):\n",
        "            # Crossover\n",
        "            if random.random() < crossover_probability:\n",
        "                parent1 = tournament_selection_fittest(population)\n",
        "                parent2 = tournament_selection_fittest(population)\n",
        "                offspring.append(Individual.crossover(parent1, parent2))\n",
        "            else:\n",
        "                #Mutation\n",
        "                parent = tournament_selection_weakest(population)\n",
        "                parent.point_mutation()\n",
        "                offspring.append(parent)\n",
        "\n",
        "        # Replace the population with the new offspring\n",
        "        population = offspring\n",
        "\n",
        "        #Checkpointing every 10 generations\n",
        "        if((gen+1)%10==0):\n",
        "            print(f\"Generation {gen+1}, loss {train_loss}\")\n",
        "            checkpoint_data = {\n",
        "                'train_accuracies': train_accuracies,\n",
        "                'train_losses': train_losses,\n",
        "                'client_selection_count': client_selection_count,\n",
        "                'population': [individual.to_dict() for individual in population]\n",
        "            }\n",
        "            save_checkpoint(model, None, gen+1, f\"LR{lr}_WD{wd}\", subfolder=\"personal_contribution\", checkpoint_data=checkpoint_data)\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, train_accuracies, train_losses, client_selection_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "XHfjoldYCeHV"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "lr = 1.0\n",
        "wd = 0.0001\n",
        "generations = 200\n",
        "population_size = 10\n",
        "num_clients = 2\n",
        "num_classes = 100\n",
        "crossover_probability = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgZaYRekNo5F",
        "outputId": "5aa3c85e-1cd5-414b-89d5-84b5a3df0a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "print(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0VkL5U5pjm7",
        "outputId": "ea7eee4e-d83e-48ec-aba5-5e1465a35795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint found, starting from epoch 1.\n",
            "Generation 10, loss 3.221942177414894\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_10_params_LR1.0_WD0.0001.pth\n",
            "Generation 20, loss 3.162678799033165\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_20_params_LR1.0_WD0.0001.pth\n",
            "Generation 30, loss 3.1417674511671065\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_30_params_LR1.0_WD0.0001.pth\n",
            "Generation 40, loss 3.123864418268204\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_40_params_LR1.0_WD0.0001.pth\n",
            "Generation 50, loss 3.078569161891937\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_50_params_LR1.0_WD0.0001.pth\n",
            "Generation 60, loss 3.0075629144906997\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_60_params_LR1.0_WD0.0001.pth\n",
            "Generation 70, loss 2.949299010634422\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_70_params_LR1.0_WD0.0001.pth\n",
            "Generation 80, loss 2.8599777638912203\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_80_params_LR1.0_WD0.0001.pth\n",
            "Generation 90, loss 2.801662975549698\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_90_params_LR1.0_WD0.0001.pth\n",
            "Generation 100, loss 2.698033419251442\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_100_params_LR1.0_WD0.0001.pth\n",
            "Generation 110, loss 2.7057989805936815\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_110_params_LR1.0_WD0.0001.pth\n",
            "Generation 120, loss 2.6589663833379746\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_120_params_LR1.0_WD0.0001.pth\n",
            "Generation 130, loss 2.6119063258171082\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_130_params_LR1.0_WD0.0001.pth\n",
            "Generation 140, loss 2.5600136488676073\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_140_params_LR1.0_WD0.0001.pth\n",
            "Generation 150, loss 2.55471970140934\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_150_params_LR1.0_WD0.0001.pth\n",
            "Generation 160, loss 2.514155423641205\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_160_params_LR1.0_WD0.0001.pth\n",
            "Generation 170, loss 2.4977332562208177\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_170_params_LR1.0_WD0.0001.pth\n",
            "Generation 180, loss 2.4717412173748015\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_180_params_LR1.0_WD0.0001.pth\n",
            "Generation 190, loss 2.4635994881391525\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_190_params_LR1.0_WD0.0001.pth\n",
            "Generation 200, loss 2.476309260725975\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/personal_contribution/model_epoch_200_params_LR1.0_WD0.0001.pth\n",
            "Test accuracy:  33.42741841423873\n"
          ]
        }
      ],
      "source": [
        "#Best lr and wd found for iid federated baseline: lr=0.1, wd=0.001\n",
        "global_model = CharLSTM(vocab_size=len(char_to_idx))\n",
        "global_model,train_accuracies,train_losses,client_selection_count=EA_algorithm(generations=generations,population_size=population_size,num_clients=num_clients,num_classes = num_classes, crossover_probability = crossover_probability, dataset= data, lr =lr , wd = wd, criterion = criterion)\n",
        "test_accuracy, test_loss = test(global_model, test_loader, criterion, DEVICE)\n",
        "print(\"Test accuracy: \",test_accuracy)\n",
        "plot_metrics(train_accuracies,train_losses,\"EA_iid.png\")\n",
        "#plot_client_selection(client_selection_count,\"EA_iid_client_selection.png\")\n",
        "#save_data(global_model,val_accuracies,val_losses,train_accuracies,train_losses,client_selection_count,\"EA_iid.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0bqkYbSJ-Yr",
        "outputId": "4a1fa5d1-24e2-4ca9-9105-855e809d81fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "num_clients = len(data['users'])\n",
        "print(num_clients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz2DgN57pEzi"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "LOCAL_STEPS_VALUES = [4, 8, 16]  # Values for J (number of local steps)\n",
        "NUM_RUNDS = {4: 200, 8: 100, 16:50}\n",
        "lr = 0.01\n",
        "wd = 0.0001\n",
        "'''\n",
        "These hyperparameters are taken from:\n",
        "Acar, Durmus Alp Emre, et al. \"Federated learning based on dynamic regularization.\" arXiv preprint arXiv:2111.04263 (2021).\n",
        "\n",
        "Notice infact that the leaf version of the Shakespeare dataset doesn't come with a linked validation dataset to\n",
        "choose the most accurate hyperparameters.\n",
        "'''\n",
        "\n",
        "# Function to perform the training and testing for a given configuration\n",
        "def run_experiment(local_steps, plot_suffix):\n",
        "    print(f\"Running experiment: local_steps={local_steps}\")\n",
        "    global_model = CharLSTM(vocab_size=len(char_to_idx))\n",
        "    server = Server(global_model, DEVICE, char_to_idx, CHECKPOINT_DIR)\n",
        "\n",
        "    #tuning_rounds = int(NUM_RUNDS[local_steps]/20)\n",
        "    #best_lr, best_wd = to be manually set\n",
        "\n",
        "    global_model, train_accuracies, train_losses, client_selection_count = server.train_federated(\n",
        "        criterion, data_loader,\n",
        "        num_clients=NUM_CLIENTS,\n",
        "        rounds=NUM_RUNDS[local_steps], lr=lr, momentum=MOMENTUM,\n",
        "        batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS,\n",
        "        local_steps=local_steps, log_freq=100,\n",
        "        detailed_print=True, gamma=None  # No skewed sampling for this experiment\n",
        "    )\n",
        "\n",
        "    # Testing and plotting\n",
        "    test_accuracy = test(global_model, test_loader)\n",
        "    plot_metrics(train_accuracies, train_losses, f\"Federated_scaled_{plot_suffix}_LR_{lr}_WD_{wd}.png\")\n",
        "    print(f\"Test accuracy for local_steps={local_steps}: {test_accuracy}\")\n",
        "\n",
        "    # Save data for future analysis\n",
        "    save_data(global_model, train_accuracies, train_losses, client_selection_count, f\"Federated_{plot_suffix}_LR_{lr}_WD_{wd}.pth\")\n",
        "\n",
        "\n",
        "local_steps = 16# and 16\n",
        "print(NUM_CLIENTS)\n",
        "plot_suffix = f\"local_steps_{local_steps}\"\n",
        "run_experiment(local_steps, plot_suffix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "UDO-0-TMnc03",
        "outputId": "6d73fb60-3a6d-47ff-af74-6f5d5b328e9e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'num_classes' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-625368a93a49>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mlocal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;31m# and 16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mplot_suffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"num_classes_{num_classes}_local_steps_{local_steps}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_suffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "LOCAL_STEPS_VALUES = [4, 8, 16]  # Values for J (number of local steps)\n",
        "NUM_RUNDS = {4: 200, 8: 100, 16:50}\n",
        "lr = 0.01\n",
        "wd = 0.0001\n",
        "'''\n",
        "These hyperparameters are taken from:\n",
        "Acar, Durmus Alp Emre, et al. \"Federated learning based on dynamic regularization.\" arXiv preprint arXiv:2111.04263 (2021).\n",
        "\n",
        "Notice infact that the leaf version of the Shakespeare dataset doesn't come with a linked validation dataset to\n",
        "choose the most accurate hyperparameters.\n",
        "'''\n",
        "\n",
        "# Function to perform the training and testing for a given configuration\n",
        "def run_experiment(local_steps, plot_suffix):\n",
        "    print(f\"Running experiment: local_steps={local_steps}\")\n",
        "    global_model = CharLSTM(vocab_size=len(char_to_idx))\n",
        "    server = Server(global_model, DEVICE, char_to_idx, CHECKPOINT_DIR)\n",
        "\n",
        "    tuning_rounds = int(NUM_RUNDS[local_steps]/20)\n",
        "    #best_lr, best_wd = to be manually set\n",
        "\n",
        "    global_model, train_accuracies, train_losses, client_selection_count = server.train_federated(\n",
        "        criterion, data_loader,\n",
        "        num_clients=NUM_CLIENTS,\n",
        "        rounds=NUM_RUNDS[local_steps], lr=lr, momentum=MOMENTUM,\n",
        "        batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS,\n",
        "        local_steps=local_steps, log_freq=100,\n",
        "        detailed_print=False, gamma=None  # No skewed sampling for this experiment\n",
        "    )\n",
        "\n",
        "    # Testing and plotting\n",
        "    test_accuracy = test(global_model, test_loader)\n",
        "    plot_metrics(train_accuracies, train_losses, f\"Federated_{plot_suffix}_LR_{lr}_WD_{wd}.png\")\n",
        "    print(f\"Test accuracy for local_steps={local_steps}: {test_accuracy}\")\n",
        "\n",
        "    # Save data for future analysis\n",
        "    save_data(global_model, train_accuracies, train_losses, client_selection_count, f\"Federated_{plot_suffix}_LR_{lr}_WD_{wd}.pth\")\n",
        "\n",
        "\n",
        "local_steps = 8# and 16\n",
        "plot_suffix = f\"num_classes_{num_classes}_local_steps_{local_steps}\"\n",
        "run_experiment(local_steps, plot_suffix)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
