{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWAhRJt5_i3e"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3okGAXgq_i3j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import string\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiHq71Y1_i3m"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of the dataset"
      ],
      "metadata": {
        "id": "HDfayYmcFxHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4-315RM_i3n",
        "outputId": "a5f3a5a9-da63-4c6d-c2e2-e0a019cda5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/kewagbln/shakespeareonline/versions/1\n"
          ]
        }
      ],
      "source": [
        "# Download latest version of the shakespeare dataset and save the path\n",
        "path = kagglehub.dataset_download(\"kewagbln/shakespeareonline\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "DATA_PATH = os.path.join(path, \"t8.shakespeare.txt\")\n",
        "OUTPUT_DIR = \"processed_data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paramters"
      ],
      "metadata": {
        "id": "AJoD-od7F01p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_FRACTION = 0.8  # percentage of the training data\n",
        "SEQ_LEN = 80  # length of the sequence for the model\n",
        "BATCH_SIZE = 4\n",
        "N_VOCAB = 90  # Numero di caratteri nel vocabolario (ASCII)\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.01\n",
        "EMBEDDING_SIZE = 8\n",
        "LSTM_HIDDEN_DIM = 256\n",
        "SEQ_LENGTH = 80"
      ],
      "metadata": {
        "id": "EwVPIZDTF4sM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Parsing and Preprocessing\n",
        "This section includes regular expressions and functions to parse Shakespeare's text into plays, characters, and their respective dialogues. The parsing handles special cases like \"The Comedy of Errors\" and prepares training and test datasets."
      ],
      "metadata": {
        "id": "RKLbEQbbIQvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHARACTER_RE = re.compile(r'^  ([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Matches character lines\n",
        "CONT_RE = re.compile(r'^    (.*)')  # Matches continuation lines\n",
        "COE_CHARACTER_RE = re.compile(r'^([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Special regex for Comedy of Errors\n",
        "COE_CONT_RE = re.compile(r'^(.*)')  # Continuation for Comedy of Errors\n",
        "\n",
        "def parse_shakespeare_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads and splits Shakespeare's text into plays, characters, and their dialogues.\n",
        "    Returns training and test datasets based on the specified fraction.\n",
        "    \"\"\"\n",
        "    with open(filepath, \"r\") as f:\n",
        "        content = f.read()\n",
        "    plays, _ = _split_into_plays(content)  # Split the text into plays\n",
        "    _, train_examples, test_examples = _get_train_test_by_character(\n",
        "        plays, test_fraction=1 - TRAIN_FRACTION\n",
        "    )\n",
        "    return train_examples, test_examples\n",
        "\n",
        "def _split_into_plays(shakespeare_full):\n",
        "    \"\"\"\n",
        "    Splits the full Shakespeare text into individual plays and characters' dialogues.\n",
        "    Handles special parsing for \"The Comedy of Errors\".\n",
        "    \"\"\"\n",
        "    plays = []\n",
        "    slines = shakespeare_full.splitlines(True)[1:]  # Skip the first line (title/header)\n",
        "    current_character = None\n",
        "    comedy_of_errors = False\n",
        "\n",
        "    for i, line in enumerate(slines):\n",
        "        # Detect play titles and initialize character dictionary\n",
        "        if \"by William Shakespeare\" in line:\n",
        "            current_character = None\n",
        "            characters = defaultdict(list)\n",
        "            title = slines[i - 2].strip() if slines[i - 2].strip() else slines[i - 3].strip()\n",
        "            comedy_of_errors = title == \"THE COMEDY OF ERRORS\"\n",
        "            plays.append((title, characters))\n",
        "            continue\n",
        "\n",
        "        # Match character lines or continuation lines\n",
        "        match = _match_character_regex(line, comedy_of_errors)\n",
        "        if match:\n",
        "            character, snippet = match.group(1).upper(), match.group(2)\n",
        "            if not (comedy_of_errors and character.startswith(\"ACT \")):\n",
        "                characters[character].append(snippet)\n",
        "                current_character = character\n",
        "        elif current_character:\n",
        "            match = _match_continuation_regex(line, comedy_of_errors)\n",
        "            if match:\n",
        "                characters[current_character].append(match.group(1))\n",
        "\n",
        "    # Filter out plays with insufficient dialogue data\n",
        "    return [play for play in plays if len(play[1]) > 1], []\n",
        "\n",
        "def _match_character_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches character dialogues, with special handling for 'The Comedy of Errors'.\"\"\"\n",
        "    return COE_CHARACTER_RE.match(line) if comedy_of_errors else CHARACTER_RE.match(line)\n",
        "\n",
        "def _match_continuation_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches continuation lines of dialogues.\"\"\"\n",
        "    return COE_CONT_RE.match(line) if comedy_of_errors else CONT_RE.match(line)\n",
        "\n",
        "def _get_train_test_by_character(plays, test_fraction=0.2):\n",
        "    \"\"\"\n",
        "    Splits dialogues by characters into training and testing datasets.\n",
        "    Ensures each character has at least one example in the training set.\n",
        "    \"\"\"\n",
        "    all_train_examples = defaultdict(list)\n",
        "    all_test_examples = defaultdict(list)\n",
        "\n",
        "    def add_examples(example_dict, example_tuple_list):\n",
        "        \"\"\"Adds examples to the respective dataset dictionary.\"\"\"\n",
        "        for play, character, sound_bite in example_tuple_list:\n",
        "            example_dict[f\"{play}_{character}\".replace(\" \", \"_\")].append(sound_bite)\n",
        "\n",
        "    for play, characters in plays:\n",
        "        for character, sound_bites in characters.items():\n",
        "            examples = [(play, character, sound_bite) for sound_bite in sound_bites]\n",
        "            if len(examples) <= 2:\n",
        "                continue\n",
        "\n",
        "            # Calculate the number of test samples\n",
        "            num_test = max(1, int(len(examples) * test_fraction))\n",
        "            num_test = min(num_test, len(examples) - 1)  # Ensure at least one training example\n",
        "\n",
        "            # Split into train and test sets\n",
        "            train_examples = examples[:-num_test]\n",
        "            test_examples = examples[-num_test:]\n",
        "\n",
        "            add_examples(all_train_examples, train_examples)\n",
        "            add_examples(all_test_examples, test_examples)\n",
        "\n",
        "    return {}, all_train_examples, all_test_examples\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N4lqN7-UINzE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing\n",
        "Functions to convert characters and words into numerical representations\n",
        "for use in neural networks. Includes padding logic for batch processing."
      ],
      "metadata": {
        "id": "qiYPyuLRIcI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def letter_to_vec(c, n_vocab=128):\n",
        "    \"\"\"Converts a single character to a vector index based on the vocabulary size.\"\"\"\n",
        "    return ord(c) % n_vocab\n",
        "\n",
        "def word_to_indices(word, n_vocab=128):\n",
        "    \"\"\"\n",
        "    Converts a word or list of words into a list of indices.\n",
        "    Each character is mapped to an index based on the vocabulary size.\n",
        "    \"\"\"\n",
        "    if isinstance(word, list):  # If input is a list of words\n",
        "        res = []\n",
        "        for stringa in word:\n",
        "            res.extend([ord(c) % n_vocab for c in stringa])  # Convert each word to indices\n",
        "        return res\n",
        "    else:  # If input is a single word\n",
        "        return [ord(c) % n_vocab for c in word]\n",
        "\n",
        "def process_x(raw_x_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw input data into padded sequences of indices.\n",
        "    Ensures all sequences are of uniform length.\n",
        "    \"\"\"\n",
        "    x_batch = [word_to_indices(word, n_vocab) for word in raw_x_batch]\n",
        "    x_batch = [x[:seq_len] + [0] * (seq_len - len(x)) for x in x_batch]\n",
        "    return torch.tensor(x_batch, dtype=torch.long)\n",
        "\n",
        "def process_y(raw_y_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw target data into padded sequences of indices.\n",
        "    Similar to process_x but for target labels.\n",
        "    \"\"\"\n",
        "    y_batch = [word_to_indices(word, n_vocab) for word in raw_y_batch]\n",
        "    y_batch = [y[:seq_len] + [0] * (seq_len - len(y)) for y in y_batch]\n",
        "    return torch.tensor(y_batch, dtype=torch.long)\n",
        "\n",
        "def create_batches(data, batch_size, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Creates batches of input and target data from dialogues.\n",
        "    Each batch contains sequences of uniform length.\n",
        "    \"\"\"\n",
        "    x_batches = []\n",
        "    y_batches = []\n",
        "    dialogues = list(data.values())\n",
        "    random.shuffle(dialogues)  # Shuffle to ensure randomness in batches\n",
        "\n",
        "    batch = []\n",
        "    for dialogue in dialogues:\n",
        "        batch.append(dialogue)\n",
        "        if len(batch) == batch_size:\n",
        "            x_batch = process_x(batch, seq_len, n_vocab)\n",
        "            y_batch = process_y(batch, seq_len, n_vocab)\n",
        "            x_batches.append(x_batch)\n",
        "            y_batches.append(y_batch)\n",
        "            batch = []\n",
        "\n",
        "    # Add the last batch if it's not full\n",
        "    if batch:\n",
        "        x_batch = process_x(batch, seq_len, n_vocab)\n",
        "        y_batch = process_y(batch, seq_len, n_vocab)\n",
        "        x_batches.append(x_batch)\n",
        "        y_batches.append(y_batch)\n",
        "\n",
        "    return x_batches, y_batches\n",
        "\n"
      ],
      "metadata": {
        "id": "f7cJk6_PIckr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader\n",
        "This code defines a custom PyTorch Dataset class for processing Shakespeare dialogues.\n",
        "The dataset converts raw text data into sequences of indices for training a character-level model.\n"
      ],
      "metadata": {
        "id": "pxsIfbBOLCTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for processing Shakespeare dialogues.\n",
        "    Converts input data into sequences of indices for input and target processing.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, seq_len, n_vocab):\n",
        "        \"\"\"\n",
        "        Initializes the ShakespeareDataset instance.\n",
        "\n",
        "        Args:\n",
        "            data: Dictionary containing dialogues (e.g., train_data or test_data).\n",
        "            seq_len: Length of sequences to generate for the model.\n",
        "            n_vocab: Size of the vocabulary for mapping characters to indices.\n",
        "        \"\"\"\n",
        "        self.data = list(data.values())  # Convert the dictionary values to a list\n",
        "        self.seq_len = seq_len  # Sequence length for the model\n",
        "        self.n_vocab = n_vocab  # Vocabulary size\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of dialogues in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a single sample (input and target) from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Processed input (x) and target (y) tensors for the model.\n",
        "        \"\"\"\n",
        "        dialogue = self.data[idx]  # Get the dialogue at the specified index\n",
        "        x = process_x([dialogue], self.seq_len, self.n_vocab)[0]  # Prepare input tensor\n",
        "        y = process_y([dialogue], self.seq_len, self.n_vocab)[0]  # Prepare target tensor\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "4nyJjLKULERy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "CharLSTM is a character-based LSTM model designed for text generation.\n",
        "It includes embedding, LSTM layers, and a fully connected layer for output."
      ],
      "metadata": {
        "id": "oBSf9y7_IlLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level LSTM model for text processing tasks.\n",
        "    Includes embedding, LSTM, and a fully connected output layer.\n",
        "    We use:\n",
        "    - embedding size equal to 8;\n",
        "    - 2 LSTM layers, each with 256 nodes;\n",
        "    - densely connected softmax output layer.\n",
        "\n",
        "    We can avoid to use explicitly the softmax function in the model and\n",
        "    keep a cross entropy loss function as a loss function.\n",
        "\n",
        "    as mentioned in paper [2] (Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "    Jakub Konečný, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size = 90, embedding_size = 8, lstm_hidden_dim = 256, seq_length=80):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
        "        self.lstm1 = nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size=lstm_hidden_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \"\"\"\n",
        "        # Layer 1: Embedding\n",
        "        x = self.embedding(x)  # Output shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Layer 2: First LSTM\n",
        "        x, _ = self.lstm1(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 3: Second LSTM\n",
        "        x, hidden = self.lstm2(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 4: Fully Connected Layer\n",
        "        x = self.fc(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Softmax Activation\n",
        "        #x = self.softmax(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "        return x, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden and cell states for the LSTM.\"\"\"\n",
        "        return (torch.zeros(2, batch_size, self.lstm_hidden_dim),\n",
        "            torch.zeros(2, batch_size, self.lstm_hidden_dim))\n",
        "        #2 is equal to the number of lstm layers!\n",
        "\n"
      ],
      "metadata": {
        "id": "CTQiMMyhIleP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n",
        "Trains the CharLSTM model using the prepared data and batches."
      ],
      "metadata": {
        "id": "Ocn8uGLYIsLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility function to evaluate the accuracy and loss of a on a validation/test set"
      ],
      "metadata": {
        "id": "tz_XAVsVxITc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "def evaluate(model, dataloader, criterion, DEVICE):\n",
        "    with torch.no_grad():\n",
        "        model.train(False) # Set Network to evaluation mode\n",
        "        running_corrects = 0\n",
        "        total_predictions = 0  # Track total predictions for normalization\n",
        "        losses = []\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(DEVICE)        # Move the data to the GPU\n",
        "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
        "            # Forward Pass\n",
        "            state = model.init_hidden(data.size(0))\n",
        "            # outputs is a tuple: (logits, hidden_state)\n",
        "            outputs, _ = model(data, state) # unpack the tuple and get only the output (predictions)\n",
        "            # Reshape the outputs for CrossEntropyLoss\n",
        "            outputs = outputs.view(-1, model.vocab_size)\n",
        "            targets = targets.view(-1)\n",
        "            loss = criterion(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            # Update Corrects (element-wise comparison for accuracy)\n",
        "            running_corrects += (preds == targets).sum().item()\n",
        "            total_predictions += targets.size(0)  # Update total prediction count\n",
        "\n",
        "        # Calculate Accuracy (divide by total predictions)\n",
        "        accuracy = (running_corrects / total_predictions) * 100\n",
        "\n",
        "    return accuracy, mean(losses)"
      ],
      "metadata": {
        "id": "IkiuJYN-xRSd"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Parsing dataset...\")\n",
        "    LOG_FREQUENCY = 10\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    train_data, test_data = parse_shakespeare_file(DATA_PATH)\n",
        "\n",
        "    train_dataset = ShakespeareDataset(train_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "    test_dataset = ShakespeareDataset(test_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "\n",
        "    # Split the train dataset into train and validation:\n",
        "    train_size = int(0.9 * len(train_dataset))  # 90%\n",
        "    valid_size = len(train_dataset) - train_size  # 10%\n",
        "    #random split:\n",
        "    train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "    # Creation of the DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Initialization of the model\n",
        "    model = CharLSTM(vocab_size = N_VOCAB, embedding_size = EMBEDDING_SIZE, lstm_hidden_dim = LSTM_HIDDEN_DIM, seq_length = SEQ_LENGTH)\n",
        "    model = model.to(DEVICE) # Move the entire model to the desired device\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # We need to initialize best_val_acc to a very low value outside the epoch loop\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Training cycle\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch = x_batch.to(DEVICE)  # Move the data to the GPU\n",
        "            y_batch = y_batch.to(DEVICE)  # Move the targets to the GPU\n",
        "            state = model.init_hidden(x_batch.size(0))  # Initialize hidden state for the batch\n",
        "\n",
        "            state = (state[0].to(DEVICE), state[1].to(DEVICE))  # Move hidden state to GPU\n",
        "            logits, _ = model(x_batch, state)  # Forward pass\n",
        "\n",
        "            # Riformattare logits e y_batch per CrossEntropyLoss\n",
        "            logits = logits.view(-1, N_VOCAB)  # Reshape to (batch_size * seq_length, vocab_size)\n",
        "            y_batch = y_batch.view(-1)  # Reshape to (batch_size * seq_length)\n",
        "\n",
        "            # Compute the predictions\n",
        "            predictions = torch.argmax(logits, dim=1)  # Predictions for each class with argmax\n",
        "            correct += (predictions == y_batch).sum().item()  # Sum of correct predictions\n",
        "            total += y_batch.size(0)  #Total number of target\n",
        "\n",
        "            loss = criterion(logits, y_batch)  # Calculate loss\n",
        "\n",
        "            #Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluate on the validation set, done every epoch\n",
        "        val_acc, val_loss = evaluate(model, val_loader, criterion, DEVICE)\n",
        "        val_accuracies.append(val_acc)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        #print(f\"Epoch {epoch + 1}/{EPOCHS}, Training Loss: {total_loss / len(train_loader):.4f}\")\n",
        "        # Compute the training accuracy\n",
        "        accuracy = correct / total\n",
        "        print(f\"Training Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "        # Update the best model if validation accuracy improves\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = deepcopy(model.state_dict())  # Save the current model state\n",
        "            print(f\"New best model found with Validation accuracy: {val_acc:.2f}%\")\n",
        "        # Evaluate on the training set\n",
        "\n",
        "        if(epoch%LOG_FREQUENCY==0):\n",
        "            print(f\"--> best validation accuracy: {best_val_acc:.2f} epoch: {epoch+1}\")\n",
        "            print(f\"--> validation loss: {val_loss:.4f}-- training loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "        # if loss is zero exit\n",
        "        if total_loss <= 0.00009:\n",
        "          break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TRXzXzqIsfQ",
        "outputId": "dcaccfb5-957a-47ff-f45c-6940bcae4f20"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing dataset...\n",
            "Training Accuracy: 0.92%\n",
            "New best model found with Validation accuracy: 99.95%\n",
            "--> best validation accuracy: 99.95 epoch: 1\n",
            "--> validation loss: 0.0025-- training loss: 0.3283\n",
            "Training Accuracy: 1.00%\n",
            "New best model found with Validation accuracy: 99.98%\n",
            "Training Accuracy: 1.00%\n",
            "New best model found with Validation accuracy: 100.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "--> best validation accuracy: 100.00 epoch: 11\n",
            "--> validation loss: 0.0000-- training loss: 0.0000\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "--> best validation accuracy: 100.00 epoch: 21\n",
            "--> validation loss: 0.0000-- training loss: 0.0000\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "--> best validation accuracy: 100.00 epoch: 31\n",
            "--> validation loss: 0.0000-- training loss: 0.0000\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n",
            "Training Accuracy: 1.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "1gFfp8-H7RzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, testloader):\n",
        "    \"\"\"\n",
        "    Test the model on the test set.\n",
        "    \"\"\"\n",
        "    accuracy, loss = evaluate(model, testloader, criterion, DEVICE)\n",
        "    return accuracy, loss"
      ],
      "metadata": {
        "id": "pyBngtp77TEB"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = test(model, test_loader)\n",
        "print(f\"Test Loss:  {val[1]:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rpHXZLQ7X6b",
        "outputId": "26bae392-7b8b-4c76-b32e-bd500e940923"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss:  0.01%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}