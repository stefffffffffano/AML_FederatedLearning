{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWAhRJt5_i3e"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GiEtSRXF386",
        "outputId": "b36b61ae-a293-4a4a-d852-1b84a335f954"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3okGAXgq_i3j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import string\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import kagglehub\n",
        "import io\n",
        "import json\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the Dataset"
      ],
      "metadata": {
        "id": "bTmrobfeI5Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must import the dataset manually since it is taken by the LEAF project.\n",
        "\n",
        "So far the project is to go on the data folder of shakespeare and:\n",
        "1. ./get_data.sh\n",
        "2. ./data_to_json.sh\n",
        "3. ././preprocess.sh -s niid --sf 0.2 -k 0 -t sample -tf 0.8 [depending on the preferencies]"
      ],
      "metadata": {
        "id": "8TD59XwRI7PL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the training dataset"
      ],
      "metadata": {
        "id": "WM07ejLQKP3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "CYwrsHfkPPvj",
        "outputId": "5bcad977-ac36-48cf-e806-a51b960c0057"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9d2bad7e-16cc-41cb-85ff-0804dd335135\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9d2bad7e-16cc-41cb-85ff-0804dd335135\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all_data_niid_06_train_8.json to all_data_niid_06_train_8.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the testing dataset"
      ],
      "metadata": {
        "id": "eY4DCrN8KTPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded2 = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "F96qBvUyaGXd",
        "outputId": "02c1c962-e62d-4967-add1-5a0459b8adbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a3f2e034-cfa9-4f7f-8ba5-921babbdb1c0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a3f2e034-cfa9-4f7f-8ba5-921babbdb1c0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all_data_niid_06_test_8.json to all_data_niid_06_test_8.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing print to verify the name of the uploaded files for testing (in general no need to\n",
        "# use this print since we already have the name of the uploaded file).\n",
        "print(list(uploaded2.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt0olksGdBTs",
        "outputId": "2e5db333-931b-4c77-ae9a-b381040d74ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['all_data_niid_06_test_8.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = json.load(io.BytesIO(uploaded['all_data_niid_06_train_8.json']))"
      ],
      "metadata": {
        "id": "Io3MCNwVRkjA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data  = json.load(io.BytesIO(uploaded2['all_data_niid_06_test_8.json']))"
      ],
      "metadata": {
        "id": "wxUypiypaVh5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the json file (the leaf dataset for Shakespeare)"
      ],
      "metadata": {
        "id": "4gPvEjGKKyaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('all_data_niid_06_train_8.json', 'r') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "6AoehnCARw0t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('all_data_niid_06_test_8.json', 'r') as f:  # Cambia il percorso con quello corretto\n",
        "    test_data = json.load(f)"
      ],
      "metadata": {
        "id": "5HrcTDxkbGFz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistics of the dataset\n",
        "Just for testing porpouses we can print some statistics about the uploaded dataset.\n",
        "\n",
        "The values used for the train/test split and the number of his samples are inspired by:\n",
        "Acar, Durmus Alp Emre, et al. \"Federated learning based on dynamic regularization.\" arXiv preprint arXiv:2111.04263 (2021)."
      ],
      "metadata": {
        "id": "XCGPeiE9K5Ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_samples = sum(data['num_samples'])\n",
        "print(f\"Total number of train samples: {total_samples}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVm6X0uN3PO",
        "outputId": "d992836b-a89a-42d2-daf1-931c9240aea9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of train samples: 199128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_samples = sum(test_data['num_samples'])\n",
        "print(f\"Total number of test samples: {total_samples}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlmGoKJWOCN8",
        "outputId": "c7c29f97-d395-4d04-84e6-01132e9380d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of test samples: 49810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users = data['users']\n",
        "num_samples = data['num_samples']\n",
        "user_data = data['user_data']"
      ],
      "metadata": {
        "id": "ir8cweSzR-FO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "fkuz0ZEULp-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The batch size has been inspired by:\n",
        "Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "Jakub Konečný, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021.\n",
        "'''\n",
        "BATCH_SIZE = 4"
      ],
      "metadata": {
        "id": "BOrgnNFvc3MR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocab creation"
      ],
      "metadata": {
        "id": "Iy1FP3iYSGoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_texts = ''.join([''.join(seq) for user in users for seq in user_data[user]['x']])\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "\n",
        "# Add the padding character\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}"
      ],
      "metadata": {
        "id": "scMFoC0WSI3r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Covert data into indices"
      ],
      "metadata": {
        "id": "fi9H7dJNSwoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [[char_to_idx[char] for char in user_data[user]['x'][0]] for user in users]\n",
        "targets = [[char_to_idx[char] for char in user_data[user]['y'][0]] for user in users]"
      ],
      "metadata": {
        "id": "UZaJF07uSy2W"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of TensorDataset and DataLoader"
      ],
      "metadata": {
        "id": "5P_okFPTTAls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "input_tensors = [torch.tensor(seq) for seq in inputs]\n",
        "target_tensors = [torch.tensor([seq]) for seq in targets]\n",
        "\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
        "\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "\n",
        "target_tensors = torch.cat(target_tensors, dim=0)\n",
        "\n",
        "dataset = TensorDataset(padded_inputs, target_tensors)\n",
        "batch_size = 4\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "ysDl6DJNUC_d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_string(tensor, idx_to_char):\n",
        "    \"\"\"Converte un tensore di indici in una stringa di caratteri.\"\"\"\n",
        "    return ''.join(idx_to_char[idx.item()] for idx in tensor)"
      ],
      "metadata": {
        "id": "N94bWSOHZIsd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "XOSOJnQXVYiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level LSTM model for text processing tasks.\n",
        "    Includes embedding, LSTM, and a fully connected output layer.\n",
        "    We use:\n",
        "    - embedding size equal to 8;\n",
        "    - 2 LSTM layers, each with 256 nodes;\n",
        "    - densely connected softmax output layer.\n",
        "\n",
        "    We can avoid to use explicitly the softmax function in the model and\n",
        "    keep a cross entropy loss function as a loss function.\n",
        "\n",
        "    as mentioned in paper [2] (Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "    Jakub Konečný, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size = 70, embedding_size = 8, lstm_hidden_dim = 256, seq_length=80):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
        "        self.lstm1 = nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size=lstm_hidden_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \"\"\"\n",
        "        # Layer 1: Embedding\n",
        "        x = self.embedding(x)  # Output shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Layer 2: First LSTM\n",
        "        x, _ = self.lstm1(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 3: Second LSTM\n",
        "        x, hidden = self.lstm2(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 4: Fully Connected Layer\n",
        "        x = self.fc(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Softmax Activation\n",
        "        #x = self.softmax(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "        return x[:, -1, :], hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden and cell states for the LSTM.\"\"\"\n",
        "        return (torch.zeros(2, batch_size, self.lstm_hidden_dim),\n",
        "            torch.zeros(2, batch_size, self.lstm_hidden_dim))\n",
        "        #2 is equal to the number of lstm layers!\n",
        "\n"
      ],
      "metadata": {
        "id": "wAPXXX77VYHr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "9iY7b3pVVc62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert character in indices:\n",
        "def char_to_tensor(characters):\n",
        "    indices = [char_to_idx[char] for char in characters]\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Prepare the test samples:\n",
        "'''\n",
        "The leaf dataset is structured in the following way:\n",
        "Users: Each dataset in LEAF is distributed across a simulated set of users (playing actor). The data for\n",
        "each user is stored separately to mimic real-world scenarios where data is distributed\n",
        "across devices or clients.\n",
        "Data Format: For each user, the data include:\n",
        "    x: sentences declared by the \"user\"/\"device\".\n",
        "    y: Labels or outputs associated with the inputs.\n",
        "'''\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in data['users']:\n",
        "    for entry, target in zip(data['user_data'][user]['x'], data['user_data'][user]['y']):\n",
        "        input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
        "        target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
        "\n",
        "# Padding and creation ofDataLoader\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "targets = torch.cat(target_tensors)\n",
        "dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "WFq79CKNy8EW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpointing management"
      ],
      "metadata": {
        "id": "XhwIgYH9r5Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/colab_checkpoints/'\n",
        "\n",
        "\n",
        "# Ensure the checkpoint directory exists, creating it if necessary\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, train_accuracies, train_losses, subfolder=\"\", checkpoint_data=None):\n",
        "    \"\"\"\n",
        "    Saves the model checkpoint and removes the previous one if it exists.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state is to be saved.\n",
        "    optimizer -- The optimizer whose state is to be saved (can be None).\n",
        "    epoch -- The current epoch of the training process.\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    train_accuracies -- List of training accuracies to save.\n",
        "    train_losses -- List of training losses to save.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to save the checkpoint.\n",
        "    checkpoint_data -- Additional data to save in a JSON file (e.g., training logs).\n",
        "    \"\"\"\n",
        "    # Define the path for the subfolder where checkpoints will be stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # Construct filenames for both the model checkpoint and the associated JSON file\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "    # Define the filenames for the previous checkpoint files\n",
        "    previous_filepath = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.pth\")\n",
        "    previous_filepath_json = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.json\")\n",
        "\n",
        "    # Remove the previous checkpoint if it exists\n",
        "    if epoch >= 1:\n",
        "        if os.path.exists(previous_filepath):\n",
        "            os.remove(previous_filepath)\n",
        "        if os.path.exists(previous_filepath_json):\n",
        "            os.remove(previous_filepath_json)\n",
        "\n",
        "    # Prepare the checkpoint data dictionary\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'train_losses': train_losses\n",
        "    }\n",
        "    if optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    # Save the model and optimizer state dictionary to the checkpoint file\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    # Save additional data to a JSON file\n",
        "    if checkpoint_data:\n",
        "        with open(filepath_json, 'w') as json_file:\n",
        "            json.dump(checkpoint_data, json_file, indent=4)\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Loads the latest checkpoint available based on the specified hyperparameters.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state will be updated from the checkpoint.\n",
        "    optimizer -- The optimizer whose state will be updated from the checkpoint (can be None).\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to look for checkpoints.\n",
        "\n",
        "    Returns:\n",
        "    The next epoch to resume from, train_accuracies, train_losses, and the associated JSON data if available.\n",
        "    \"\"\"\n",
        "    # Define the path to the subfolder where checkpoints are stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint directory found, starting from epoch 1.\")\n",
        "        return 1, [], [], None\n",
        "\n",
        "    # Search for checkpoint files in the subfolder that match the hyperparameters\n",
        "    files = [f for f in os.listdir(subfolder_path) if f.endswith('.pth')]\n",
        "\n",
        "    if files:\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))  # Find the latest epoch file\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        # Load the model state from the checkpoint\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        # If an optimizer is provided, load its state as well\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Load training accuracies and losses\n",
        "        train_accuracies = checkpoint.get('train_accuracies', [])\n",
        "        train_losses = checkpoint.get('train_losses', [])\n",
        "\n",
        "        # Try to load the associated JSON file if available\n",
        "        json_filepath = os.path.join(subfolder_path, latest_file.replace('.pth', '.json'))\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "\n",
        "        print(f\"Checkpoint found: Resuming from epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1, train_accuracies, train_losses, json_data\n",
        "\n",
        "    print(\"No checkpoint found, starting from epoch 1.\")\n",
        "    return 1, [], [], None\n",
        "\n",
        "\n",
        "def delete_existing_checkpoints(subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Deletes all existing checkpoints in the specified subfolder.\n",
        "\n",
        "    Arguments:\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to delete checkpoints from.\n",
        "    \"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if os.path.exists(subfolder_path):\n",
        "        for file_name in os.listdir(subfolder_path):\n",
        "            file_path = os.path.join(subfolder_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        print(f\"All existing checkpoints in {subfolder_path} have been deleted.\")\n",
        "    else:\n",
        "        print(f\"No checkpoint folder found at {subfolder_path}.\")"
      ],
      "metadata": {
        "id": "v_pKNLFEr65X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a91792-45c3-4421-cbad-0cff0c8038b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwCJcIPLL5qm",
        "outputId": "f21f39b3-ba7c-4f53-8d6e-e4b4767fff9b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/colab_checkpoints/Federated/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsWEZS4kL92C",
        "outputId": "40838d8b-aaf7-4849-d287-8dd2f2ad021f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_epoch_1099_params_LR0.01_WD0.0001.json  model_epoch_199_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1099_params_LR0.01_WD0.0001.pth   model_epoch_199_params_LR0.01_WD0.0001.pth\n",
            "model_epoch_1199_params_LR0.01_WD0.0001.json  model_epoch_299_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1199_params_LR0.01_WD0.0001.pth   model_epoch_299_params_LR0.01_WD0.0001.pth\n",
            "model_epoch_1299_params_LR0.01_WD0.0001.json  model_epoch_399_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1299_params_LR0.01_WD0.0001.pth   model_epoch_399_params_LR0.01_WD0.0001.pth\n",
            "model_epoch_1399_params_LR0.01_WD0.0001.json  model_epoch_499_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1399_params_LR0.01_WD0.0001.pth   model_epoch_499_params_LR0.01_WD0.0001.pth\n",
            "model_epoch_1499_params_LR0.01_WD0.0001.json  model_epoch_599_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1499_params_LR0.01_WD0.0001.pth   model_epoch_599_params_LR0.01_WD0.0001.pth\n",
            "model_epoch_1599_params_LR0.01_WD0.0001.json  model_epoch_699_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1599_params_LR0.01_WD0.0001.pth   model_epoch_699_params_LR0.01_WD0.0001.pth\n",
            "model_epoch_1699_params_LR0.01_WD0.0001.json  model_epoch_799_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1699_params_LR0.01_WD0.0001.pth   model_epoch_799_params_LR0.01_WD0.0001.pth\n",
            "model_epoch_1799_params_LR0.01_WD0.0001.json  model_epoch_899_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1799_params_LR0.01_WD0.0001.pth   model_epoch_899_params_LR0.01_WD0.0001.pth\n",
            "model_epoch_1899_params_LR0.01_WD0.0001.json  model_epoch_999_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1899_params_LR0.01_WD0.0001.pth   model_epoch_999_params_LR0.01_WD0.0001.pth\n",
            "model_epoch_1999_params_LR0.01_WD0.0001.json  model_epoch_99_params_LR0.01_WD0.0001.json\n",
            "model_epoch_1999_params_LR0.01_WD0.0001.pth   model_epoch_99_params_LR0.01_WD0.0001.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot function"
      ],
      "metadata": {
        "id": "CzEZbh5BvJXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(train_accuracies, train_losses, file_name):\n",
        "    \"\"\"\n",
        "    Plot the training metrics.\n",
        "\n",
        "    Args:\n",
        "        train_accuracies (list): List of training accuracies.\n",
        "        train_losses (list): List of training losses.\n",
        "        file_name (str): Name of the file to save the plot.\n",
        "    \"\"\"\n",
        "    DIR = '/content/drive/MyDrive/colab_plots/'\n",
        "    # Fixed base directory\n",
        "    directory = DIR + '/plots_federated/'\n",
        "    # Ensure the base directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Create a list of epochs for the x-axis\n",
        "    epochs = list(range(1, len(train_losses) + 1))\n",
        "\n",
        "    # Plot the training and validation losses\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
        "    plt.xlabel('Rounds', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.title('Training and Validation Loss', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path.replace('.png', '_loss.png'), format='png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the training and validation accuracies\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n",
        "    plt.xlabel('Rounds', fontsize=14)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.title('Training and Validation Accuracy', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path.replace('.png', '_accuracy.png'), format='png', dpi=300)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "S9fMwjCJvLPU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Model to use\n",
        "model = CharLSTM(vocab_size=len(char_to_idx))\n",
        "model.train()  # set the model in training mode\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(DEVICE)  # Move the entire model to the right device\n",
        "\n",
        "# Definition (manual) of loss and optimizer (with related hyperparameters)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "wd = 0.0001\n",
        "momentum = 0.9\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "\n",
        "            # Reset the existing gradients (if any)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # ininzialize the hidden state:\n",
        "            hidden = model.init_hidden(inputs.size(0))\n",
        "            hidden = (hidden[0].to(DEVICE), hidden[1].to(DEVICE))\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = model(inputs, hidden)\n",
        "\n",
        "            # loss\n",
        "            outputs_flat = outputs.view(-1, len(char_to_idx))\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = criterion(outputs_flat, targets_flat)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_samples += targets_flat.size(0)\n",
        "\n",
        "            # Accuracy\n",
        "            _, predicted = outputs_flat.max(1)\n",
        "            total_correct += (predicted == targets_flat).sum().item()\n",
        "\n",
        "        # loss and accuracy for epoch\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = total_correct / total_samples\n",
        "\n",
        "        # Append the result to list:\n",
        "        train_losses.append(avg_loss)\n",
        "        train_accuracies.append(accuracy)\n",
        "\n",
        "        # Save the checkpoint:\n",
        "        save_checkpoint(model,optimizer=None, epoch=epoch, hyperparameters=f\"LR{lr}\", train_accuracies = train_accuracies, train_losses = train_losses, subfolder=\"Federated/\")\n",
        "\n",
        "        # print statistics\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, %')\n",
        "\n",
        "# Execute the model:\n",
        "train_model(model, loader, criterion, optimizer, num_epochs=200)\n"
      ],
      "metadata": {
        "id": "suVFX7jpVbiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe0d4c4-2b65-469e-e3ef-f5e2f52d77be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_0_params_LR0.001.pth\n",
            "Epoch 1/200, Loss: 2.9428, Accuracy: 0.2260, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_1_params_LR0.001.pth\n",
            "Epoch 2/200, Loss: 2.3854, Accuracy: 0.3339, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_2_params_LR0.001.pth\n",
            "Epoch 3/200, Loss: 2.1268, Accuracy: 0.3949, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_3_params_LR0.001.pth\n",
            "Epoch 4/200, Loss: 1.9817, Accuracy: 0.4311, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_4_params_LR0.001.pth\n",
            "Epoch 5/200, Loss: 1.8845, Accuracy: 0.4549, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_5_params_LR0.001.pth\n",
            "Epoch 6/200, Loss: 1.8120, Accuracy: 0.4734, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_6_params_LR0.001.pth\n",
            "Epoch 7/200, Loss: 1.7547, Accuracy: 0.4871, %\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_7_params_LR0.001.pth\n",
            "Epoch 8/200, Loss: 1.7066, Accuracy: 0.4987, %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "UTiz78nbaCRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def char_to_tensor(characters):\n",
        "    indices = [char_to_idx.get(char, char_to_idx['<pad>']) for char in characters] # Get the index for the character. If not found, use the index for padding.\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in test_data['users']:\n",
        "    for entry, target in zip(test_data['user_data'][user]['x'], test_data['user_data'][user]['y']):\n",
        "        input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
        "        target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
        "\n",
        "\n",
        "# Padding e creazione di DataLoader\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx[' '])  # Usa spazio per padding se non hai '<pad>'\n",
        "targets = torch.cat(target_tensors)\n",
        "test_dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in test_dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, targets in test_loader:\n",
        "    inputs = inputs.to(DEVICE)  # move input to correct dev\n",
        "    targets = targets.to(DEVICE)  # move target to correct dev\n",
        "\n",
        "    # Inizialize the hidden state\n",
        "    hidden = model.init_hidden(inputs.size(0))\n",
        "    hidden = (hidden[0].to(DEVICE), hidden[1].to(DEVICE))  # Move the hidden state to correct dev\n",
        "\n",
        "    outputs, _ = model(inputs, hidden)\n",
        "    outputs_flat = outputs.view(-1, len(char_to_idx))\n",
        "    targets_flat = targets.view(-1)\n",
        "    _, predicted = outputs_flat.max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += (predicted == targets_flat).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total}%')"
      ],
      "metadata": {
        "id": "zJOZBBEobRE1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}