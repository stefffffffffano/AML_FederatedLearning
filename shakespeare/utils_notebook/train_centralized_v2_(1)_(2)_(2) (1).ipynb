{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWAhRJt5_i3e"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3okGAXgq_i3j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import string\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import kagglehub\n",
        "import io\n",
        "import json\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTmrobfeI5Q7"
      },
      "source": [
        "# Import the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TD59XwRI7PL"
      },
      "source": [
        "We must import the dataset manually since it is taken by the LEAF project.\n",
        "\n",
        "So far the project is to go on the data folder of shakespeare and:\n",
        "1. ./get_data.sh\n",
        "2. ./data_to_json.sh\n",
        "3. ././preprocess.sh -s niid --sf 0.2 -k 0 -t sample -tf 0.8 [depending on the preferencies]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM07ejLQKP3u"
      },
      "source": [
        "## Upload the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "CYwrsHfkPPvj",
        "outputId": "6d128c65-c8ac-44fb-9de2-210d2eb96494"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bfcba672-8922-429f-a759-7da0620afa2e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bfcba672-8922-429f-a759-7da0620afa2e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving all_data_niid_06_train_8.json to all_data_niid_06_train_8.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY4DCrN8KTPW"
      },
      "source": [
        "## Upload the testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "F96qBvUyaGXd",
        "outputId": "315af4ee-ba9a-4015-a528-eafc360f43da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fee1e693-f1f2-41f2-b1be-71185b5703ef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fee1e693-f1f2-41f2-b1be-71185b5703ef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving all_data_niid_06_test_8.json to all_data_niid_06_test_8.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded2 = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt0olksGdBTs",
        "outputId": "f56ab6e6-8502-4a0a-afc3-93d397bbef14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['all_data_niid_06_test_8.json']\n"
          ]
        }
      ],
      "source": [
        "# Testing print to verify the name of the uploaded files for testing (in general no need to\n",
        "# use this print since we already have the name of the uploaded file).\n",
        "print(list(uploaded2.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Io3MCNwVRkjA"
      },
      "outputs": [],
      "source": [
        "data = json.load(io.BytesIO(uploaded['all_data_niid_06_train_8.json']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wxUypiypaVh5"
      },
      "outputs": [],
      "source": [
        "test_data  = json.load(io.BytesIO(uploaded2['all_data_niid_06_test_8.json']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gPvEjGKKyaA"
      },
      "source": [
        "## Upload the json file (the leaf dataset for Shakespeare)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6AoehnCARw0t"
      },
      "outputs": [],
      "source": [
        "with open('all_data_niid_06_train_8.json', 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5HrcTDxkbGFz"
      },
      "outputs": [],
      "source": [
        "with open('all_data_niid_06_test_8.json', 'r') as f:  # Cambia il percorso con quello corretto\n",
        "    test_data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCGPeiE9K5Ve"
      },
      "source": [
        "## Statistics of the dataset\n",
        "Just for testing porpouses we can print some statistics about the uploaded dataset.\n",
        "\n",
        "The values used for the train/test split and the number of his samples are inspired by:\n",
        "Acar, Durmus Alp Emre, et al. \"Federated learning based on dynamic regularization.\" arXiv preprint arXiv:2111.04263 (2021)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVm6X0uN3PO",
        "outputId": "440e21ab-82ee-4dab-f718-f0f2b1c8f0e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of train samples: 198981\n"
          ]
        }
      ],
      "source": [
        "total_samples = sum(data['num_samples'])\n",
        "print(f\"Total number of train samples: {total_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlmGoKJWOCN8",
        "outputId": "0089f057-efe6-4450-9121-5555bb9ef7a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of test samples: 49773\n"
          ]
        }
      ],
      "source": [
        "total_samples = sum(test_data['num_samples'])\n",
        "print(f\"Total number of test samples: {total_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ir8cweSzR-FO"
      },
      "outputs": [],
      "source": [
        "users = data['users']\n",
        "num_samples = data['num_samples']\n",
        "user_data = data['user_data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkuz0ZEULp-O"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BOrgnNFvc3MR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The batch size has been inspired by:\n",
        "Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "Jakub Konečný, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021.\n",
        "'''\n",
        "BATCH_SIZE = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy1FP3iYSGoD"
      },
      "source": [
        "## Vocab creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "scMFoC0WSI3r"
      },
      "outputs": [],
      "source": [
        "all_texts = ''.join([''.join(seq) for user in users for seq in user_data[user]['x']])\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "\n",
        "# Add the padding character\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi9H7dJNSwoc"
      },
      "source": [
        "## Covert data into indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UZaJF07uSy2W"
      },
      "outputs": [],
      "source": [
        "inputs = [[char_to_idx[char] for char in user_data[user]['x'][0]] for user in users]\n",
        "targets = [[char_to_idx[char] for char in user_data[user]['y'][0]] for user in users]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P_okFPTTAls"
      },
      "source": [
        "## Creation of TensorDataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ysDl6DJNUC_d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "input_tensors = [torch.tensor(seq) for seq in inputs]\n",
        "target_tensors = [torch.tensor([seq]) for seq in targets]\n",
        "\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
        "\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "\n",
        "target_tensors = torch.cat(target_tensors, dim=0)\n",
        "\n",
        "dataset = TensorDataset(padded_inputs, target_tensors)\n",
        "batch_size = 4\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "N94bWSOHZIsd"
      },
      "outputs": [],
      "source": [
        "def tensor_to_string(tensor, idx_to_char):\n",
        "    \"\"\"Converte un tensore di indici in una stringa di caratteri.\"\"\"\n",
        "    return ''.join(idx_to_char[idx.item()] for idx in tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOSOJnQXVYiZ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wAPXXX77VYHr"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level LSTM model for text processing tasks.\n",
        "    Includes embedding, LSTM, and a fully connected output layer.\n",
        "    We use:\n",
        "    - embedding size equal to 8;\n",
        "    - 2 LSTM layers, each with 256 nodes;\n",
        "    - densely connected softmax output layer.\n",
        "\n",
        "    We can avoid to use explicitly the softmax function in the model and\n",
        "    keep a cross entropy loss function as a loss function.\n",
        "\n",
        "    as mentioned in paper [2] (Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "    Jakub Konečný, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size = 70, embedding_size = 8, lstm_hidden_dim = 256, seq_length=80):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
        "        self.lstm1 = nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size=lstm_hidden_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \"\"\"\n",
        "        # Layer 1: Embedding\n",
        "        x = self.embedding(x)  # Output shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Layer 2: First LSTM\n",
        "        x, _ = self.lstm1(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 3: Second LSTM\n",
        "        x, hidden = self.lstm2(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 4: Fully Connected Layer\n",
        "        x = self.fc(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Softmax Activation\n",
        "        #x = self.softmax(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "        return x[:, -1, :], hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden and cell states for the LSTM.\"\"\"\n",
        "        return (torch.zeros(2, batch_size, self.lstm_hidden_dim),\n",
        "            torch.zeros(2, batch_size, self.lstm_hidden_dim))\n",
        "        #2 is equal to the number of lstm layers!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iY7b3pVVc62"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WFq79CKNy8EW"
      },
      "outputs": [],
      "source": [
        "# Function to convert character in indices:\n",
        "def char_to_tensor(characters):\n",
        "    indices = [char_to_idx[char] for char in characters]\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Prepare the test samples:\n",
        "'''\n",
        "The leaf dataset is structured in the following way:\n",
        "Users: Each dataset in LEAF is distributed across a simulated set of users (playing actor). The data for\n",
        "each user is stored separately to mimic real-world scenarios where data is distributed\n",
        "across devices or clients.\n",
        "Data Format: For each user, the data include:\n",
        "    x: sentences declared by the \"user\"/\"device\".\n",
        "    y: Labels or outputs associated with the inputs.\n",
        "'''\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in data['users']:\n",
        "    for entry, target in zip(data['user_data'][user]['x'], data['user_data'][user]['y']):\n",
        "        input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
        "        target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
        "\n",
        "# Padding and creation ofDataLoader\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "targets = torch.cat(target_tensors)\n",
        "dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhwIgYH9r5Cn"
      },
      "source": [
        "# Checkpointing management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_pKNLFEr65X",
        "outputId": "591c5888-6073-4ec7-b664-b4a82538045c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/colab_checkpoints/'\n",
        "\n",
        "\n",
        "# Ensure the checkpoint directory exists, creating it if necessary\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", checkpoint_data=None):\n",
        "    \"\"\"\n",
        "    Saves the model checkpoint and removes the previous one if it exists.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state is to be saved.\n",
        "    optimizer -- The optimizer whose state is to be saved (can be None).\n",
        "    epoch -- The current epoch of the training process.\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to save the checkpoint.\n",
        "    checkpoint_data -- Data to save in a JSON file (e.g., training logs).\n",
        "    \"\"\"\n",
        "    # Define the path for the subfolder where checkpoints will be stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    # Create the subfolder if it doesn't exist\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # Construct filenames for both the model checkpoint and the associated JSON file\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "    # Define the filenames for the previous checkpoint files, to remove them if necessary\n",
        "    previous_filepath = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.pth\")\n",
        "    previous_filepath_json = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.json\")\n",
        "\n",
        "    # Remove the previous checkpoint if it exists, but only for epochs greater than 1\n",
        "    # if epoch > 1 and os.path.exists(previous_filepath):\n",
        "    #     os.remove(previous_filepath)\n",
        "    #     os.remove(previous_filepath_json)\n",
        "\n",
        "      # Remove the previous checkpoint if it exists, but only for epochs greater than 1\n",
        "    if epoch >= 1:\n",
        "        if os.path.exists(previous_filepath):\n",
        "            os.remove(previous_filepath)\n",
        "        if os.path.exists(previous_filepath_json):\n",
        "            os.remove(previous_filepath_json)\n",
        "\n",
        "\n",
        "    # Prepare the checkpoint data dictionary\n",
        "    checkpoint = {'model_state_dict': model.state_dict(), 'epoch': epoch}\n",
        "    # If an optimizer is provided, save its state as well\n",
        "    if optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    # Save the model and optimizer (if provided) state dictionary to the checkpoint file\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    # If additional data (e.g., training logs) is provided, save it to a JSON file\n",
        "    if checkpoint_data:\n",
        "        with open(filepath_json, 'w') as json_file:\n",
        "            json.dump(checkpoint_data, json_file, indent=4)\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Loads the latest checkpoint available based on the specified hyperparameters.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state will be updated from the checkpoint.\n",
        "    optimizer -- The optimizer whose state will be updated from the checkpoint (can be None).\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to look for checkpoints.\n",
        "\n",
        "    Returns:\n",
        "    The next epoch to resume from and the associated JSON data if available.\n",
        "    \"\"\"\n",
        "    # Define the path to the subfolder where checkpoints are stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "\n",
        "    # If the subfolder doesn't exist, print a message and start from epoch 1\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, starting from epoch 1.\")\n",
        "        return 1, None  # Epoch starts from 1\n",
        "\n",
        "    # Search for checkpoint files in the subfolder that match the hyperparameters\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "\n",
        "    # If checkpoint files are found, load the one with the highest epoch number\n",
        "    if files:\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))  # Find the latest epoch file\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath, weights_only=True)\n",
        "\n",
        "        # Load the model state from the checkpoint\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        # If an optimizer is provided, load its state as well\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Try to load the associated JSON file if available\n",
        "        json_filepath = os.path.join(subfolder_path, latest_file.replace('.pth', '.json'))\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            # If the JSON file exists, load its contents\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(\"Data loaded!\")\n",
        "        else:\n",
        "            # If no JSON file exists, print a message\n",
        "            print(\"No data found\")\n",
        "\n",
        "        # Print the epoch from which the model is resuming\n",
        "        print(f\"Checkpoint found: Resuming from epoch {checkpoint['epoch'] + 1}\\n\\n\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    # If no checkpoint is found, print a message and start from epoch 1\n",
        "    print(\"No checkpoint found, starting from epoch 1..\\n\\n\")\n",
        "    return 1, None  # Epoch starts from 1\n",
        "\n",
        "def delete_existing_checkpoints(subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Deletes all existing checkpoints in the specified subfolder.\n",
        "\n",
        "    Arguments:\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to delete checkpoints from.\n",
        "    \"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if os.path.exists(subfolder_path):\n",
        "        for file_name in os.listdir(subfolder_path):\n",
        "            file_path = os.path.join(subfolder_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        print(f\"All existing checkpoints in {subfolder_path} have been deleted.\")\n",
        "    else:\n",
        "        print(f\"No checkpoint folder found at {subfolder_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suVFX7jpVbiX",
        "outputId": "d8c8aeba-0e4e-4dc6-ff0a-633e1f5a4082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_0_params_LR0.001.pth\n",
            "Epoch 1/200, Loss: 1.9230, Accuracy: 0.4476\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_1_params_LR0.001.pth\n",
            "Epoch 2/200, Loss: 1.6513, Accuracy: 0.5136\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_2_params_LR0.001.pth\n",
            "Epoch 3/200, Loss: 1.5715, Accuracy: 0.5316\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_3_params_LR0.001.pth\n",
            "Epoch 4/200, Loss: 1.5226, Accuracy: 0.5437\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_4_params_LR0.001.pth\n",
            "Epoch 5/200, Loss: 1.4987, Accuracy: 0.5488\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_5_params_LR0.001.pth\n",
            "Epoch 6/200, Loss: 1.4810, Accuracy: 0.5529\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_6_params_LR0.001.pth\n",
            "Epoch 7/200, Loss: 1.4702, Accuracy: 0.5557\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_7_params_LR0.001.pth\n",
            "Epoch 8/200, Loss: 1.4606, Accuracy: 0.5574\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_8_params_LR0.001.pth\n",
            "Epoch 9/200, Loss: 1.4525, Accuracy: 0.5594\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_9_params_LR0.001.pth\n",
            "Epoch 10/200, Loss: 1.4487, Accuracy: 0.5615\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_10_params_LR0.001.pth\n",
            "Epoch 11/200, Loss: 1.4435, Accuracy: 0.5619\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_11_params_LR0.001.pth\n",
            "Epoch 12/200, Loss: 1.4435, Accuracy: 0.5630\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_12_params_LR0.001.pth\n",
            "Epoch 13/200, Loss: 1.4379, Accuracy: 0.5645\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_13_params_LR0.001.pth\n",
            "Epoch 14/200, Loss: 1.4372, Accuracy: 0.5640\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_14_params_LR0.001.pth\n",
            "Epoch 15/200, Loss: 1.4311, Accuracy: 0.5655\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_15_params_LR0.001.pth\n",
            "Epoch 16/200, Loss: 1.4308, Accuracy: 0.5656\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_16_params_LR0.001.pth\n",
            "Epoch 17/200, Loss: 1.4292, Accuracy: 0.5664\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_17_params_LR0.001.pth\n",
            "Epoch 18/200, Loss: 1.4328, Accuracy: 0.5654\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_18_params_LR0.001.pth\n",
            "Epoch 19/200, Loss: 1.4257, Accuracy: 0.5670\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_19_params_LR0.001.pth\n",
            "Epoch 20/200, Loss: 1.4259, Accuracy: 0.5668\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_20_params_LR0.001.pth\n",
            "Epoch 21/200, Loss: 1.4240, Accuracy: 0.5678\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_21_params_LR0.001.pth\n",
            "Epoch 22/200, Loss: 1.4234, Accuracy: 0.5680\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_22_params_LR0.001.pth\n",
            "Epoch 23/200, Loss: 1.4185, Accuracy: 0.5695\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_23_params_LR0.001.pth\n",
            "Epoch 24/200, Loss: 1.4153, Accuracy: 0.5696\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_24_params_LR0.001.pth\n",
            "Epoch 25/200, Loss: 1.4179, Accuracy: 0.5694\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_25_params_LR0.001.pth\n",
            "Epoch 26/200, Loss: 1.4108, Accuracy: 0.5713\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_26_params_LR0.001.pth\n",
            "Epoch 27/200, Loss: 1.4104, Accuracy: 0.5717\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_27_params_LR0.001.pth\n",
            "Epoch 28/200, Loss: 1.4138, Accuracy: 0.5701\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_28_params_LR0.001.pth\n",
            "Epoch 29/200, Loss: 1.4073, Accuracy: 0.5724\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_29_params_LR0.001.pth\n",
            "Epoch 30/200, Loss: 1.4137, Accuracy: 0.5713\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_30_params_LR0.001.pth\n",
            "Epoch 31/200, Loss: 1.4105, Accuracy: 0.5713\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_31_params_LR0.001.pth\n",
            "Epoch 32/200, Loss: 1.4093, Accuracy: 0.5728\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_32_params_LR0.001.pth\n",
            "Epoch 33/200, Loss: 1.4111, Accuracy: 0.5728\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_33_params_LR0.001.pth\n",
            "Epoch 34/200, Loss: 1.4038, Accuracy: 0.5735\n",
            "Checkpoint saved: /content/drive/MyDrive/colab_checkpoints/Federated/model_epoch_34_params_LR0.001.pth\n",
            "Epoch 35/200, Loss: 1.4117, Accuracy: 0.5721\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Model to use\n",
        "model = CharLSTM(vocab_size=len(char_to_idx))\n",
        "model.train()  # set the model in training mode\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(DEVICE)  # Move the entire model to the right device\n",
        "\n",
        "# Definition (manual) of loss and optimizer (with related hyperparameters)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "\n",
        "            # Reset the existing gradients (if any)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # ininzialize the hidden state:\n",
        "            hidden = model.init_hidden(inputs.size(0))\n",
        "            hidden = (hidden[0].to(DEVICE), hidden[1].to(DEVICE))\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = model(inputs, hidden)\n",
        "\n",
        "            # loss\n",
        "            outputs_flat = outputs.view(-1, len(char_to_idx))\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = criterion(outputs_flat, targets_flat)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_samples += targets_flat.size(0)\n",
        "\n",
        "            # Accuracy\n",
        "            _, predicted = outputs_flat.max(1)\n",
        "            total_correct += (predicted == targets_flat).sum().item()\n",
        "\n",
        "        # loss and accuracy for epoch\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = total_correct / total_samples\n",
        "\n",
        "        # Save the checkpoint:\n",
        "        save_checkpoint(model,optimizer=None, epoch=epoch, hyperparameters=f\"LR{lr}\", subfolder=\"Federated/\")\n",
        "\n",
        "        # print statistics\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, %')\n",
        "\n",
        "# Execute the model:\n",
        "train_model(model, loader, criterion, optimizer, num_epochs=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTiz78nbaCRx"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJOZBBEobRE1"
      },
      "outputs": [],
      "source": [
        "def char_to_tensor(characters):\n",
        "    indices = [char_to_idx[char] for char in characters]\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in test_data['users']:\n",
        "    for entry in test_data['user_data'][user]['x']:\n",
        "        input_tensors.append(char_to_tensor(entry[:-1]))  # Tutti i caratteri tranne l'ultimo\n",
        "        target_tensors.append(char_to_tensor(entry[-1]))  # L'ultimo carattere come target\n",
        "\n",
        "# Padding e creazione di DataLoader\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx[' '])  # Usa spazio per padding se non hai '<pad>'\n",
        "targets = torch.cat(target_tensors)\n",
        "test_dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in test_dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# Testa il modello\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, targets in test_loader:\n",
        "    inputs = inputs.to(DEVICE)  # Sposta gli input sul dispositivo corretto\n",
        "    targets = targets.to(DEVICE)  # Sposta i target sul dispositivo corretto\n",
        "\n",
        "    # Inizializza lo stato nascosto\n",
        "    hidden = model.init_hidden(inputs.size(0))\n",
        "    hidden = (hidden[0].to(DEVICE), hidden[1].to(DEVICE))  # Sposta lo stato nascosto sul dispositivo corretto\n",
        "\n",
        "    outputs, _ = model(inputs, hidden)\n",
        "    outputs_flat = outputs.view(-1, len(char_to_idx))\n",
        "    targets_flat = targets.view(-1)\n",
        "    _, predicted = outputs_flat.max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += (predicted == targets_flat).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
