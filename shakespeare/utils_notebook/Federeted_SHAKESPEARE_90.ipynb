{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Test1 federeted\n",
        "Implementa l'algoritmo FedAvg.\n",
        "\n",
        "Fissa K=100, C=0.1, e adotta una partizione iid del dataset di addestramento.\n",
        "\n",
        "Esegui FedAvg su Shakespeare per 200 round senza alcun learning rate schedule."
      ],
      "metadata": {
        "id": "xXucCjUiVi5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "L0OGlKQH5VNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from torch.utils.data import Subset\n",
        "from statistics import mean\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from torchvision import datasets, transforms\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "u-427H805W1j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "OQ4lpvEM5brc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants for FL training\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(DEVICE)\n",
        "\n",
        "NUM_CLIENTS = 100  # Total number of clients in the federation\n",
        "FRACTION_CLIENTS = 0.1  # Fraction of clients selected per round (C)\n",
        "LOCAL_STEPS = 4  # Number of local steps (J)\n",
        "GLOBAL_ROUNDS = 2000  # Total number of communication rounds\n",
        "\n",
        "BATCH_SIZE = 4  # Batch size for local training\n",
        "LR = 1e-2  # Initial learning rate for local optimizers\n",
        "MOMENTUM = 0.9  # Momentum for SGD optimizer\n",
        "WEIGHT_DECAY = 1e-4  # Regularization term for local training\n",
        "\n",
        "LOG_FREQUENCY = 10  # Frequency of logging training progress\n",
        "\n",
        "TRAIN_FRACTION = 0.9  # percentage of the training data\n",
        "SEQ_LEN = 80  # length of the sequence for the model\n",
        "BATCH_SIZE = 4\n",
        "N_VOCAB = 90  # Numero di caratteri nel vocabolario (ASCII)\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.01\n",
        "EMBEDDING_SIZE = 8\n",
        "LSTM_HIDDEN_DIM = 256\n",
        "SEQ_LENGTH = 80"
      ],
      "metadata": {
        "id": "kIVwlr6D5dN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d41e9e4-6572-4eb9-aa92-54ad1e3ea002"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility code"
      ],
      "metadata": {
        "id": "GqPiR3Od_izT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from statistics import mean\n",
        "import torch.nn as nn\n",
        "\n",
        "\"\"\"\n",
        "Utility function used both in the centralized and federated learning\n",
        "Computes the accuracy and the loss on the validation/test set depending on the dataloader passed\n",
        "\"\"\"\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "criterion = nn.NLLLoss()# our loss function for classification tasks on CIFAR-100\n",
        "def evaluate(model, dataloader):\n",
        "    with torch.no_grad():\n",
        "        model.train(False) # Set Network to evaluation mode\n",
        "        total_predictions = 0  # Track total predictions for normalization\n",
        "        running_corrects = 0\n",
        "        losses = []\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(DEVICE)        # Move the data to the GPU\n",
        "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
        "            # Forward Pass\n",
        "            #state = model.init_hidden(data.size(0))\n",
        "            # outputs is a tuple: (logits, hidden_state)\n",
        "            outputs, _ = model(data) # unpack the tuple and get only the output (predictions)\n",
        "            # Reshape the outputs for CrossEntropyLoss\n",
        "            outputs = outputs.view(-1, model.vocab_size)\n",
        "            targets = targets.view(-1)\n",
        "            loss = criterion(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            # Update Corrects (element-wise comparison for accuracy)\n",
        "            running_corrects += (preds == targets).sum().item()\n",
        "            total_predictions += targets.size(0)  # Update total prediction count\n",
        "\n",
        "        # Calculate Accuracy (divide by total predictions)\n",
        "        accuracy = (running_corrects / total_predictions) * 100\n",
        "\n",
        "    return accuracy, mean(losses)"
      ],
      "metadata": {
        "id": "Ed5M_oah_kSr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "CHECKPOINT_DIR = \"cartellaCheckpoint\"\n",
        "# Ensure the checkpoint directory exists, creating it if necessary\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", checkpoint_data=None):\n",
        "    \"\"\"\n",
        "    Saves the model checkpoint and removes the previous one if it exists.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state is to be saved.\n",
        "    optimizer -- The optimizer whose state is to be saved (can be None).\n",
        "    epoch -- The current epoch of the training process.\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to save the checkpoint.\n",
        "    checkpoint_data -- Data to save in a JSON file (e.g., training logs).\n",
        "    \"\"\"\n",
        "    # Define the path for the subfolder where checkpoints will be stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    # Create the subfolder if it doesn't exist\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # Construct filenames for both the model checkpoint and the associated JSON file\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "    # Define the filenames for the previous checkpoint files, to remove them if necessary\n",
        "    previous_filepath = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.pth\")\n",
        "    previous_filepath_json = os.path.join(subfolder_path, f\"model_epoch_{epoch - 1}_params_{hyperparameters}.json\")\n",
        "\n",
        "    # Remove the previous checkpoint if it exists, but only for epochs greater than 1\n",
        "    if epoch > 1 and os.path.exists(previous_filepath):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Prepare the checkpoint data dictionary\n",
        "    checkpoint = {'model_state_dict': model.state_dict(), 'epoch': epoch}\n",
        "    # If an optimizer is provided, save its state as well\n",
        "    if optimizer is not None:\n",
        "        checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "    # Save the model and optimizer (if provided) state dictionary to the checkpoint file\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "    # If additional data (e.g., training logs) is provided, save it to a JSON file\n",
        "    if checkpoint_data:\n",
        "        with open(filepath_json, 'w') as json_file:\n",
        "            json.dump(checkpoint_data, json_file, indent=4)\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Loads the latest checkpoint available based on the specified hyperparameters.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The model whose state will be updated from the checkpoint.\n",
        "    optimizer -- The optimizer whose state will be updated from the checkpoint (can be None).\n",
        "    hyperparameters -- A string representing the model's hyperparameters for file naming.\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to look for checkpoints.\n",
        "\n",
        "    Returns:\n",
        "    The next epoch to resume from and the associated JSON data if available.\n",
        "    \"\"\"\n",
        "    # Define the path to the subfolder where checkpoints are stored\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "\n",
        "    # If the subfolder doesn't exist, print a message and start from epoch 1\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, starting from epoch 1.\")\n",
        "        return 1, None  # Epoch starts from 1\n",
        "\n",
        "    # Search for checkpoint files in the subfolder that match the hyperparameters\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "\n",
        "    # If checkpoint files are found, load the one with the highest epoch number\n",
        "    if files:\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))  # Find the latest epoch file\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath, weights_only=True)\n",
        "\n",
        "        # Load the model state from the checkpoint\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        # If an optimizer is provided, load its state as well\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Try to load the associated JSON file if available\n",
        "        json_filepath = os.path.join(subfolder_path, latest_file.replace('.pth', '.json'))\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            # If the JSON file exists, load its contents\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(\"Data loaded!\")\n",
        "        else:\n",
        "            # If no JSON file exists, print a message\n",
        "            print(\"No data found\")\n",
        "\n",
        "        # Print the epoch from which the model is resuming\n",
        "        print(f\"Checkpoint found: Resuming from epoch {checkpoint['epoch'] + 1}\\n\\n\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    # If no checkpoint is found, print a message and start from epoch 1\n",
        "    print(\"No checkpoint found, starting from epoch 1..\\n\\n\")\n",
        "    return 1, None  # Epoch starts from 1\n",
        "\n",
        "def delete_existing_checkpoints(subfolder=\"\"):\n",
        "    \"\"\"\n",
        "    Deletes all existing checkpoints in the specified subfolder.\n",
        "\n",
        "    Arguments:\n",
        "    subfolder -- Optional subfolder within the checkpoint directory to delete checkpoints from.\n",
        "    \"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if os.path.exists(subfolder_path):\n",
        "        for file_name in os.listdir(subfolder_path):\n",
        "            file_path = os.path.join(subfolder_path, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "        print(f\"All existing checkpoints in {subfolder_path} have been deleted.\")\n",
        "    else:\n",
        "        print(f\"No checkpoint folder found at {subfolder_path}.\")"
      ],
      "metadata": {
        "id": "2339UQre_o_C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for data loading"
      ],
      "metadata": {
        "id": "c_5mcyl3O2C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHARACTER_RE = re.compile(r'^  ([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Matches character lines\n",
        "CONT_RE = re.compile(r'^    (.*)')  # Matches continuation lines\n",
        "COE_CHARACTER_RE = re.compile(r'^([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Special regex for Comedy of Errors\n",
        "COE_CONT_RE = re.compile(r'^(.*)')  # Continuation for Comedy of Errors\n",
        "\n",
        "def parse_shakespeare_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads and splits Shakespeare's text into plays, characters, and their dialogues.\n",
        "    Returns training and test datasets based on the specified fraction.\n",
        "    \"\"\"\n",
        "    with open(filepath, \"r\") as f:\n",
        "        content = f.read()\n",
        "    plays, _ = _split_into_plays(content)  # Split the text into plays\n",
        "    _, train_examples, test_examples = _get_train_test_by_character(\n",
        "        plays, test_fraction=1 - TRAIN_FRACTION\n",
        "    )\n",
        "    return train_examples, test_examples\n",
        "\n",
        "def _split_into_plays(shakespeare_full):\n",
        "    \"\"\"\n",
        "    Splits the full Shakespeare text into individual plays and characters' dialogues.\n",
        "    Handles special parsing for \"The Comedy of Errors\".\n",
        "    \"\"\"\n",
        "    plays = []\n",
        "    slines = shakespeare_full.splitlines(True)[1:]  # Skip the first line (title/header)\n",
        "    current_character = None\n",
        "    comedy_of_errors = False\n",
        "\n",
        "    for i, line in enumerate(slines):\n",
        "        # Detect play titles and initialize character dictionary\n",
        "        if \"by William Shakespeare\" in line:\n",
        "            current_character = None\n",
        "            characters = defaultdict(list)\n",
        "            title = slines[i - 2].strip() if slines[i - 2].strip() else slines[i - 3].strip()\n",
        "            comedy_of_errors = title == \"THE COMEDY OF ERRORS\"\n",
        "            plays.append((title, characters))\n",
        "            continue\n",
        "\n",
        "        # Match character lines or continuation lines\n",
        "        match = _match_character_regex(line, comedy_of_errors)\n",
        "        if match:\n",
        "            character, snippet = match.group(1).upper(), match.group(2)\n",
        "            if not (comedy_of_errors and character.startswith(\"ACT \")):\n",
        "                characters[character].append(snippet)\n",
        "                current_character = character\n",
        "        elif current_character:\n",
        "            match = _match_continuation_regex(line, comedy_of_errors)\n",
        "            if match:\n",
        "                characters[current_character].append(match.group(1))\n",
        "\n",
        "    # Filter out plays with insufficient dialogue data\n",
        "    return [play for play in plays if len(play[1]) > 1], []\n",
        "\n",
        "def _match_character_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches character dialogues, with special handling for 'The Comedy of Errors'.\"\"\"\n",
        "    return COE_CHARACTER_RE.match(line) if comedy_of_errors else CHARACTER_RE.match(line)\n",
        "\n",
        "def _match_continuation_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches continuation lines of dialogues.\"\"\"\n",
        "    return COE_CONT_RE.match(line) if comedy_of_errors else CONT_RE.match(line)\n",
        "\n",
        "def _get_train_test_by_character(plays, test_fraction=0.2):\n",
        "    \"\"\"\n",
        "    Splits dialogues by characters into training and testing datasets.\n",
        "    Ensures each character has at least one example in the training set.\n",
        "    \"\"\"\n",
        "    all_train_examples = defaultdict(list)\n",
        "    all_test_examples = defaultdict(list)\n",
        "\n",
        "    def add_examples(example_dict, example_tuple_list):\n",
        "        \"\"\"Adds examples to the respective dataset dictionary.\"\"\"\n",
        "        for play, character, sound_bite in example_tuple_list:\n",
        "            example_dict[f\"{play}_{character}\".replace(\" \", \"_\")].append(sound_bite)\n",
        "\n",
        "    for play, characters in plays:\n",
        "        for character, sound_bites in characters.items():\n",
        "            examples = [(play, character, sound_bite) for sound_bite in sound_bites]\n",
        "            if len(examples) <= 2:\n",
        "                continue\n",
        "\n",
        "            # Calculate the number of test samples\n",
        "            num_test = max(1, int(len(examples) * test_fraction))\n",
        "            num_test = min(num_test, len(examples) - 1)  # Ensure at least one training example\n",
        "\n",
        "            # Split into train and test sets\n",
        "            train_examples = examples[:-num_test]\n",
        "            test_examples = examples[-num_test:]\n",
        "\n",
        "            add_examples(all_train_examples, train_examples)\n",
        "            add_examples(all_test_examples, test_examples)\n",
        "\n",
        "    return {}, all_train_examples, all_test_examples\n",
        "\n",
        "\n",
        "def letter_to_vec(c, n_vocab=128):\n",
        "    \"\"\"Converts a single character to a vector index based on the vocabulary size.\"\"\"\n",
        "    return ord(c) % n_vocab\n",
        "\n",
        "def word_to_indices(word, n_vocab=128):\n",
        "    \"\"\"\n",
        "    Converts a word or list of words into a list of indices.\n",
        "    Each character is mapped to an index based on the vocabulary size.\n",
        "    \"\"\"\n",
        "    if isinstance(word, list):  # If input is a list of words\n",
        "        res = []\n",
        "        for stringa in word:\n",
        "            res.extend([ord(c) % n_vocab for c in stringa])  # Convert each word to indices\n",
        "        return res\n",
        "    else:  # If input is a single word\n",
        "        return [ord(c) % n_vocab for c in word]\n",
        "\n",
        "def process_x(raw_x_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw input data into padded sequences of indices.\n",
        "    Ensures all sequences are of uniform length.\n",
        "    \"\"\"\n",
        "    x_batch = [word_to_indices(word, n_vocab) for word in raw_x_batch]\n",
        "    x_batch = [x[:seq_len] + [0] * (seq_len - len(x)) for x in x_batch]\n",
        "    return torch.tensor(x_batch, dtype=torch.long)\n",
        "\n",
        "\n",
        "def process_y(raw_y_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw target data into padded sequences of indices.\n",
        "    Shifts the sequence by one character to the right.\n",
        "    y[1:seq_len + 1] takes the input data, right shift of an\n",
        "    element and uses the next element of the sequence to fill\n",
        "    and at the end (with [0]) final padding (zeros) are (eventually)\n",
        "    added to reach the desired sequence length.\n",
        "    \"\"\"\n",
        "    y_batch = [word_to_indices(word, n_vocab) for word in raw_y_batch]\n",
        "    y_batch = [y[1:seq_len + 1] + [0] * (seq_len - len(y[1:seq_len + 1])) for y in y_batch]  # Shifting and final padding\n",
        "    return torch.tensor(y_batch, dtype=torch.long)\n",
        "\n",
        "def create_batches(data, batch_size, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Creates batches of input and target data from dialogues.\n",
        "    Each batch contains sequences of uniform length.\n",
        "    \"\"\"\n",
        "    x_batches = []\n",
        "    y_batches = []\n",
        "    dialogues = list(data.values())\n",
        "    random.shuffle(dialogues)  # Shuffle to ensure randomness in batches\n",
        "\n",
        "    batch = []\n",
        "    for dialogue in dialogues:\n",
        "        batch.append(dialogue)\n",
        "        if len(batch) == batch_size:\n",
        "            x_batch = process_x(batch, seq_len, n_vocab)\n",
        "            y_batch = process_y(batch, seq_len, n_vocab)\n",
        "            x_batches.append(x_batch)\n",
        "            y_batches.append(y_batch)\n",
        "            batch = []\n",
        "\n",
        "    # Add the last batch if it's not full\n",
        "    if batch:\n",
        "        x_batch = process_x(batch, seq_len, n_vocab)\n",
        "        y_batch = process_y(batch, seq_len, n_vocab)\n",
        "        x_batches.append(x_batch)\n",
        "        y_batches.append(y_batch)\n",
        "\n",
        "    return x_batches, y_batches\n",
        "\n"
      ],
      "metadata": {
        "id": "2cc3PU2QO4DK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Client class"
      ],
      "metadata": {
        "id": "fnpUofSNgk7x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "486wbJDAggZx"
      },
      "outputs": [],
      "source": [
        "from torch.backends import cudnn\n",
        "import time\n",
        "\n",
        "\n",
        "class Client:\n",
        "    def __init__(self, client_id, data_loader, model, device):\n",
        "        \"\"\"\n",
        "        Initializes a federated learning client.\n",
        "        :param client_id: Unique identifier for the client.\n",
        "        :param data_loader: Data loader specific to the client.\n",
        "        :param model: The model class to be used by the client.\n",
        "        :param device: The device (CPU/GPU) to perform computations.\n",
        "        \"\"\"\n",
        "        self.client_id = client_id\n",
        "        self.data_loader = data_loader\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def client_update(self, client_data, criterion, optimizer, local_steps=4, detailed_print=False):\n",
        "        \"\"\"\n",
        "        Trains a given client's local model on its dataset for a fixed number of steps (`local_steps`).\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): The local model to be updated.\n",
        "            client_id (int): Identifier for the client (used for logging/debugging purposes).\n",
        "            client_data (DataLoader): The data loader for the client's dataset.\n",
        "            criterion (Loss): The loss function used for training (e.g., CrossEntropyLoss).\n",
        "            optimizer (Optimizer): The optimizer used for updating model parameters (e.g., SGD).\n",
        "            local_steps (int): Number of local epochs to train on the client's dataset.\n",
        "            detailed_print (bool): If True, logs the final loss after training.\n",
        "\n",
        "        Returns:\n",
        "            dict: The state dictionary of the updated model.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.model.train()  # Set the model to training mode\n",
        "        step_count = 0\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        while step_count < local_steps:\n",
        "            x_batches, y_batches = create_batches(train_data, BATCH_SIZE, SEQ_LEN, N_VOCAB)\n",
        "            for x_batch, y_batch in zip(x_batches, y_batches):\n",
        "\n",
        "                x_batch = x_batch.to(DEVICE)  # Move the data to the GPU\n",
        "                y_batch = y_batch.to(DEVICE)  # Move the targets to the GPU\n",
        "                # Move data and targets to the specified device (e.g., GPU or CPU)\n",
        "                #data, targets = data.to(self.device), targets.to(self.device)\n",
        "\n",
        "\n",
        "                start_time = time.time()  # for testing-----------------------------\n",
        "\n",
        "                # Reset the gradients before backpropagation\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass: compute model predictions\n",
        "                logits, _ = self.model(x_batch)\n",
        "\n",
        "                logits = logits.view(-1, N_VOCAB)  # Reshape to (batch_size * seq_length, vocab_size)\n",
        "                y_batch = y_batch.view(-1)  # Reshape to (batch_size * seq_length)\n",
        "\n",
        "                # Compute the loss\n",
        "                loss = criterion(logits, y_batch)\n",
        "\n",
        "                # Backward pass: compute gradients and update weights\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                #---------- Accumulate metrics\n",
        "                #  Accumulates the weighted loss for the number of samples in the batch to account for any variation in\n",
        "                #  batch size due to, for example, the smaller final batch. A little too precise? :)\n",
        "                total_loss += loss.item() * x_batch.size(0)\n",
        "                _, predicted = logits.max(1)\n",
        "                correct_predictions += predicted.eq(y_batch).sum().item()\n",
        "                total_samples += x_batch.size(0)\n",
        "\n",
        "                step_count +=1\n",
        "                if step_count >= local_steps:\n",
        "                  break\n",
        "\n",
        "        #---------- Compute averaged metrics\n",
        "        avg_loss = total_loss / total_samples\n",
        "        avg_accuracy = correct_predictions / total_samples * 100\n",
        "\n",
        "        # Optionally, print the loss for the last epoch of training\n",
        "        if detailed_print:\n",
        "          print(f'Client {self.client_id} --> Final Loss (Step {step_count}/{local_steps}): {loss.item()}')\n",
        "\n",
        "\n",
        "        # Return the updated model's state dictionary (weights)\n",
        "        return self.model.state_dict(), avg_loss, avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DIR = '/content/drive/MyDrive/colab_plots/'\n",
        "\n",
        "def client_selection(number_of_clients, clients_fraction, gamma=None):\n",
        "    \"\"\"\n",
        "    Selects a subset of clients based on uniform or skewed distribution.\n",
        "\n",
        "    Args:\n",
        "    number_of_clients (int): Total number of clients.\n",
        "    clients_fraction (float): Fraction of clients to be selected.\n",
        "    uniform (bool): If True, selects clients uniformly. If False, selects clients based on a skewed distribution.\n",
        "    gamma (float): Hyperparameter for the Dirichlet distribution controlling the skewness (only used if uniform=False).\n",
        "\n",
        "    Returns:\n",
        "    list: List of selected client indices.\n",
        "    \"\"\"\n",
        "    num_clients_to_select = int(number_of_clients * clients_fraction)\n",
        "\n",
        "    if gamma is None:\n",
        "        # Uniformly select clients without replacement\n",
        "        selected_clients = np.random.choice(number_of_clients, num_clients_to_select, replace=False)\n",
        "    else:\n",
        "        # Generate skewed probabilities using a Dirichlet distribution\n",
        "        probabilities = np.random.dirichlet(np.ones(number_of_clients) * gamma)\n",
        "        selected_clients = np.random.choice(number_of_clients, num_clients_to_select, replace=False, p=probabilities)\n",
        "\n",
        "    return selected_clients\n",
        "\n",
        "\n",
        "def plot_client_selection(client_selection_count, file_name):\n",
        "    \"\"\"\n",
        "    Bar plot to visualize the frequency of client selections in a federated learning simulation.\n",
        "\n",
        "    Args:\n",
        "        client_selection_count (list): list containing the number of times each client was selected.\n",
        "        file_name (str): name of the file to save the plot.\n",
        "    \"\"\"\n",
        "    # Fixed base directory\n",
        "    directory = DIR +  'plots_federated/'\n",
        "    # Ensure the base directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    num_clients = len(client_selection_count)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(num_clients), client_selection_count, alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel(\"Client ID\", fontsize=14)\n",
        "    plt.ylabel(\"Selection Count\", fontsize=14)\n",
        "    plt.title(\"Client Selection Frequency\", fontsize=16)\n",
        "    plt.xticks(range(num_clients), fontsize=10, rotation=90 if num_clients > 20 else 0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path, format=\"png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "def test(global_model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate the global model on the test dataset.\n",
        "\n",
        "    Args:\n",
        "        global_model (nn.Module): The global model to be evaluated.\n",
        "        test_loader (DataLoader): DataLoader for the test dataset.\n",
        "\n",
        "    Returns:\n",
        "        float: The accuracy of the model on the test dataset.\n",
        "    \"\"\"\n",
        "    test_accuracy, _ = evaluate(global_model, test_loader)\n",
        "    return test_accuracy\n",
        "\n",
        "def plot_metrics(train_accuracies, train_losses, val_accuracies,val_losses, file_name):\n",
        "    \"\"\"\n",
        "    Plot the training and validation metrics for a federated learning simulation.\n",
        "\n",
        "    Args:\n",
        "        train_accuracies (list): List of training accuracies.\n",
        "        train_losses (list): List of training losses.\n",
        "        val_accuracies (list): List of validation accuracies.\n",
        "        val_losses (list): List of validation losses.\n",
        "        file_name (str): Name of the file to save the plot.\n",
        "    \"\"\"\n",
        "    # Fixed base directory\n",
        "    directory = DIR + '/plots_federated/'\n",
        "    # Ensure the base directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Create a list of epochs for the x-axis\n",
        "    epochs = list(range(1, len(train_losses) + 1))\n",
        "\n",
        "    # Plot the training and validation losses\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n",
        "    plt.plot(epochs, val_losses, label='Validation Loss', color='red')\n",
        "    plt.xlabel('Rounds', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.title('Training and Validation Loss', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path.replace('.png', '_loss.png'), format='png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the training and validation accuracies\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n",
        "    plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='red')\n",
        "    plt.xlabel('Rounds', fontsize=14)\n",
        "    plt.ylabel('Accuracy', fontsize=14)\n",
        "    plt.title('Training and Validation Accuracy', fontsize=16)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(file_path.replace('.png', '_accuracy.png'), format='png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_data(global_model, val_accuracies, val_losses, train_accuracies, train_losses,client_count, file_name):\n",
        "    \"\"\"\n",
        "    Save the global model, val_accuracies, val_losses, train_accuracies,train_losses and client_count to a file.\n",
        "\n",
        "    Args:\n",
        "        global_model (nn.Module): The global model to be saved.\n",
        "        val_accuracies (list): List of validation accuracies.\n",
        "        val_losses (list): List of validation losses.\n",
        "        train_accuracies (list): List of training accuracies.\n",
        "        train_losses (list): List of training losses.\n",
        "        file_name (str): Name of the file to save the data.\n",
        "    \"\"\"\n",
        "    # Fixed base directory\n",
        "    directory = DIR + '/trained_models/'\n",
        "    # Ensure the base directory exists\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Save all data (model state and metrics) into a dictionary\n",
        "    save_dict = {\n",
        "        'model_state': global_model.state_dict(),\n",
        "        'val_accuracies': val_accuracies,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'train_losses': train_losses,\n",
        "        'client_count': client_count\n",
        "    }\n",
        "\n",
        "    # Save the dictionary to the specified file\n",
        "    torch.save(save_dict, file_path)\n",
        "    print(f\"Data saved successfully to {file_path}\")\n",
        "\n",
        "def load_data(model, file_name):\n",
        "    \"\"\"\n",
        "    Load the model weights and metrics from a file.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to load the weights into.\n",
        "        file_name (str): Name of the file to load the data from.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the model, val_accuracies, val_losses, train_accuracies train_losses and client_count.\n",
        "    \"\"\"\n",
        "    # Fixed base directory\n",
        "    directory = DIR+ 'trained_models/'\n",
        "    # Complete path for the file\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Load the saved data from the specified file\n",
        "    save_dict = torch.load(file_path)\n",
        "\n",
        "    # Load the model state\n",
        "    model.load_state_dict(save_dict['model_state'])\n",
        "\n",
        "    # Extract the metrics\n",
        "    val_accuracies = save_dict['val_accuracies']\n",
        "    val_losses = save_dict['val_losses']\n",
        "    train_accuracies = save_dict['train_accuracies']\n",
        "    train_losses = save_dict['train_losses']\n",
        "    client_count = save_dict['client_count']\n",
        "\n",
        "    print(f\"Data loaded successfully from {file_path}\")\n",
        "\n",
        "    return model, val_accuracies, val_losses, train_accuracies, train_losses,client_count"
      ],
      "metadata": {
        "id": "1RznPucS_sz-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Server class"
      ],
      "metadata": {
        "id": "CH58Z0_0gp30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "import os\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "\n",
        "class Server:\n",
        "    def __init__(self, global_model, device, CHECKPOINT_DIR):\n",
        "        self.global_model = global_model\n",
        "        self.device = device\n",
        "        self.CHECKPOINT_DIR = CHECKPOINT_DIR\n",
        "        # Ensure the checkpoint directory exists, creating it if necessary\n",
        "        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    def fedavg_aggregate(self, client_states, client_sizes, client_avg_losses, client_avg_accuracies):\n",
        "        \"\"\"\n",
        "        Aggregates model updates and client metrics from selected clients using the Federated Averaging (FedAvg) algorithm.\n",
        "        The updates and metrics are weighted by the size of each client's dataset.\n",
        "\n",
        "        Args:\n",
        "            global_model (nn.Module): The global model whose structure is used for aggregation.\n",
        "            client_states (list[dict]): A list of state dictionaries (model weights) from participating clients.\n",
        "            client_sizes (list[int]): A list of dataset sizes for the participating clients.\n",
        "            client_avg_losses (list[float]): A list of average losses for the participating clients.\n",
        "            client_avg_accuracies (list[float]): A list of average accuracies for the participating clients.\n",
        "\n",
        "        Returns:\n",
        "            tuple: The aggregated state dictionary with updated model parameters, global average loss, and global average accuracy.\n",
        "        \"\"\"\n",
        "        # Copy the global model's state dictionary for aggregation\n",
        "        new_state = deepcopy(self.global_model.state_dict())\n",
        "\n",
        "        # Calculate the total number of samples across all participating clients\n",
        "        total_samples = sum(client_sizes)\n",
        "\n",
        "        # Initialize all parameters in the new state to zero\n",
        "        for key in new_state:\n",
        "            new_state[key] = torch.zeros_like(new_state[key])\n",
        "\n",
        "        # Initialize metrics\n",
        "        total_loss = 0.0\n",
        "        total_accuracy = 0.0\n",
        "\n",
        "        # Perform a weighted average of client updates and metrics\n",
        "        for state, size, avg_loss, avg_accuracy in zip(client_states, client_sizes, client_avg_losses, client_avg_accuracies):\n",
        "            for key in new_state:\n",
        "                # Add the weighted contribution of each client's parameters\n",
        "                new_state[key] += (state[key] * size / total_samples)\n",
        "            total_loss += avg_loss * size\n",
        "            total_accuracy += avg_accuracy * size\n",
        "\n",
        "        # Calculate global metrics\n",
        "        global_avg_loss = total_loss / total_samples\n",
        "        global_avg_accuracy = total_accuracy / total_samples\n",
        "\n",
        "        # Return the aggregated state dictionary with updated weights and global metrics\n",
        "        return new_state, global_avg_loss, global_avg_accuracy\n",
        "\n",
        "\n",
        "    # Federated Learning Training Loop\n",
        "    def train_federated(self, criterion, trainloader, validloader, num_clients, num_classes, rounds, lr, momentum, batchsize, wd, C=0.1, local_steps=4, log_freq=100, detailed_print=False,gamma=None):\n",
        "        val_accuracies = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        train_losses = []\n",
        "        best_model_state = None  # The model with the best accuracy\n",
        "        client_selection_count = [0] * num_clients #Count how many times a client has been selected\n",
        "        best_val_acc = 0.0\n",
        "\n",
        "        shards = self.sharding(trainloader.dataset, num_clients, num_classes) #each shard represent the training data for one client\n",
        "        client_sizes = [len(shard) for shard in shards]\n",
        "\n",
        "        self.global_model.to(self.device) #as alwayse, we move the global model to the specified device (CPU or GPU)\n",
        "\n",
        "        #loading checkpoint if it exists\n",
        "        checkpoint_start_step, data_to_load = load_checkpoint(model=self.global_model,optimizer=None,hyperparameters=f\"LR{lr}_WD{wd}\", subfolder=\"Federated/\")\n",
        "        if data_to_load is not None:\n",
        "          val_accuracies = data_to_load['val_accuracies']\n",
        "          val_losses = data_to_load['val_losses']\n",
        "          train_accuracies = data_to_load['train_accuracies']\n",
        "          train_losses = data_to_load['train_losses']\n",
        "          client_selection_count = data_to_load['client_selection_count']\n",
        "\n",
        "\n",
        "        # ********************* HOW IT WORKS ***************************************\n",
        "        # The training runs for rounds iterations (GLOBAL_ROUNDS=2000)\n",
        "        # Each round simulates one communication step in federated learning, including:\n",
        "        # 1) client selection\n",
        "        # 2) local training (of each client)\n",
        "        # 3) central aggregation\n",
        "        for round_num in range(checkpoint_start_step, rounds):\n",
        "            if (round_num+1) % log_freq == 0 and detailed_print:\n",
        "              print(f\"------------------------------------- Round {round_num+1} ------------------------------------------------\" )\n",
        "\n",
        "            # 1) client selection: In each round, a fraction C (e.g., 10%) of clients is randomly selected to participate.\n",
        "            #     This reduces computation costs and mimics real-world scenarios where not all devices are active.\n",
        "            selected_clients = client_selection(num_clients, C,gamma)\n",
        "            client_states = []\n",
        "            client_avg_losses = []\n",
        "            client_avg_accuracies = []\n",
        "            for client_id in selected_clients:\n",
        "                client_selection_count[client_id] += 1\n",
        "\n",
        "            # 2) local training: for each client updates the model using the client's data for local_steps epochs\n",
        "            for client_id in selected_clients:\n",
        "                local_model = deepcopy(self.global_model) #it creates a local copy of the global model\n",
        "                optimizer = optim.SGD(local_model.parameters(), lr=lr, momentum=momentum, weight_decay=wd) #same of the centralized version\n",
        "                client_loader = DataLoader(shards[client_id], batch_size=batchsize, shuffle=True)\n",
        "\n",
        "                print_log =  (round_num+1) % log_freq == 0 and detailed_print\n",
        "                client = Client(client_id, client_loader, local_model, self.device)\n",
        "                client_local_state, client_avg_loss, client_avg_accuracy  = client.client_update(client_loader, criterion, optimizer, local_steps, print_log)\n",
        "\n",
        "                client_states.append(client_local_state)\n",
        "                client_avg_losses.append(client_avg_loss)\n",
        "                client_avg_accuracies.append(client_avg_accuracy)\n",
        "\n",
        "\n",
        "            # 3) central aggregation: aggregates participating client updates using fedavg_aggregate\n",
        "            #    and replaces the current parameters of global_model with the returned ones.\n",
        "            aggregated_state, train_loss, train_accuracy = self.fedavg_aggregate(client_states, [client_sizes[i] for i in selected_clients], client_avg_losses, client_avg_accuracies)\n",
        "\n",
        "            self.global_model.load_state_dict(aggregated_state)\n",
        "\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            train_losses.append(train_loss)\n",
        "            #Validation at the server\n",
        "            val_accuracy, val_loss = evaluate(self.global_model, validloader)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            val_losses.append(val_loss)\n",
        "            if val_accuracy > best_val_acc:\n",
        "                best_val_acc = val_accuracy\n",
        "                best_model_state = deepcopy(self.global_model.state_dict())\n",
        "            if(round_num+1) % 100 == 0:\n",
        "              print('Reached round '+str(round_num+1))\n",
        "            if (round_num+1) % log_freq ==0:\n",
        "                if(detailed_print):\n",
        "                  print(f\"--> best validation accuracy: {best_val_acc:.2f}\\n--> training accuracy: {train_accuracy:.2f}\")\n",
        "                  print(f\"--> validation loss: {val_loss:.4f}\\n--> training loss: {train_loss:.4f}\")\n",
        "\n",
        "                # checkpointing\n",
        "                checkpoint_data = {\n",
        "                    'val_accuracies': val_accuracies,\n",
        "                    'val_losses': val_losses,\n",
        "                    'train_accuracies': train_accuracies,\n",
        "                    'train_losses': train_losses,\n",
        "                    'client_selection_count': client_selection_count\n",
        "                }\n",
        "                save_checkpoint(self.global_model,optimizer=None, epoch=round_num, hyperparameters=f\"LR{lr}_WD{wd}\", subfolder=\"Federated/\", checkpoint_data=checkpoint_data)\n",
        "                if(detailed_print):\n",
        "                  print(f\"------------------------------ Round {round_num+1} terminated: model updated -----------------------------\\n\\n\" )\n",
        "\n",
        "        self.global_model.load_state_dict(best_model_state)\n",
        "\n",
        "        return self.global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count\n",
        "\n",
        "    def sharding(self, dataset, number_of_clients, number_of_classes=N_VOCAB):\n",
        "        \"\"\"\n",
        "        Function that performs the sharding of the dataset given as input.\n",
        "        dataset: dataset to be split;\n",
        "        number_of_clients: the number of partitions we want to obtain;\n",
        "        number_of_classes: (int) the number of classes inside each partition, or vocab len for IID;\n",
        "        \"\"\"\n",
        "\n",
        "        # Validation of input parameters\n",
        "        if not (1 <= number_of_classes <= N_VOCAB):\n",
        "            raise ValueError(\"number_of_classes should be an integer between 1 and 100\")\n",
        "\n",
        "        # Shuffle dataset indices for randomness\n",
        "        indices = np.random.permutation(len(dataset))\n",
        "\n",
        "        # Compute basic partition sizes\n",
        "        basic_partition_size = len(dataset) // number_of_clients\n",
        "        remainder = len(dataset) % number_of_clients\n",
        "\n",
        "        shards = []\n",
        "        start_idx = 0\n",
        "\n",
        "        if number_of_classes == N_VOCAB:  # IID Case\n",
        "            # Equally distribute indices among clients: we can just randomly assign to each client an equal amount of records\n",
        "            for i in range(number_of_clients):\n",
        "                end_idx = start_idx + basic_partition_size + (1 if i < remainder else 0)\n",
        "                shards.append(Subset(dataset, indices[start_idx:end_idx]))\n",
        "                start_idx = end_idx\n",
        "        else:  # non-IID Case\n",
        "            # Count of each class in the dataset\n",
        "            from collections import Counter\n",
        "            target_counts = Counter(target for _, target in dataset)\n",
        "\n",
        "            # Calculate per client class allocation\n",
        "            class_per_client = np.random.choice(list(target_counts.keys()), size=number_of_classes, replace=False)\n",
        "            class_idx = {class_: np.where([target == class_ for _, target in dataset])[0] for class_ in class_per_client}\n",
        "\n",
        "            # Assign class indices evenly to clients\n",
        "            for i in range(number_of_clients):\n",
        "                client_indices = np.array([], dtype=int)\n",
        "                for class_ in class_per_client:\n",
        "                    n_samples = len(class_idx[class_]) // number_of_clients + (1 if i < remainder else 0)\n",
        "                    client_indices = np.concatenate((client_indices, class_idx[class_][:n_samples]))\n",
        "                    class_idx[class_] = np.delete(class_idx[class_], np.arange(n_samples))\n",
        "\n",
        "                shards.append(Subset(dataset, indices=client_indices))\n",
        "\n",
        "        return shards"
      ],
      "metadata": {
        "id": "0utrAGQKgrx_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "j2DTaLJa2bP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level LSTM model for text processing tasks.\n",
        "    Includes embedding, LSTM, and a fully connected output layer.\n",
        "    We use:\n",
        "    - embedding size equal to 8;\n",
        "    - 2 LSTM layers, each with 256 nodes;\n",
        "    - densely connected softmax output layer.\n",
        "\n",
        "    We can avoid to use explicitly the softmax function in the model and\n",
        "    keep a cross entropy loss function as a loss function.\n",
        "\n",
        "    as mentioned in paper [2] (Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "    Jakub Konen, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size = 90, embedding_size = 8, lstm_hidden_dim = 256, seq_length=80):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
        "        self.lstm1 = nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size=lstm_hidden_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \"\"\"\n",
        "        # Layer 1: Embedding\n",
        "        x = self.embedding(x)  # Output shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Layer 2: First LSTM\n",
        "        x, _ = self.lstm1(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 3: Second LSTM\n",
        "        x, hidden = self.lstm2(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 4: Fully Connected Layer\n",
        "        x = self.fc(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Softmax Activation\n",
        "        #x = self.softmax(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "        return x, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden and cell states for the LSTM.\"\"\"\n",
        "        return (torch.zeros(2, batch_size),\n",
        "            torch.zeros(2, batch_size))\n",
        "        #2 is equal to the number of lstm layers!"
      ],
      "metadata": {
        "id": "W7MXn7lN2c7c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "RGFUsTed4mNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "metadata": {
        "id": "plaEwlMb5DHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for processing Shakespeare dialogues.\n",
        "    Converts input data into sequences of indices for input and target processing.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, seq_len, n_vocab):\n",
        "        \"\"\"\n",
        "        Initializes the ShakespeareDataset instance.\n",
        "\n",
        "        Args:\n",
        "            data: Dictionary containing dialogues (e.g., train_data or test_data).\n",
        "            seq_len: Length of sequences to generate for the model.\n",
        "            n_vocab: Size of the vocabulary for mapping characters to indices.\n",
        "        \"\"\"\n",
        "        self.data = list(data.values())  # Convert the dictionary values to a list\n",
        "        self.seq_len = seq_len  # Sequence length for the model\n",
        "        self.n_vocab = n_vocab  # Vocabulary size\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of dialogues in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a single sample (input and target) from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Processed input (x) and target (y) tensors for the model.\n",
        "        \"\"\"\n",
        "        dialogue = self.data[idx]  # Get the dialogue at the specified index\n",
        "        x = process_x([dialogue], self.seq_len, self.n_vocab)[0]  # Prepare input tensor\n",
        "        y = process_y([dialogue], self.seq_len, self.n_vocab)[0]  # Prepare target tensor\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "C7srz_sF5E74"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the dataset"
      ],
      "metadata": {
        "id": "mtpwOSfkAflq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version of the shakespeare dataset and save the path\n",
        "path = kagglehub.dataset_download(\"kewagbln/shakespeareonline\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "DATA_PATH = os.path.join(path, \"t8.shakespeare.txt\")\n",
        "OUTPUT_DIR = \"processed_data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FScjV04sAhY7",
        "outputId": "f06aeac0-6680-442e-edc6-c2216427bb56"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kewagbln/shakespeareonline?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1.97M/1.97M [00:00<00:00, 137MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/kewagbln/shakespeareonline/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_data, test_data = parse_shakespeare_file(DATA_PATH)\n",
        "\n",
        "train_dataset = ShakespeareDataset(train_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "test_dataset = ShakespeareDataset(test_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "\n",
        "# Split the train dataset into train and validation:\n",
        "train_size = int(0.9 * len(train_dataset))  # 90%\n",
        "valid_size = len(train_dataset) - train_size  # 10%\n",
        "#random split:\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Creation of the DataLoaders\n",
        "trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "Lx3QokYw5K-G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Model & Loss"
      ],
      "metadata": {
        "id": "g8p3bcnI5oyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_model = CharLSTM(vocab_size = N_VOCAB, embedding_size = EMBEDDING_SIZE, lstm_hidden_dim = LSTM_HIDDEN_DIM, seq_length = SEQ_LENGTH)\n",
        "global_model = global_model.to(DEVICE) # Move the model to the device\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "tdul87Pz5si3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the training"
      ],
      "metadata": {
        "id": "ttEDuyh55tNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\n",
        "gamma = 0.05, first hyperparameter tuning with 100 rounds, then training with 2000 rounds and testing\n",
        "\"\"\"\n",
        "# Generate 3 values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
        "lr_values = np.logspace(-3, -1, num=3)\n",
        "# Generate 4 values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
        "wd_values = np.logspace(-4, -1, num=4)\n",
        "rounds = 100 #fewer communication rounds for hyperparameter tuning\n",
        "best_val_accuracy = 0\n",
        "\n",
        "best_setting = None\n",
        "for lr in lr_values:\n",
        "    for wd in wd_values:\n",
        "        print(f\"Learning rate: {lr}, Weight decay: {wd}\")\n",
        "        global_model = CharLSTM(vocab_size = N_VOCAB, embedding_size = EMBEDDING_SIZE, lstm_hidden_dim = LSTM_HIDDEN_DIM, seq_length = SEQ_LENGTH)\n",
        "        global_model = global_model.to(DEVICE) # Move the model to the device\n",
        "        server = Server(global_model, DEVICE, CHECKPOINT_DIR)\n",
        "        global_model, val_accuracies, val_losses, train_accuracies, train_losses, client_selection_count = server.train_federated(criterion, trainloader, validloader, num_clients=NUM_CLIENTS, num_classes=N_VOCAB, rounds=rounds, lr=lr, momentum=MOMENTUM, batchsize=BATCH_SIZE, wd=wd, C=FRACTION_CLIENTS, local_steps=LOCAL_STEPS,log_freq=100, detailed_print=False,gamma=0.05)\n",
        "        plot_metrics(train_accuracies, train_losses,val_accuracies, val_losses, f\"Federatedgamma005Tuning_lr_{lr}_wd_{wd}.png\")\n",
        "        print(f\"Validation accuracy: {val_accuracies[-1]} with lr: {lr} and wd: {wd}\")\n",
        "        max_val_accuracy = max(val_accuracies)\n",
        "        if max_val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = max_val_accuracy\n",
        "            best_setting = (lr,wd)\n",
        "print(f\"Best setting: {best_setting} with validation accuracy: {best_val_accuracy}\")"
      ],
      "metadata": {
        "id": "621sFqNL5vF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28167de-95fd-40eb-b6f8-17b6bf330047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate: 0.001, Weight decay: 0.0001\n",
            "No checkpoint found, starting from epoch 1.\n",
            "Reached round 100\n",
            "Checkpoint saved: cartellaCheckpoint/Federated/model_epoch_99_params_LR0.001_WD0.0001.pth\n",
            "Validation accuracy: 18.11965811965812 with lr: 0.001 and wd: 0.0001\n",
            "Learning rate: 0.001, Weight decay: 0.001\n",
            "No checkpoint found, starting from epoch 1..\n",
            "\n",
            "\n",
            "Reached round 100\n",
            "Checkpoint saved: cartellaCheckpoint/Federated/model_epoch_99_params_LR0.001_WD0.001.pth\n",
            "Validation accuracy: 12.542735042735043 with lr: 0.001 and wd: 0.001\n",
            "Learning rate: 0.001, Weight decay: 0.01\n",
            "No checkpoint found, starting from epoch 1..\n",
            "\n",
            "\n",
            "Reached round 100\n",
            "Checkpoint saved: cartellaCheckpoint/Federated/model_epoch_99_params_LR0.001_WD0.01.pth\n",
            "Validation accuracy: 18.11965811965812 with lr: 0.001 and wd: 0.01\n",
            "Learning rate: 0.001, Weight decay: 0.1\n",
            "No checkpoint found, starting from epoch 1..\n",
            "\n",
            "\n",
            "Reached round 100\n",
            "Checkpoint saved: cartellaCheckpoint/Federated/model_epoch_99_params_LR0.001_WD0.1.pth\n",
            "Validation accuracy: 18.11965811965812 with lr: 0.001 and wd: 0.1\n",
            "Learning rate: 0.01, Weight decay: 0.0001\n",
            "No checkpoint found, starting from epoch 1..\n",
            "\n",
            "\n",
            "Reached round 100\n",
            "Checkpoint saved: cartellaCheckpoint/Federated/model_epoch_99_params_LR0.01_WD0.0001.pth\n",
            "Validation accuracy: 18.11965811965812 with lr: 0.01 and wd: 0.0001\n",
            "Learning rate: 0.01, Weight decay: 0.001\n",
            "No checkpoint found, starting from epoch 1..\n",
            "\n",
            "\n",
            "Reached round 100\n",
            "Checkpoint saved: cartellaCheckpoint/Federated/model_epoch_99_params_LR0.01_WD0.001.pth\n",
            "Validation accuracy: 18.11965811965812 with lr: 0.01 and wd: 0.001\n",
            "Learning rate: 0.01, Weight decay: 0.01\n",
            "No checkpoint found, starting from epoch 1..\n",
            "\n",
            "\n",
            "Reached round 100\n",
            "Checkpoint saved: cartellaCheckpoint/Federated/model_epoch_99_params_LR0.01_WD0.01.pth\n",
            "Validation accuracy: 18.11965811965812 with lr: 0.01 and wd: 0.01\n",
            "Learning rate: 0.01, Weight decay: 0.1\n",
            "No checkpoint found, starting from epoch 1..\n",
            "\n",
            "\n",
            "Reached round 100\n",
            "Checkpoint saved: cartellaCheckpoint/Federated/model_epoch_99_params_LR0.01_WD0.1.pth\n",
            "Validation accuracy: 18.11965811965812 with lr: 0.01 and wd: 0.1\n",
            "Learning rate: 0.1, Weight decay: 0.0001\n",
            "No checkpoint found, starting from epoch 1..\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}