{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWAhRJt5_i3e"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3okGAXgq_i3j",
        "outputId": "a522c09a-3d83-499d-e481-e0e65c70d1fa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import string\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import io\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "from Model import CharLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "wd = 0.0001\n",
        "momentum = 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTmrobfeI5Q7"
      },
      "source": [
        "# Import the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TD59XwRI7PL"
      },
      "source": [
        "We must import the dataset manually since it is taken by the LEAF project.\n",
        "\n",
        "So far the project is to go on the data folder of shakespeare and:\n",
        "1. ./get_data.sh inside the preprocess folder\n",
        "2. ./data_to_json.sh\n",
        "3. cd ..\n",
        "3. ././preprocess.sh -s niid --sf 0.2 -k 0 -t sample -tf 0.8 [depending on the preferencies]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM07ejLQKP3u"
      },
      "source": [
        "## Upload the training and the testing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please upload the training dataset provided by LEAF here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "root = tk.Tk()\n",
        "#root.withdraw()\n",
        "\n",
        "file_path = filedialog.askopenfilename(filetypes=[(\"JSON files\", \"*.json\")])\n",
        "\n",
        "if file_path:\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "            \n",
        "root.destroy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please upload the test dataset provided by LEAF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "root = tk.Tk()\n",
        "#root.withdraw()\n",
        "\n",
        "file_path = filedialog.askopenfilename(filetypes=[(\"JSON files\", \"*.json\")])\n",
        "\n",
        "if file_path:\n",
        "    with open(file_path, 'r') as file:\n",
        "        test_data = json.load(file)\n",
        "            \n",
        "root.destroy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCGPeiE9K5Ve"
      },
      "source": [
        "## Statistics of the dataset\n",
        "Just for testing porpouses we can print some statistics about the uploaded dataset.\n",
        "\n",
        "The values used for the train/test split and the number of his samples are inspired by:\n",
        "Acar, Durmus Alp Emre, et al. \"Federated learning based on dynamic regularization.\" arXiv preprint arXiv:2111.04263 (2021)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVm6X0uN3PO",
        "outputId": "3dd0972c-66f3-4d03-f4d8-7e3fb786909c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of train samples: 253569\n",
            "Total number of test samples: 50769\n"
          ]
        }
      ],
      "source": [
        "total_samples = sum(data['num_samples'])\n",
        "print(f\"Total number of train samples: {total_samples}\")\n",
        "\n",
        "total_samples = sum(test_data['num_samples'])\n",
        "print(f\"Total number of test samples: {total_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hlmGoKJWOCN8"
      },
      "outputs": [],
      "source": [
        "users = data['users']\n",
        "num_samples = data['num_samples']\n",
        "user_data = data['user_data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir8cweSzR-FO",
        "outputId": "85da940b-faff-4bab-9922-375522855e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of clients: 100\n"
          ]
        }
      ],
      "source": [
        "number_of_clients = len(users)\n",
        "print(f\"Number of clients: {number_of_clients}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy1FP3iYSGoD"
      },
      "source": [
        "## Vocab creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "scMFoC0WSI3r"
      },
      "outputs": [],
      "source": [
        "all_texts = ''.join([''.join(seq) for user in users for seq in user_data[user]['x']])\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "\n",
        "# Add the padding character\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi9H7dJNSwoc"
      },
      "source": [
        "## Covert data into indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UZaJF07uSy2W"
      },
      "outputs": [],
      "source": [
        "inputs = [[char_to_idx[char] for char in user_data[user]['x'][0]] for user in users]\n",
        "targets = [[char_to_idx[char] for char in user_data[user]['y'][0]] for user in users]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P_okFPTTAls"
      },
      "source": [
        "## Creation of TensorDataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ysDl6DJNUC_d"
      },
      "outputs": [],
      "source": [
        "input_tensors = [torch.tensor(seq) for seq in inputs]\n",
        "target_tensors = [torch.tensor([seq]) for seq in targets]\n",
        "\n",
        "chars = sorted(set(all_texts))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "char_to_idx['<pad>'] = len(char_to_idx)\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
        "\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "\n",
        "target_tensors = torch.cat(target_tensors, dim=0)\n",
        "\n",
        "dataset = TensorDataset(padded_inputs, target_tensors)\n",
        "batch_size = 4\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N94bWSOHZIsd"
      },
      "outputs": [],
      "source": [
        "def tensor_to_string(tensor, idx_to_char):\n",
        "    \"\"\"Converte un tensore di indici in una stringa di caratteri.\"\"\"\n",
        "    return ''.join(idx_to_char[idx.item()] for idx in tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WFq79CKNy8EW"
      },
      "outputs": [],
      "source": [
        "# Function to convert character in indices:\n",
        "# def char_to_tensor(characters):\n",
        "#     indices = [char_to_idx[char] for char in characters]\n",
        "#     return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "def char_to_tensor(characters):\n",
        "    indices = [char_to_idx.get(char, char_to_idx['<pad>']) for char in characters] # Get the index for the character. If not found, use the index for padding.\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "# Prepare the test samples:\n",
        "'''\n",
        "The leaf dataset is structured in the following way:\n",
        "Users: Each dataset in LEAF is distributed across a simulated set of users (playing actor). The data for\n",
        "each user is stored separately to mimic real-world scenarios where data is distributed\n",
        "across devices or clients.\n",
        "Data Format: For each user, the data include:\n",
        "    x: sentences declared by the \"user\"/\"device\".\n",
        "    y: Labels or outputs associated with the inputs.\n",
        "'''\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in data['users']:\n",
        "    for entry, target in zip(data['user_data'][user]['x'], data['user_data'][user]['y']):\n",
        "        input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
        "        target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
        "\n",
        "# Padding and creation ofDataLoader\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "targets = torch.cat(target_tensors)\n",
        "dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "suVFX7jpVbiX",
        "outputId": "f114933f-87ed-49d5-b761-7fa7e636079a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# Define the Model to use\n",
        "model = CharLSTM(vocab_size=len(char_to_idx))\n",
        "model.train()  # Set the model in training mode\n",
        "model = model.to(DEVICE)  # Move the entire model to the right device\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=200)  # T_max is the number of epochs\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "\n",
        "            # Reset the existing gradients (if any)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Initialize the hidden state:\n",
        "            hidden = model.init_hidden(inputs.size(0))\n",
        "            hidden = (hidden[0].to(DEVICE), hidden[1].to(DEVICE))\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = model(inputs, hidden)\n",
        "\n",
        "            # Calculate loss\n",
        "            outputs_flat = outputs.view(-1, len(char_to_idx))\n",
        "            targets_flat = targets.view(-1)\n",
        "            loss = criterion(outputs_flat, targets_flat)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_samples += targets_flat.size(0)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = outputs_flat.max(1)\n",
        "            total_correct += (predicted == targets_flat).sum().item()\n",
        "\n",
        "        # Adjust learning rate based on the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate loss and accuracy for the epoch\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        accuracy = total_correct / total_samples\n",
        "        train_losses.append(avg_loss)\n",
        "        train_accuracies.append(accuracy)\n",
        "\n",
        "        # Save the checkpoint:\n",
        "        \n",
        "        # Print statistics\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, %')\n",
        "\n",
        "# Execute the model:\n",
        "train_model(model, loader, criterion, optimizer, scheduler, num_epochs=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTiz78nbaCRx"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJOZBBEobRE1"
      },
      "outputs": [],
      "source": [
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for user in test_data['users']:\n",
        "    for entry, target in zip(test_data['user_data'][user]['x'], test_data['user_data'][user]['y']):\n",
        "        input_tensors.append(char_to_tensor(entry))  # Use the full sequence of x\n",
        "        target_tensors.append(char_to_tensor(target))  # Directly use the corresponding y as target\n",
        "\n",
        "\n",
        "padded_inputs = pad_sequence(input_tensors, batch_first=True, padding_value=char_to_idx['<pad>'])\n",
        "targets = torch.cat(target_tensors)\n",
        "test_dataset = TensorDataset(padded_inputs, targets)\n",
        "for elem1, elem2 in test_dataset:\n",
        "  elem2 = elem2.unsqueeze(0)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, targets in test_loader:\n",
        "    inputs = inputs.to(DEVICE)  # move input to correct dev\n",
        "    targets = targets.to(DEVICE)  # move target to correct dev\n",
        "\n",
        "    # Inizialize the hidden state\n",
        "    hidden = model.init_hidden(inputs.size(0))\n",
        "    hidden = (hidden[0].to(DEVICE), hidden[1].to(DEVICE))  # Move the hidden state to correct dev\n",
        "\n",
        "    outputs, _ = model(inputs, hidden)\n",
        "    outputs_flat = outputs.view(-1, len(char_to_idx))\n",
        "    targets_flat = targets.view(-1)\n",
        "    _, predicted = outputs_flat.max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += (predicted == targets_flat).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
