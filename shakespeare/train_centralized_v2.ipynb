{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWAhRJt5_i3e"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3okGAXgq_i3j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import string\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiHq71Y1_i3m"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of the dataset"
      ],
      "metadata": {
        "id": "HDfayYmcFxHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4-315RM_i3n",
        "outputId": "fb2c3b1d-f0d2-4d34-84c9-34b350c4e2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/kewagbln/shakespeareonline/versions/1\n"
          ]
        }
      ],
      "source": [
        "# Download latest version of the shakespeare dataset and save the path\n",
        "path = kagglehub.dataset_download(\"kewagbln/shakespeareonline\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "DATA_PATH = os.path.join(path, \"t8.shakespeare.txt\")\n",
        "OUTPUT_DIR = \"processed_data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paramters"
      ],
      "metadata": {
        "id": "AJoD-od7F01p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_FRACTION = 0.8  # percentage of the training data\n",
        "SEQ_LEN = 128  # length of the sequence for the model\n",
        "BATCH_SIZE = 32\n",
        "N_VOCAB = 128  # Numero di caratteri nel vocabolario (ASCII)\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.01"
      ],
      "metadata": {
        "id": "EwVPIZDTF4sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Parsing and Preprocessing\n",
        "This section includes regular expressions and functions to parse Shakespeare's text into plays, characters, and their respective dialogues. The parsing handles special cases like \"The Comedy of Errors\" and prepares training and test datasets."
      ],
      "metadata": {
        "id": "RKLbEQbbIQvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHARACTER_RE = re.compile(r'^  ([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Matches character lines\n",
        "CONT_RE = re.compile(r'^    (.*)')  # Matches continuation lines\n",
        "COE_CHARACTER_RE = re.compile(r'^([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Special regex for Comedy of Errors\n",
        "COE_CONT_RE = re.compile(r'^(.*)')  # Continuation for Comedy of Errors\n",
        "\n",
        "def parse_shakespeare_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads and splits Shakespeare's text into plays, characters, and their dialogues.\n",
        "    Returns training and test datasets based on the specified fraction.\n",
        "    \"\"\"\n",
        "    with open(filepath, \"r\") as f:\n",
        "        content = f.read()\n",
        "    plays, _ = _split_into_plays(content)  # Split the text into plays\n",
        "    _, train_examples, test_examples = _get_train_test_by_character(\n",
        "        plays, test_fraction=1 - TRAIN_FRACTION\n",
        "    )\n",
        "    return train_examples, test_examples\n",
        "\n",
        "def _split_into_plays(shakespeare_full):\n",
        "    \"\"\"\n",
        "    Splits the full Shakespeare text into individual plays and characters' dialogues.\n",
        "    Handles special parsing for \"The Comedy of Errors\".\n",
        "    \"\"\"\n",
        "    plays = []\n",
        "    slines = shakespeare_full.splitlines(True)[1:]  # Skip the first line (title/header)\n",
        "    current_character = None\n",
        "    comedy_of_errors = False\n",
        "\n",
        "    for i, line in enumerate(slines):\n",
        "        # Detect play titles and initialize character dictionary\n",
        "        if \"by William Shakespeare\" in line:\n",
        "            current_character = None\n",
        "            characters = defaultdict(list)\n",
        "            title = slines[i - 2].strip() if slines[i - 2].strip() else slines[i - 3].strip()\n",
        "            comedy_of_errors = title == \"THE COMEDY OF ERRORS\"\n",
        "            plays.append((title, characters))\n",
        "            continue\n",
        "\n",
        "        # Match character lines or continuation lines\n",
        "        match = _match_character_regex(line, comedy_of_errors)\n",
        "        if match:\n",
        "            character, snippet = match.group(1).upper(), match.group(2)\n",
        "            if not (comedy_of_errors and character.startswith(\"ACT \")):\n",
        "                characters[character].append(snippet)\n",
        "                current_character = character\n",
        "        elif current_character:\n",
        "            match = _match_continuation_regex(line, comedy_of_errors)\n",
        "            if match:\n",
        "                characters[current_character].append(match.group(1))\n",
        "\n",
        "    # Filter out plays with insufficient dialogue data\n",
        "    return [play for play in plays if len(play[1]) > 1], []\n",
        "\n",
        "def _match_character_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches character dialogues, with special handling for 'The Comedy of Errors'.\"\"\"\n",
        "    return COE_CHARACTER_RE.match(line) if comedy_of_errors else CHARACTER_RE.match(line)\n",
        "\n",
        "def _match_continuation_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches continuation lines of dialogues.\"\"\"\n",
        "    return COE_CONT_RE.match(line) if comedy_of_errors else CONT_RE.match(line)\n",
        "\n",
        "def _get_train_test_by_character(plays, test_fraction=0.2):\n",
        "    \"\"\"\n",
        "    Splits dialogues by characters into training and testing datasets.\n",
        "    Ensures each character has at least one example in the training set.\n",
        "    \"\"\"\n",
        "    all_train_examples = defaultdict(list)\n",
        "    all_test_examples = defaultdict(list)\n",
        "\n",
        "    def add_examples(example_dict, example_tuple_list):\n",
        "        \"\"\"Adds examples to the respective dataset dictionary.\"\"\"\n",
        "        for play, character, sound_bite in example_tuple_list:\n",
        "            example_dict[f\"{play}_{character}\".replace(\" \", \"_\")].append(sound_bite)\n",
        "\n",
        "    for play, characters in plays:\n",
        "        for character, sound_bites in characters.items():\n",
        "            examples = [(play, character, sound_bite) for sound_bite in sound_bites]\n",
        "            if len(examples) <= 2:\n",
        "                continue\n",
        "\n",
        "            # Calculate the number of test samples\n",
        "            num_test = max(1, int(len(examples) * test_fraction))\n",
        "            num_test = min(num_test, len(examples) - 1)  # Ensure at least one training example\n",
        "\n",
        "            # Split into train and test sets\n",
        "            train_examples = examples[:-num_test]\n",
        "            test_examples = examples[-num_test:]\n",
        "\n",
        "            add_examples(all_train_examples, train_examples)\n",
        "            add_examples(all_test_examples, test_examples)\n",
        "\n",
        "    return {}, all_train_examples, all_test_examples\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N4lqN7-UINzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing\n",
        "Functions to convert characters and words into numerical representations\n",
        "for use in neural networks. Includes padding logic for batch processing."
      ],
      "metadata": {
        "id": "qiYPyuLRIcI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def letter_to_vec(c, n_vocab=128):\n",
        "    \"\"\"Converts a single character to a vector index based on the vocabulary size.\"\"\"\n",
        "    return ord(c) % n_vocab\n",
        "\n",
        "def word_to_indices(word, n_vocab=128):\n",
        "    \"\"\"\n",
        "    Converts a word or list of words into a list of indices.\n",
        "    Each character is mapped to an index based on the vocabulary size.\n",
        "    \"\"\"\n",
        "    if isinstance(word, list):  # If input is a list of words\n",
        "        res = []\n",
        "        for stringa in word:\n",
        "            res.extend([ord(c) % n_vocab for c in stringa])  # Convert each word to indices\n",
        "        return res\n",
        "    else:  # If input is a single word\n",
        "        return [ord(c) % n_vocab for c in word]\n",
        "\n",
        "def process_x(raw_x_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw input data into padded sequences of indices.\n",
        "    Ensures all sequences are of uniform length.\n",
        "    \"\"\"\n",
        "    x_batch = [word_to_indices(word, n_vocab) for word in raw_x_batch]\n",
        "    x_batch = [x[:seq_len] + [0] * (seq_len - len(x)) for x in x_batch]\n",
        "    return torch.tensor(x_batch, dtype=torch.long)\n",
        "\n",
        "def process_y(raw_y_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw target data into padded sequences of indices.\n",
        "    Similar to process_x but for target labels.\n",
        "    \"\"\"\n",
        "    y_batch = [word_to_indices(word, n_vocab) for word in raw_y_batch]\n",
        "    y_batch = [y[:seq_len] + [0] * (seq_len - len(y)) for y in y_batch]\n",
        "    return torch.tensor(y_batch, dtype=torch.long)\n",
        "\n",
        "def create_batches(data, batch_size, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Creates batches of input and target data from dialogues.\n",
        "    Each batch contains sequences of uniform length.\n",
        "    \"\"\"\n",
        "    x_batches = []\n",
        "    y_batches = []\n",
        "    dialogues = list(data.values())\n",
        "    random.shuffle(dialogues)  # Shuffle to ensure randomness in batches\n",
        "\n",
        "    batch = []\n",
        "    for dialogue in dialogues:\n",
        "        batch.append(dialogue)\n",
        "        if len(batch) == batch_size:\n",
        "            x_batch = process_x(batch, seq_len, n_vocab)\n",
        "            y_batch = process_y(batch, seq_len, n_vocab)\n",
        "            x_batches.append(x_batch)\n",
        "            y_batches.append(y_batch)\n",
        "            batch = []\n",
        "\n",
        "    # Add the last batch if it's not full\n",
        "    if batch:\n",
        "        x_batch = process_x(batch, seq_len, n_vocab)\n",
        "        y_batch = process_y(batch, seq_len, n_vocab)\n",
        "        x_batches.append(x_batch)\n",
        "        y_batches.append(y_batch)\n",
        "\n",
        "    return x_batches, y_batches\n",
        "\n"
      ],
      "metadata": {
        "id": "f7cJk6_PIckr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader\n",
        "This code defines a custom PyTorch Dataset class for processing Shakespeare dialogues.\n",
        "The dataset converts raw text data into sequences of indices for training a character-level model.\n"
      ],
      "metadata": {
        "id": "pxsIfbBOLCTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for processing Shakespeare dialogues.\n",
        "    Converts input data into sequences of indices for input and target processing.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, seq_len, n_vocab):\n",
        "        \"\"\"\n",
        "        Initializes the ShakespeareDataset instance.\n",
        "\n",
        "        Args:\n",
        "            data: Dictionary containing dialogues (e.g., train_data or test_data).\n",
        "            seq_len: Length of sequences to generate for the model.\n",
        "            n_vocab: Size of the vocabulary for mapping characters to indices.\n",
        "        \"\"\"\n",
        "        self.data = list(data.values())  # Convert the dictionary values to a list\n",
        "        self.seq_len = seq_len  # Sequence length for the model\n",
        "        self.n_vocab = n_vocab  # Vocabulary size\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of dialogues in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a single sample (input and target) from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Processed input (x) and target (y) tensors for the model.\n",
        "        \"\"\"\n",
        "        dialogue = self.data[idx]  # Get the dialogue at the specified index\n",
        "        x = process_x([dialogue], self.seq_len, self.n_vocab)[0]  # Prepare input tensor\n",
        "        y = process_y([dialogue], self.seq_len, self.n_vocab)[0]  # Prepare target tensor\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "4nyJjLKULERy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "CharLSTM is a character-based LSTM model designed for text generation.\n",
        "It includes embedding, LSTM layers, and a fully connected layer for output."
      ],
      "metadata": {
        "id": "oBSf9y7_IlLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level LSTM model for text processing tasks.\n",
        "    Includes embedding, LSTM, and a fully connected output layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        Outputs log probabilities for each character.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        output = self.fc(output)  # Fully connected layer for output\n",
        "        output = F.log_softmax(output, dim=-1)  # Log probabilities\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden and cell states for the LSTM.\"\"\"\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n"
      ],
      "metadata": {
        "id": "CTQiMMyhIleP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n",
        "Trains the CharLSTM model using the prepared data and batches."
      ],
      "metadata": {
        "id": "Ocn8uGLYIsLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Parsing dataset...\")\n",
        "    train_data, test_data = parse_shakespeare_file(DATA_PATH)\n",
        "\n",
        "    train_dataset = ShakespeareDataset(train_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "    val_dataset = ShakespeareDataset(test_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "\n",
        "    # Creazione dei DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    print(\"Preparing model...\")\n",
        "    model = CharLSTM(input_size=N_VOCAB, embedding_size=8, hidden_size=256, num_layers=2, output_size=N_VOCAB)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Create batches for the current epoch\n",
        "        x_batches, y_batches = create_batches(train_data, BATCH_SIZE, SEQ_LEN, N_VOCAB)\n",
        "\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            state = model.init_hidden(x_batch.size(0))  # Initialize hidden state for the batch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits, _ = model(x_batch, state)  # Forward pass\n",
        "            loss = criterion(logits, y_batch)  # Calculate loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(x_batches):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TRXzXzqIsfQ",
        "outputId": "4bdb6114-8105-491e-9389-568a296add72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing dataset...\n",
            "Preparing model...\n",
            "Training model...\n",
            "Epoch 1/200, Loss: 4.8474\n",
            "Epoch 2/200, Loss: 4.8291\n",
            "Epoch 3/200, Loss: 4.8229\n",
            "Epoch 4/200, Loss: 4.8197\n",
            "Epoch 5/200, Loss: 4.8150\n",
            "Epoch 6/200, Loss: 4.8101\n",
            "Epoch 7/200, Loss: 4.8024\n",
            "Epoch 8/200, Loss: 4.7895\n",
            "Epoch 9/200, Loss: 4.7711\n",
            "Epoch 10/200, Loss: 4.7407\n",
            "Epoch 11/200, Loss: 4.6985\n",
            "Epoch 12/200, Loss: 4.6426\n",
            "Epoch 13/200, Loss: 4.5835\n",
            "Epoch 14/200, Loss: 4.5250\n",
            "Epoch 15/200, Loss: 4.4620\n",
            "Epoch 16/200, Loss: 4.4000\n",
            "Epoch 17/200, Loss: 4.3419\n",
            "Epoch 18/200, Loss: 4.2886\n",
            "Epoch 19/200, Loss: 4.2385\n",
            "Epoch 20/200, Loss: 4.1811\n",
            "Epoch 21/200, Loss: 4.1306\n",
            "Epoch 22/200, Loss: 4.0876\n",
            "Epoch 23/200, Loss: 4.0406\n",
            "Epoch 24/200, Loss: 4.0028\n",
            "Epoch 25/200, Loss: 3.9697\n",
            "Epoch 26/200, Loss: 3.9266\n",
            "Epoch 27/200, Loss: 3.8821\n",
            "Epoch 28/200, Loss: 3.8448\n",
            "Epoch 29/200, Loss: 3.8133\n",
            "Epoch 30/200, Loss: 3.7763\n",
            "Epoch 31/200, Loss: 3.7468\n",
            "Epoch 32/200, Loss: 3.7186\n",
            "Epoch 33/200, Loss: 3.6944\n",
            "Epoch 34/200, Loss: 3.6708\n",
            "Epoch 35/200, Loss: 3.6359\n",
            "Epoch 36/200, Loss: 3.6060\n",
            "Epoch 37/200, Loss: 3.5794\n",
            "Epoch 38/200, Loss: 3.5504\n",
            "Epoch 39/200, Loss: 3.5356\n",
            "Epoch 40/200, Loss: 3.5105\n",
            "Epoch 41/200, Loss: 3.4919\n",
            "Epoch 42/200, Loss: 3.4759\n",
            "Epoch 43/200, Loss: 3.4579\n",
            "Epoch 44/200, Loss: 3.4339\n",
            "Epoch 45/200, Loss: 3.4039\n",
            "Epoch 46/200, Loss: 3.3805\n",
            "Epoch 47/200, Loss: 3.3614\n",
            "Epoch 48/200, Loss: 3.3508\n",
            "Epoch 49/200, Loss: 3.3323\n",
            "Epoch 50/200, Loss: 3.3125\n",
            "Epoch 51/200, Loss: 3.2885\n",
            "Epoch 52/200, Loss: 3.2897\n",
            "Epoch 53/200, Loss: 3.2704\n",
            "Epoch 54/200, Loss: 3.2556\n",
            "Epoch 55/200, Loss: 3.2406\n",
            "Epoch 56/200, Loss: 3.2290\n",
            "Epoch 57/200, Loss: 3.2046\n",
            "Epoch 58/200, Loss: 3.1953\n",
            "Epoch 59/200, Loss: 3.1792\n",
            "Epoch 60/200, Loss: 3.1763\n",
            "Epoch 61/200, Loss: 3.1651\n",
            "Epoch 62/200, Loss: 3.1523\n",
            "Epoch 63/200, Loss: 3.1454\n",
            "Epoch 64/200, Loss: 3.1237\n",
            "Epoch 65/200, Loss: 3.1024\n",
            "Epoch 66/200, Loss: 3.0985\n",
            "Epoch 67/200, Loss: 3.0919\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}