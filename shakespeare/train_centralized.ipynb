{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWAhRJt5_i3e"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3okGAXgq_i3j"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andrea\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import string\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiHq71Y1_i3m"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDfayYmcFxHh"
      },
      "source": [
        "## Loading of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4-315RM_i3n",
        "outputId": "7039dc95-b3ff-4a02-93eb-040afe567ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\andrea\\.cache\\kagglehub\\datasets\\kewagbln\\shakespeareonline\\versions\\1\n"
          ]
        }
      ],
      "source": [
        "# Download latest version of the shakespeare dataset and save the path\n",
        "path = kagglehub.dataset_download(\"kewagbln/shakespeareonline\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "DATA_PATH = os.path.join(path, \"t8.shakespeare.txt\")\n",
        "OUTPUT_DIR = \"processed_data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJoD-od7F01p"
      },
      "source": [
        "## Paramters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EwVPIZDTF4sM"
      },
      "outputs": [],
      "source": [
        "TRAIN_FRACTION = 0.9  # percentage of the training data\n",
        "SEQ_LEN = 80  # length of the sequence for the model\n",
        "BATCH_SIZE = 4\n",
        "N_VOCAB = 90  # Numero di caratteri nel vocabolario (ASCII)\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.01\n",
        "EMBEDDING_SIZE = 8\n",
        "LSTM_HIDDEN_DIM = 256\n",
        "SEQ_LENGTH = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKLbEQbbIQvh"
      },
      "source": [
        "# Dataset Parsing and Preprocessing\n",
        "This section includes regular expressions and functions to parse Shakespeare's text into plays, characters, and their respective dialogues. The parsing handles special cases like \"The Comedy of Errors\" and prepares training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N4lqN7-UINzE"
      },
      "outputs": [],
      "source": [
        "CHARACTER_RE = re.compile(r'^  ([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Matches character lines\n",
        "CONT_RE = re.compile(r'^    (.*)')  # Matches continuation lines\n",
        "COE_CHARACTER_RE = re.compile(r'^([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Special regex for Comedy of Errors\n",
        "COE_CONT_RE = re.compile(r'^(.*)')  # Continuation for Comedy of Errors\n",
        "\n",
        "def parse_shakespeare_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads and splits Shakespeare's text into plays, characters, and their dialogues.\n",
        "    Returns training and test datasets based on the specified fraction.\n",
        "    \"\"\"\n",
        "    with open(filepath, \"r\") as f:\n",
        "        content = f.read()\n",
        "    plays, _ = _split_into_plays(content)  # Split the text into plays\n",
        "    _, train_examples, test_examples = _get_train_test_by_character(\n",
        "        plays, test_fraction=1 - TRAIN_FRACTION\n",
        "    )\n",
        "    return train_examples, test_examples\n",
        "\n",
        "def _split_into_plays(shakespeare_full):\n",
        "    \"\"\"\n",
        "    Splits the full Shakespeare text into individual plays and characters' dialogues.\n",
        "    Handles special parsing for \"The Comedy of Errors\".\n",
        "    \"\"\"\n",
        "    plays = []\n",
        "    slines = shakespeare_full.splitlines(True)[1:]  # Skip the first line (title/header)\n",
        "    current_character = None\n",
        "    comedy_of_errors = False\n",
        "\n",
        "    for i, line in enumerate(slines):\n",
        "        # Detect play titles and initialize character dictionary\n",
        "        if \"by William Shakespeare\" in line:\n",
        "            current_character = None\n",
        "            characters = defaultdict(list)\n",
        "            title = slines[i - 2].strip() if slines[i - 2].strip() else slines[i - 3].strip()\n",
        "            comedy_of_errors = title == \"THE COMEDY OF ERRORS\"\n",
        "            plays.append((title, characters))\n",
        "            continue\n",
        "\n",
        "        # Match character lines or continuation lines\n",
        "        match = _match_character_regex(line, comedy_of_errors)\n",
        "        if match:\n",
        "            character, snippet = match.group(1).upper(), match.group(2)\n",
        "            if not (comedy_of_errors and character.startswith(\"ACT \")):\n",
        "                characters[character].append(snippet)\n",
        "                current_character = character\n",
        "        elif current_character:\n",
        "            match = _match_continuation_regex(line, comedy_of_errors)\n",
        "            if match:\n",
        "                characters[current_character].append(match.group(1))\n",
        "\n",
        "    # Filter out plays with insufficient dialogue data\n",
        "    return [play for play in plays if len(play[1]) > 1], []\n",
        "\n",
        "def _match_character_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches character dialogues, with special handling for 'The Comedy of Errors'.\"\"\"\n",
        "    return COE_CHARACTER_RE.match(line) if comedy_of_errors else CHARACTER_RE.match(line)\n",
        "\n",
        "def _match_continuation_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches continuation lines of dialogues.\"\"\"\n",
        "    return COE_CONT_RE.match(line) if comedy_of_errors else CONT_RE.match(line)\n",
        "\n",
        "def _get_train_test_by_character(plays, test_fraction=0.2):\n",
        "    \"\"\"\n",
        "    Splits dialogues by characters into training and testing datasets.\n",
        "    Ensures each character has at least one example in the training set.\n",
        "    \"\"\"\n",
        "    all_train_examples = defaultdict(list)\n",
        "    all_test_examples = defaultdict(list)\n",
        "\n",
        "    def add_examples(example_dict, example_tuple_list):\n",
        "        \"\"\"Adds examples to the respective dataset dictionary.\"\"\"\n",
        "        for play, character, sound_bite in example_tuple_list:\n",
        "            example_dict[f\"{play}_{character}\".replace(\" \", \"_\")].append(sound_bite)\n",
        "\n",
        "    for play, characters in plays:\n",
        "        for character, sound_bites in characters.items():\n",
        "            examples = [(play, character, sound_bite) for sound_bite in sound_bites]\n",
        "            if len(examples) <= 2:\n",
        "                continue\n",
        "\n",
        "            # Calculate the number of test samples\n",
        "            num_test = max(1, int(len(examples) * test_fraction))\n",
        "            num_test = min(num_test, len(examples) - 1)  # Ensure at least one training example\n",
        "\n",
        "            # Split into train and test sets\n",
        "            train_examples = examples[:-num_test]\n",
        "            test_examples = examples[-num_test:]\n",
        "\n",
        "            add_examples(all_train_examples, train_examples)\n",
        "            add_examples(all_test_examples, test_examples)\n",
        "\n",
        "    return {}, all_train_examples, all_test_examples\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiYPyuLRIcI-"
      },
      "source": [
        "# Text Processing\n",
        "Functions to convert characters and words into numerical representations\n",
        "for use in neural networks. Includes padding logic for batch processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f7cJk6_PIckr"
      },
      "outputs": [],
      "source": [
        "def letter_to_vec(c, n_vocab=128):\n",
        "    \"\"\"Converts a single character to a vector index based on the vocabulary size.\"\"\"\n",
        "    return ord(c) % n_vocab\n",
        "\n",
        "def word_to_indices(word, n_vocab=128):\n",
        "    \"\"\"\n",
        "    Converts a word or list of words into a list of indices.\n",
        "    Each character is mapped to an index based on the vocabulary size.\n",
        "    \"\"\"\n",
        "    if isinstance(word, list):  # If input is a list of words\n",
        "        res = []\n",
        "        for stringa in word:\n",
        "            res.extend([ord(c) % n_vocab for c in stringa])  # Convert each word to indices\n",
        "        return res\n",
        "    else:  # If input is a single word\n",
        "        return [ord(c) % n_vocab for c in word]\n",
        "\n",
        "def process_x(raw_x_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw input data into padded sequences of indices.\n",
        "    Ensures all sequences are of uniform length.\n",
        "    \"\"\"\n",
        "    x_batch = [word_to_indices(word, n_vocab) for word in raw_x_batch]\n",
        "    x_batch = [x[:seq_len] + [0] * (seq_len - len(x)) for x in x_batch]\n",
        "    return torch.tensor(x_batch, dtype=torch.long)\n",
        "\n",
        "\n",
        "def process_y(raw_y_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw target data into padded sequences of indices.\n",
        "    Shifts the sequence by one character to the right.\n",
        "    y[1:seq_len + 1] takes the input data, right shift of an\n",
        "    element and uses the next element of the sequence to fill\n",
        "    and at the end (with [0]) final padding (zeros) are (eventually)\n",
        "    added to reach the desired sequence length.\n",
        "    \"\"\"\n",
        "    y_batch = [word_to_indices(word, n_vocab) for word in raw_y_batch]\n",
        "    y_batch = [y[1:seq_len + 1] + [0] * (seq_len - len(y[1:seq_len + 1])) for y in y_batch]  # Shifting and final padding\n",
        "    return torch.tensor(y_batch, dtype=torch.long)\n",
        "\n",
        "def create_batches(data, batch_size, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Creates batches of input and target data from dialogues.\n",
        "    Each batch contains sequences of uniform length.\n",
        "    \"\"\"\n",
        "    x_batches = []\n",
        "    y_batches = []\n",
        "    dialogues = list(data.values())\n",
        "    random.shuffle(dialogues)  # Shuffle to ensure randomness in batches\n",
        "\n",
        "    batch = []\n",
        "    for dialogue in dialogues:\n",
        "        batch.append(dialogue)\n",
        "        if len(batch) == batch_size:\n",
        "            x_batch = process_x(batch, seq_len, n_vocab)\n",
        "            y_batch = process_y(batch, seq_len, n_vocab)\n",
        "            x_batches.append(x_batch)\n",
        "            y_batches.append(y_batch)\n",
        "            batch = []\n",
        "\n",
        "    # Add the last batch if it's not full\n",
        "    if batch:\n",
        "        x_batch = process_x(batch, seq_len, n_vocab)\n",
        "        y_batch = process_y(batch, seq_len, n_vocab)\n",
        "        x_batches.append(x_batch)\n",
        "        y_batches.append(y_batch)\n",
        "\n",
        "    return x_batches, y_batches\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxsIfbBOLCTt"
      },
      "source": [
        "# Data Loader\n",
        "This code defines a custom PyTorch Dataset class for processing Shakespeare dialogues.\n",
        "The dataset converts raw text data into sequences of indices for training a character-level model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4nyJjLKULERy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for processing Shakespeare dialogues.\n",
        "    Converts input data into sequences of indices for input and target processing.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, seq_len, n_vocab):\n",
        "        \"\"\"\n",
        "        Initializes the ShakespeareDataset instance.\n",
        "\n",
        "        Args:\n",
        "            data: Dictionary containing dialogues (e.g., train_data or test_data).\n",
        "            seq_len: Length of sequences to generate for the model.\n",
        "            n_vocab: Size of the vocabulary for mapping characters to indices.\n",
        "        \"\"\"\n",
        "        self.data = list(data.values())  # Convert the dictionary values to a list\n",
        "        self.seq_len = seq_len  # Sequence length for the model\n",
        "        self.n_vocab = n_vocab  # Vocabulary size\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of dialogues in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a single sample (input and target) from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Processed input (x) and target (y) tensors for the model.\n",
        "        \"\"\"\n",
        "        dialogue = self.data[idx]  # Get the dialogue at the specified index\n",
        "        x = process_x([dialogue], self.seq_len, self.n_vocab)[0]  # Prepare input tensor\n",
        "        y = process_y([dialogue], self.seq_len, self.n_vocab)[0]  # Prepare target tensor\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBSf9y7_IlLh"
      },
      "source": [
        "# Model Definition\n",
        "CharLSTM is a character-based LSTM model designed for text generation.\n",
        "It includes embedding, LSTM layers, and a fully connected layer for output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CTQiMMyhIleP"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level LSTM model for text processing tasks.\n",
        "    Includes embedding, LSTM, and a fully connected output layer.\n",
        "    We use:\n",
        "    - embedding size equal to 8;\n",
        "    - 2 LSTM layers, each with 256 nodes;\n",
        "    - densely connected softmax output layer.\n",
        "\n",
        "    We can avoid to use explicitly the softmax function in the model and\n",
        "    keep a cross entropy loss function as a loss function.\n",
        "\n",
        "    as mentioned in paper [2] (Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "    Jakub Konečný, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size = 90, embedding_size = 8, lstm_hidden_dim = 256, seq_length=80):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
        "        self.lstm1 = nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(input_size=lstm_hidden_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \"\"\"\n",
        "        # Layer 1: Embedding\n",
        "        x = self.embedding(x)  # Output shape: (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # Layer 2: First LSTM\n",
        "        x, _ = self.lstm1(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 3: Second LSTM\n",
        "        x, hidden = self.lstm2(x)  # Output shape: (batch_size, seq_length, lstm_hidden_dim)\n",
        "\n",
        "        # Layer 4: Fully Connected Layer\n",
        "        x = self.fc(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Softmax Activation\n",
        "        #x = self.softmax(x)  # Output shape: (batch_size, seq_length, vocab_size)\n",
        "        return x, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden and cell states for the LSTM.\"\"\"\n",
        "        return (torch.zeros(2, batch_size),\n",
        "            torch.zeros(2, batch_size))\n",
        "        #2 is equal to the number of lstm layers!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ocn8uGLYIsLP"
      },
      "source": [
        "# Training Loop\n",
        "Trains the CharLSTM model using the prepared data and batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz_XAVsVxITc"
      },
      "source": [
        "Utility function to evaluate the accuracy and loss of a on a validation/test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4fnioLueKYk"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dBY-TXP5eEQl"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\andrea\\\\.cache\\\\kagglehub\\\\datasets\\\\kewagbln\\\\shakespeareonline\\\\versions\\\\1\\\\t8.shakespeare.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_split\n\u001b[1;32m----> 3\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mparse_shakespeare_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ShakespeareDataset(train_data, seq_len\u001b[38;5;241m=\u001b[39mSEQ_LEN, n_vocab\u001b[38;5;241m=\u001b[39mN_VOCAB)\n\u001b[0;32m      6\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m ShakespeareDataset(test_data, seq_len\u001b[38;5;241m=\u001b[39mSEQ_LEN, n_vocab\u001b[38;5;241m=\u001b[39mN_VOCAB)\n",
            "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mparse_shakespeare_file\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_shakespeare_file\u001b[39m(filepath):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    Reads and splits Shakespeare's text into plays, characters, and their dialogues.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Returns training and test datasets based on the specified fraction.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m         content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     13\u001b[0m     plays, _ \u001b[38;5;241m=\u001b[39m _split_into_plays(content)  \u001b[38;5;66;03m# Split the text into plays\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\andrea\\\\.cache\\\\kagglehub\\\\datasets\\\\kewagbln\\\\shakespeareonline\\\\versions\\\\1\\\\t8.shakespeare.txt'"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_data, test_data = parse_shakespeare_file(DATA_PATH)\n",
        "\n",
        "train_dataset = ShakespeareDataset(train_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "test_dataset = ShakespeareDataset(test_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "\n",
        "# Split the train dataset into train and validation:\n",
        "train_size = int(0.9 * len(train_dataset))  # 90%\n",
        "valid_size = len(train_dataset) - train_size  # 10%\n",
        "#random split:\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Creation of the DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IkiuJYN-xRSd"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "\n",
        "def evaluate(model, dataloader, criterion, DEVICE):\n",
        "    with torch.no_grad():\n",
        "        model.train(False) # Set Network to evaluation mode\n",
        "        running_corrects = 0\n",
        "        total_predictions = 0  # Track total predictions for normalization\n",
        "        losses = []\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(DEVICE)        # Move the data to the GPU\n",
        "            targets = targets.to(DEVICE)  # Move the targets to the GPU\n",
        "            # Forward Pass\n",
        "            #state = model.init_hidden(data.size(0))\n",
        "            # outputs is a tuple: (logits, hidden_state)\n",
        "            outputs, _ = model(data) # unpack the tuple and get only the output (predictions)\n",
        "            # Reshape the outputs for CrossEntropyLoss\n",
        "            outputs = outputs.view(-1, model.vocab_size)\n",
        "            targets = targets.view(-1)\n",
        "            loss = criterion(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            # Update Corrects (element-wise comparison for accuracy)\n",
        "            running_corrects += (preds == targets).sum().item()\n",
        "            total_predictions += targets.size(0)  # Update total prediction count\n",
        "\n",
        "        # Calculate Accuracy (divide by total predictions)\n",
        "        accuracy = (running_corrects / total_predictions) * 100\n",
        "\n",
        "    return accuracy, mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eAuS2JyRo7Yj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "\n",
        "def train(num_epochs, model, optimizer, scheduler):\n",
        "    print(\"Parsing dataset...\")\n",
        "    LOG_FREQUENCY = 10\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # We need to initialize best_val_acc to a very low value outside the epoch loop\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Training cycle\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        x_batches, y_batches = create_batches(train_data, BATCH_SIZE, SEQ_LEN, N_VOCAB)\n",
        "        for x_batch, y_batch in zip(x_batches, y_batches):\n",
        "\n",
        "\n",
        "            x_batch = x_batch.to(DEVICE)  # Move the data to the GPU\n",
        "            y_batch = y_batch.to(DEVICE)  # Move the targets to the GPU\n",
        "\n",
        "            model.train() # Set network to train mode\n",
        "            optimizer.zero_grad() # Zero the gradients\n",
        "            logits, _ = model(x_batch)  # Forward pass\n",
        "            # Riformat logits and y_batch for CrossEntropyLoss\n",
        "            logits = logits.view(-1, N_VOCAB)  # Reshape to (batch_size * seq_length, vocab_size)\n",
        "            y_batch = y_batch.view(-1)  # Reshape to (batch_size * seq_length)\n",
        "            loss = criterion(logits, y_batch)  # Calculate loss\n",
        "            #Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            # Compute the accuracy\n",
        "            predictions = torch.argmax(logits, dim=1)  # Predictions for each class with argmax\n",
        "            correct += (predictions == y_batch).sum().item()  # Sum of correct predictions\n",
        "            total += y_batch.size(0)  #Total number of target\n",
        "\n",
        "        # Evaluate on the validation set, done every epoch\n",
        "        val_acc, val_loss = evaluate(model, val_loader, criterion, DEVICE)\n",
        "        val_accuracies.append(val_acc)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        #print(f\"Epoch {epoch + 1}/{EPOCHS}, Training Loss: {total_loss / len(train_loader):.4f}\")\n",
        "        # Compute the training accuracy\n",
        "        accuracy = correct / total\n",
        "        print(f\"Training Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "        # Update the best model if validation accuracy improves\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = deepcopy(model.state_dict())  # Save the current model state\n",
        "            print(f\"New best model found with Validation accuracy: {val_acc:.2f}%\")\n",
        "        # Evaluate on the training set\n",
        "\n",
        "        if(epoch%LOG_FREQUENCY==0):\n",
        "            print(f\"--> best validation accuracy: {best_val_acc:.2f} epoch: {epoch+1}\")\n",
        "            print(f\"--> validation loss: {val_loss:.4f}-- training loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # if loss is zero exit\n",
        "        if total_loss <= 0.00009:\n",
        "          break\n",
        "    return train_accuracies, train_losses, val_accuracies, val_losses, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h1K4VRBFty5",
        "outputId": "297bc9ac-7f88-4f1d-df50-44c9d3f4204a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing dataset...\n",
            "Training Accuracy: 0.18%\n",
            "New best model found with Validation accuracy: 18.46%\n",
            "--> best validation accuracy: 18.46 epoch: 1\n",
            "--> validation loss: 3.1584-- training loss: 3.5539\n",
            "Training Accuracy: 0.21%\n",
            "New best model found with Validation accuracy: 26.21%\n",
            "Training Accuracy: 0.29%\n",
            "New best model found with Validation accuracy: 32.97%\n",
            "Training Accuracy: 0.33%\n",
            "New best model found with Validation accuracy: 34.35%\n",
            "Training Accuracy: 0.37%\n",
            "New best model found with Validation accuracy: 38.34%\n",
            "Training Accuracy: 0.39%\n",
            "New best model found with Validation accuracy: 40.47%\n",
            "Training Accuracy: 0.41%\n",
            "New best model found with Validation accuracy: 42.38%\n",
            "Training Accuracy: 0.43%\n",
            "New best model found with Validation accuracy: 44.65%\n",
            "Training Accuracy: 0.45%\n",
            "New best model found with Validation accuracy: 46.72%\n",
            "Training Accuracy: 0.47%\n",
            "New best model found with Validation accuracy: 48.01%\n",
            "Training Accuracy: 0.49%\n",
            "New best model found with Validation accuracy: 49.64%\n",
            "--> best validation accuracy: 49.64 epoch: 11\n",
            "--> validation loss: 1.7417-- training loss: 2.0054\n",
            "Training Accuracy: 0.50%\n",
            "New best model found with Validation accuracy: 50.81%\n",
            "Training Accuracy: 0.50%\n",
            "New best model found with Validation accuracy: 51.77%\n",
            "Training Accuracy: 0.51%\n",
            "New best model found with Validation accuracy: 52.67%\n",
            "Training Accuracy: 0.52%\n",
            "New best model found with Validation accuracy: 53.37%\n",
            "Training Accuracy: 0.53%\n",
            "New best model found with Validation accuracy: 54.47%\n",
            "Training Accuracy: 0.53%\n",
            "New best model found with Validation accuracy: 54.51%\n",
            "Training Accuracy: 0.54%\n",
            "New best model found with Validation accuracy: 55.18%\n",
            "Training Accuracy: 0.55%\n",
            "New best model found with Validation accuracy: 56.00%\n",
            "Training Accuracy: 0.55%\n",
            "New best model found with Validation accuracy: 56.24%\n",
            "Training Accuracy: 0.56%\n",
            "New best model found with Validation accuracy: 57.12%\n",
            "--> best validation accuracy: 57.12 epoch: 21\n",
            "--> validation loss: 1.4224-- training loss: 1.6625\n",
            "Training Accuracy: 0.56%\n",
            "New best model found with Validation accuracy: 58.13%\n",
            "Training Accuracy: 0.57%\n",
            "Training Accuracy: 0.57%\n",
            "New best model found with Validation accuracy: 58.37%\n",
            "Training Accuracy: 0.58%\n",
            "New best model found with Validation accuracy: 59.35%\n",
            "Training Accuracy: 0.58%\n",
            "New best model found with Validation accuracy: 59.97%\n",
            "Training Accuracy: 0.58%\n",
            "New best model found with Validation accuracy: 60.12%\n",
            "Training Accuracy: 0.59%\n",
            "New best model found with Validation accuracy: 60.58%\n",
            "Training Accuracy: 0.59%\n",
            "New best model found with Validation accuracy: 60.67%\n",
            "Training Accuracy: 0.60%\n",
            "New best model found with Validation accuracy: 60.76%\n",
            "Training Accuracy: 0.60%\n",
            "New best model found with Validation accuracy: 61.05%\n",
            "--> best validation accuracy: 61.05 epoch: 31\n",
            "--> validation loss: 1.2656-- training loss: 1.4714\n",
            "Training Accuracy: 0.60%\n",
            "New best model found with Validation accuracy: 61.47%\n"
          ]
        }
      ],
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = CharLSTM(vocab_size = N_VOCAB, embedding_size = EMBEDDING_SIZE, lstm_hidden_dim = LSTM_HIDDEN_DIM, seq_length = SEQ_LENGTH)\n",
        "model = model.to(DEVICE) # Move the entire model to the desired device\n",
        "\n",
        "# Definisci l'ottimizzatore Adam\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Definisci lo scheduler CosineAnnealingLR\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-4)\n",
        "\n",
        "train(200, model, optimizer, scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gFfp8-H7RzE"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDylj0YaBbGn"
      },
      "source": [
        "# Hyperparameters Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGshyLXHBmM4"
      },
      "outputs": [],
      "source": [
        "def get_scheduler_factory(num_epochs):\n",
        "    \"\"\"\n",
        "    Return a set of predefined learning rate scheduler factories with reasonable parameters.\n",
        "\n",
        "    Args:\n",
        "        num_epochs (int): Total number of epochs.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples with scheduler names and factory functions.\n",
        "    \"\"\"\n",
        "    schedulers = [\n",
        "        # StepLR\n",
        "        (\"StepLR (step_size=num_epochs//3, gamma=0.1)\",\n",
        "         lambda optimizer: torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs // 3, gamma=0.1)),\n",
        "\n",
        "        # CosineAnnealingLR\n",
        "        (\"CosineAnnealingLR (T_max=num_epochs//3, eta_min=1e-4)\",\n",
        "         lambda optimizer: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs // 3, eta_min=1e-4)),\n",
        "\n",
        "        # ExponentialLR\n",
        "        (\"ExponentialLR (gamma=0.9)\",\n",
        "         lambda optimizer: torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)),\n",
        "    ]\n",
        "    return schedulers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQAR2OREB5gT"
      },
      "source": [
        "In this section we will perform the hyperparameters selection using grid search among the considered values of paper [2] (Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\n",
        "    Jakub Konečný, Sanjiv Kumar, H. Brendan McMahan; Adaptive Federated Optimization, 2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C-ZMFDUBpU5"
      },
      "outputs": [],
      "source": [
        "# Generate values for the learning rate (lr) between 1e-3 and 1e-1 in log-uniform\n",
        "learning_rates = [10**i for i in np.arange(-3, 1.1, 1.5)]\n",
        "\n",
        "# Generate values for the weight decay (lr) between 1e-4 and 1e-1 in log-uniform\n",
        "weight_decays = [10**i for i in np.arange(-5, -1, 1.5)]\n",
        "\n",
        "print(\"Learning Rate Values (log-uniform):\", learning_rates)\n",
        "print(\"Weight Decay Values (log-uniform):\", weight_decays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRLslPN3BrBb"
      },
      "outputs": [],
      "source": [
        "num_epochs = 3 # low value for parameter tuning\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "scheduler_factories = get_scheduler_factory(num_epochs)\n",
        "results = []\n",
        "best_validation_accuracy_overall = 0.0\n",
        "best_setting = None\n",
        "best_scheduler = None\n",
        "best_lr = 0.01\n",
        "best_wd = 0.01\n",
        "print('Starting the parameter tuning loop...')\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        for scheduler_name, scheduler_factory in scheduler_factories:\n",
        "            # Reset the model\n",
        "            model = CharLSTM(vocab_size = N_VOCAB, embedding_size = EMBEDDING_SIZE, lstm_hidden_dim = LSTM_HIDDEN_DIM, seq_length = SEQ_LENGTH)\n",
        "            model = model.to(DEVICE) # Move the entire model to the desired device\n",
        "\n",
        "            # Create the optimizer\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "            # Definisci lo scheduler CosineAnnealingLR\n",
        "            #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-4)\n",
        "            #optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "            # Create the scheduler\n",
        "            scheduler = scheduler_factory(optimizer)\n",
        "            # Ezecute training\n",
        "            train_accuracies, train_losses, val_accuracies, val_losses, model = train(num_epochs, model, optimizer, scheduler)\n",
        "            # Print the best validation accuracy\n",
        "            best_val_accuracy = max(val_accuracies)\n",
        "            if best_val_accuracy > best_validation_accuracy_overall:\n",
        "                best_validation_accuracy_overall = best_val_accuracy\n",
        "                best_setting = (lr, wd)\n",
        "                best_wd = wd\n",
        "                best_lr = lr\n",
        "                best_scheduler = scheduler\n",
        "            print(f'Learning Rate: {lr}, Weight Decay: {wd}, Best Validation Accuracy: {best_val_accuracy:.2f}%')\n",
        "\n",
        "            results.append({\n",
        "                'learning_rate': lr,\n",
        "                'weight_decay': wd,\n",
        "                'train_accuracies': train_accuracies,\n",
        "                'train_losses': train_losses,\n",
        "                'val_accuracies': val_accuracies,\n",
        "                'val_losses': val_losses,\n",
        "            })\n",
        "print(\"Finished training loop.\")\n",
        "print(f'Best validation accuracy overall: {best_validation_accuracy_overall*100:.2f}%')\n",
        "print(f'Best setting: {best_setting}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-wLNxlRObo_"
      },
      "source": [
        "# Train the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7tgmbgaOdhQ"
      },
      "outputs": [],
      "source": [
        "model = CharLSTM(vocab_size = N_VOCAB, embedding_size = EMBEDDING_SIZE, lstm_hidden_dim = LSTM_HIDDEN_DIM, seq_length = SEQ_LENGTH)\n",
        "model = model.to(DEVICE) # Move the entire model to the desired device\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr, weight_decay=best_wd)\n",
        "scheduler = best_scheduler\n",
        "num_epochs = 200\n",
        "train_accuracies, train_losses, val_accuracies, val_losses, model = train(num_epochs, model, optimizer, scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glphuK3cndaw"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fEREiIPnekn"
      },
      "outputs": [],
      "source": [
        "def test(model, testloader):\n",
        "    \"\"\"\n",
        "    Test the model on the test set.\n",
        "    \"\"\"\n",
        "    accuracy, loss = evaluate(model, testloader, nn.CrossEntropyLoss(), DEVICE)\n",
        "    return accuracy, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDB2qzlqNd7y"
      },
      "source": [
        "# Plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWbEfdZfNgGP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re  # Imported module for regular expressions\n",
        "\n",
        "def plot_results(results, save_dir='./plots_centralized'):\n",
        "    \"\"\"\n",
        "    Save plots comparing training accuracy and validation accuracy per epoch for each combination of hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        results (list): List of dictionaries, where each dictionary contains:\n",
        "                        - 'learning_rate': Learning rate used.\n",
        "                        - 'weight_decay': Weight decay used.\n",
        "                        - 'scheduler_name': Name of the scheduler.\n",
        "                        - 'train_accuracies': List of training accuracies.\n",
        "                        - 'val_accuracies': List of validation accuracies.\n",
        "        save_dir (str): Directory where the plots will be saved.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for res in results:\n",
        "        # Extract hyperparameter values\n",
        "        lr = res['learning_rate']\n",
        "        wd = res['weight_decay']\n",
        "        scheduler_name = res['scheduler_name']\n",
        "\n",
        "        # Clean up the scheduler name for filename compatibility\n",
        "        clean_scheduler_name = re.sub(r\"[^a-zA-Z0-9]\", \"_\", scheduler_name)  # Sostituisce i caratteri non alfanumerici con '_'\n",
        "\n",
        "        # Generate a unique filename prefix for each configuration\n",
        "        file_prefix = f\"LR_{lr}_WD_{wd}_Scheduler_{clean_scheduler_name}\"\n",
        "\n",
        "        # Plot training and validation accuracy per epoch\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(res['train_accuracies'], label='Training Accuracy')\n",
        "        plt.plot(res['val_accuracies'], label='Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title(f\"Training vs Validation Accuracy (LR={lr}, WD={wd}, Scheduler={scheduler_name})\")\n",
        "        plt.legend()\n",
        "        accuracy_plot_path = os.path.join(save_dir, f\"{file_prefix}_training_vs_validation_accuracy.png\")\n",
        "        plt.savefig(accuracy_plot_path)\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"Plots saved to directory: {save_dir}\")\n",
        "\n",
        "#Plot only the best result\n",
        "filtered_results = [res for res in results if res['learning_rate'] == 0.01 and res['weight_decay'] == 0.0001 and res['scheduler_name']==\"CosineAnnealingLR (T_max=num_epochs//3, eta_min=1e-4)\"]\n",
        "plot_results(filtered_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ku0zVBfhLmh"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v87iSBS-4DVN"
      },
      "outputs": [],
      "source": [
        "test(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
