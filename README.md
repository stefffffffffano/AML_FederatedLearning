# Federated Learning Project: Optimizing Client Selection with Evolutionary Algorithms  

## ğŸ“– Overview  

Federated Learning (FL) is a decentralized machine learning approach where multiple devices train a shared model without exchanging local data. This project explores **client selection strategies** using **Evolutionary Algorithms (EAs)** to analyze their impact on model convergence and communication efficiency, comparing them with baseline methods.  

We conduct experiments on **two datasets**:  
- ğŸ“¸ **CIFAR-100** â€“ Image classification task  
- ğŸ­ **Shakespeare** â€“ Character-level language modeling  

The experiments include **centralized** and **federated** settings to provide a comparative analysis of Federated Learning performance.  

---

## ğŸ“‚ Repository Structure  

The repository is structured into two main dataset-specific directories:  

### ğŸ”¹ `cifar100/`  
- `data/cifar100/` â†’ Data loading utilities  
- `models/` â†’ Model definitions  
- `plots_centralized/`, `plots_federated/` â†’ Experiment results  
- `trained_models/` â†’ Saved models  
- `utils/` â†’ Utility scripts  
- `Client.py` â†’ Client-side logic for FL  
- `Server.py` â†’ Server-side logic for FL  
- `federated_notebook.ipynb` â†’ Jupyter notebook for FL experiments  
- `train_centralized.ipynb` â†’ Jupyter notebook for centralized experiments  
- `Report.md` â†’ Detailed results and analysis on centralized and federated baseline configurations 
- `personal_contribution/` â†’ **Implementation of Evolutionary Algorithms (EA) for client selection** with experiment result (`plots_federated`) and Jupyter notebook (`Experiments.ipynb`)

### ğŸ”¹ `shakespeare/`  
- `LEAF_data/` â†’ Dataset preprocessing scripts from **LEAF Benchmark**  
- `trained_models/` â†’ Saved models  
- `plot_centralized/`, `plot_federated/` â†’ Experiment results  
- `Client.py` â†’ Client-side logic  
- `Server.py` â†’ Server-side logic  
- `Model.py` â†’ LSTM model for character prediction  
- `Centralized_Shakespeare.ipynb` â†’ Centralized experiments  
- `Federated_Shakespeare.ipynb` â†’ Federated experiments  
- `Report.md` â†’ **Dataset creation information** and architecture details  
- `personal_contribution/` â†’ **Implementation of Evolutionary Algorithms (EA) for client selection** with experiment result (`plots_federated`) and Jupyter notebook (`Experiments.ipynb`)
---

## ğŸ¯ Personal Contribution Methodology  

We propose an **evolutionary-based client selection strategy** to tackle statistical and system heterogeneity in FL.  

 **Key Contributions:**  
âœ… **Adaptive Client Selection**: Using EAs to select high-loss clients for better model convergence  
âœ… **Fairness Considerations**: Ensuring diverse client participation without exposing private data  

---

## ğŸ“Š Experimental Setup  

### ğŸ—„ **Datasets**  
1ï¸âƒ£ **CIFAR-100**: Standard image classification dataset  
2ï¸âƒ£ **Shakespeare** (from [LEAF Benchmark](https://leaf.cmu.edu/)): Character-level text prediction  

### ğŸ— **Federated Learning Setup**  
- **Client-Server Architecture** with multiple clients training local models  
- **Heterogeneous Data Distribution** to simulate real-world FL scenarios  
- **Client Selection Strategies**:  
  - Baseline methods (Random selection, Uniform sampling)  
  - **Evolutionary Algorithm (EA)-based selection** for adaptive client participation  
- **Impact of Skewed Client Participation**:  
  - Clients selected with probabilities drawn from a **Dirichlet distribution** (Î³-controlled heterogeneity)  
  - Analysis of test accuracy degradation under different Î³ values  

### ğŸ”„ **Impact of Local Updates (J) and Communication Rounds**  
- **Evaluation under different numbers of local steps (J)**  
  - Trade-off analysis between local model drift and communication frequency  
  - Scaling rounds to balance global updates vs. local training  
  - Effect of J on CIFAR-100 and Shakespeare datasets  

### ğŸ”§ **Hyperparameter Tuning**  
- **CIFAR-100**: Grid search on **learning rates, weight decays, and schedulers** (Only for Centralized version: StepLR, Cosine Annealing, Exponential LR)  
- **Shakespeare**: Preprocessing and tuning based on **LEAF Benchmark** and literature guidelines  
- **EA-based selection**: Hyperparameter tuning on **crossover probabilities**  

---

## ğŸš€ Getting Started  

### ğŸ”¹ **1. Clone the repository**  
```bash
git clone https://github.com/stefffffffffano/AML_FederatedLearning.git
cd AML_FederatedLearning
```

### ğŸ”¹ **2. Set up the environment**  
```bash
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
pip install -r requirements.txt
```

### ğŸ”¹ **3. Run Experiments**  
#### ğŸ–¼ CIFAR-100  
- **Centralized Training:**  
  ```bash
  jupyter notebook cifar100/train_centralized.ipynb
  ```
- **Federated Learning:**  
  ```bash
  jupyter notebook cifar100/federated_notebook.ipynb
  ```
- **Personal Contribution (EA-based Client Selection) Notebook:**  
  ```bash
  jupyter notebook personal_contribution/Experiments.ipynb
  ```

#### ğŸ­ Shakespeare  
- **Centralized Training:**  
  ```bash
  jupyter notebook shakespeare/Centralized_Shakespeare.ipynb
  ```
- **Federated Learning:**  
  ```bash
  jupyter notebook shakespeare/Federated_Shakespeare.ipynb
  ```
- **Personal Contribution (EA-based Client Selection) Notebook:**  
  ```bash
  jupyter notebook personal_contribution/Experiments.ipynb
  ```

---

## ğŸ¤ Contributions  

Contributions are welcome! Feel free to fork the repo and submit pull requests. ğŸš€  

---

## ğŸ“¬ Contact  

**Authors:**  
- ğŸ§‘â€ğŸ’» [Luca Dadone](https://github.com/dadoluca) 
- ğŸ§‘â€ğŸ’» [Stefano Fumero ](https://github.com/stefffffffffano) 
- ğŸ§‘â€ğŸ’» [Andrea Mirenda](https://github.com/Andrea-1704) 

For any inquiries, please reach out to the authors.  

